{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will explore different combinations of the features that were used in submission and save the results/evaluations to compare the different scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bioc import BioCXMLReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../final_system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import annotation\n",
    "import base_feature\n",
    "import made_utils\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "\n",
    "from nltk import ngrams as nltk_ngrams\n",
    "import re\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "DATADIR = '/Users/alec/Data/NLP_Challenge'\n",
    "# ALLDIR = os.path.join(DATADIR, 'original_data')\n",
    "TRAINDIR = os.path.join(DATADIR, 'MADE-1.0')\n",
    "TESTDIR = os.path.join(DATADIR, 'made_test_data')\n",
    "print(os.path.exists(TRAINDIR))\n",
    "print(os.path.exists(TESTDIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_grams(ngram_string):\n",
    "    \"\"\"\n",
    "    Normalizes the values in a string of joined ngrams\n",
    "    \"\"\"\n",
    "    # Substitute numbers\n",
    "    ngram_string = re.sub('[\\d]+|one|two|three|four|five|six|seven|eight|nine|ten', '<NUMBER>', ngram_string)\n",
    "    return ngram_string\n",
    "\n",
    "\n",
    "\n",
    "class LexicalFeatureExtractor(base_feature.BaseFeatureExtractor):\n",
    "    \"\"\"\n",
    "    ngram_window - the length of ngrams to include in the vocabulary.\n",
    "    context_window - the number of ngrams to include before and after the entity.\n",
    "    \"\"\"\n",
    "    def __init__(self, ngram_window=(1, 1), context_window=(2, 2),\n",
    "                vocab=None, pos_vocab=None, min_vocab_count=5, min_pos_count=5):\n",
    "        super().__init__()\n",
    "        self.ngram_window = ngram_window\n",
    "        if min(ngram_window) < 1 or max(ngram_window) > 3:\n",
    "            raise NotImplementedError(\"Ngram Window must be between one and 3\")\n",
    "        self.context_window = context_window\n",
    "        self.min_vocab_count = min_vocab_count\n",
    "        self.min_pos_count = min_pos_count\n",
    "\n",
    "        # Set vocab and POS vocab\n",
    "        self._unfiltered_vocab = vocab # Contains unigrams-trigrams, no count threshold\n",
    "        self._unfiltered_pos_vocab = pos_vocab\n",
    "\n",
    "        self.vocab = self.create_vocab(vocab, min_vocab_count, self.ngram_window) # Only contains ngrams defined by context_window\n",
    "        #print(self.vocab); exit()\n",
    "        self.pos_vocab =  self.create_vocab(pos_vocab, min_pos_count, self.ngram_window)\n",
    "        #self.tokens = [gram for (gram, idx) in self.vocab.items() if len(gram.split()) == 1] # Only unigrams\n",
    "        self.pos = {} # Will eventually contain mapping for POS tags\n",
    "\n",
    "        # pyConText tools\n",
    "        #self.modifiers = itemData.instantiateFromCSVtoitemData(\"https://raw.githubusercontent.com/chapmanbe/pyConTextNLP/master/KB/lexical_kb_05042016.tsv\")\n",
    "        #self.targets = itemData.instantiateFromCSVtoitemData(\"https://raw.githubusercontent.com/abchapman93/MADE_relations/master/feature_extraction/targets.tsv?token=AUOYx9rYHO6A5fiZS3mB9e_3DP83Uws8ks5aownVwA%3D%3D\")\n",
    "\n",
    "\n",
    "        #self.all_features_values = self.create_base_features()\n",
    "\n",
    "\n",
    "\n",
    "    def create_base_features(self):\n",
    "        \"\"\"\n",
    "        Enumerates possible feature values from the vocab, as well as an OOV value.\n",
    "        Any features that are binary should only get one index and are encoded as 0.\n",
    "        \"\"\"\n",
    "        # This will be a dictionary that contains all possible values for each feature\n",
    "        all_features_values = {\n",
    "            'same_sentence': 0,\n",
    "            'num_tokens_between': 0,\n",
    "            'grams_between': ['OOV'] + list(self.vocab),\n",
    "            'grams_before': ['OOV'] + list(self.vocab),\n",
    "            'grams_after': ['OOV'] + list(self.vocab),\n",
    "            'pos_grams_between': ['OOV'] + list(self.pos_vocab),\n",
    "            #'pos_grams_before': ['OOV'] + list(self.pos_vocab),\n",
    "            #'pos_grams_after': ['OOV'] + list(self.pos_vocab),\n",
    "            'first_entity_type': 0,#list(ENTITY_TYPES_MAPPING.values()),\n",
    "            'second_entity_type': 0,#list(ENTITY_TYPES_MAPPING.values()),\n",
    "\n",
    "            }\n",
    "        return all_features_values\n",
    "\n",
    "    def create_feature_dict(self, relat, doc, entities=True, entities_between=True, surface=True):\n",
    "        \"\"\"\n",
    "        Takes a RelationAnnotation and an AnnotatedDocument.\n",
    "        Returns the a dictionary containing the defined lexical features.\n",
    "        \"\"\"\n",
    "\n",
    "        lex_features = {}\n",
    "\n",
    "        if entities:\n",
    "            lex_features.update(self.get_entity_features(relat, doc))\n",
    "        if entities_between:\n",
    "            lex_features.update(self.get_entities_between_features(relat, doc))\n",
    "        if surface:\n",
    "            lex_features.update(self.get_surface_features(relat, doc))\n",
    "        return lex_features\n",
    "    \n",
    "    \n",
    "    def get_entity_features(self, relat, doc):\n",
    "        features = {}\n",
    "        \n",
    "        # The full string of the entities\n",
    "        anno1, anno2 = relat.get_annotations()\n",
    "        features['text_in_anno1'] = anno1.text.lower()\n",
    "        features['text_in_anno2'] = anno2.text.lower()\n",
    "        features['concat_text'] = anno1.text.lower() + ':' + anno2.text.lower()\n",
    "        \n",
    "        # Features for types of the entities\n",
    "        features['first_entity_type:<{}>'.format(relat.entity_types[0].upper())] = 1\n",
    "        features['second_entity_type:<{}>'.format(relat.entity_types[1].upper())] = 1\n",
    "        \n",
    "        # Feature types for entities, left to right\n",
    "        sorted_entities = sorted((relat.annotation_1, relat.annotation_2), key=lambda a: a.span[0])\n",
    "        features['entity_types_concat'] = '<=>'.join(['<{}>'.format(a.type.upper()) for a in sorted_entities])\n",
    "        return features\n",
    "    \n",
    "    \n",
    "    def get_entities_between_features(self, relat, doc):\n",
    "       \n",
    "        features = {}\n",
    "        # One binary feature for every type of entity between\n",
    "        entities_between = self.get_entities_between(relat, doc)\n",
    "        # TODO: Maybe change this to a count\n",
    "        features.update({\n",
    "            'entities_between:<{}>'.format(v.type.upper()): 1 for v in entities_between\n",
    "            })\n",
    "        features['num_entities_between'] = len(entities_between)\n",
    "\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_surface_features(self, relat, doc):        \n",
    "        \n",
    "        features = {}\n",
    "        \n",
    "        # Same sentence\n",
    "        features['num_sentences_overlap'] = len(doc.get_sentences_overlap_span(relat.get_span()))\n",
    "        # Get the number of tokens between\n",
    "        # NOTE: only unigrams\n",
    "        \n",
    "        features['num_tokens_between'] = len(self.get_grams_between(relat, doc, ngram_window=(1, 1)))\n",
    "        # Get all tokens/POS tags in between\n",
    "        # Create one feature for each ngram/tag\n",
    "        features.update({\n",
    "            'grams_between:<{}>'.format(v): 1 for v in self.get_grams_between(relat, doc)\n",
    "            })\n",
    "        features.update({\n",
    "            'grams_before:<{}>'.format(v): 1 for v in self.get_grams_before(relat, doc)\n",
    "            })\n",
    "        features.update({\n",
    "            'grams_after:<{}>'.format(v): 1 for v in self.get_grams_after(relat, doc)\n",
    "            })\n",
    "\n",
    "        features.update({\n",
    "            'tags_between:<{}>'.format(v): 1 for v in self.get_grams_between(relat, doc, seq='tags')\n",
    "            })\n",
    "        features.update({\n",
    "            'tags_before:<{}>'.format(v): 1 for v in self.get_grams_before(relat, doc, seq='tags')\n",
    "            })\n",
    "        features.update({\n",
    "            'tags_after:<{}>'.format(v): 1 for v in self.get_grams_after(relat, doc, seq='tags')\n",
    "            })\n",
    "\n",
    "        # Get features for information about entities/context between\n",
    "        # Binary feature: Are they in the same sentence?\n",
    "        features['same_sentence'] = doc.in_same_sentence(relat.get_span())\n",
    "        return features\n",
    "        \n",
    "\n",
    "    def get_grams_between(self, relat, doc, seq='tokens', ngram_window=None):\n",
    "        \"\"\"\n",
    "        Returns the N-grams between the two entities connected in relat.\n",
    "        Represents it as OOV if it's not in the vocabulary.\n",
    "        Returns a unique set.\n",
    "        \"\"\"\n",
    "\n",
    "        if seq == 'tokens':\n",
    "            vocab = self.vocab\n",
    "        elif seq == 'tags':\n",
    "            vocab = self.pos_vocab\n",
    "        else:\n",
    "            raise ValueError(\"Must specify seq: {}\".format(seq))\n",
    "\n",
    "        if not ngram_window:\n",
    "            ngram_window = self.ngram_window\n",
    "\n",
    "        all_grams = []\n",
    "        span1, span2 = relat.spans\n",
    "        # Fixed this: get the start and span of the middle, not of the entire relation\n",
    "        _, start, end, _ = sorted(span1 +span2)\n",
    "        tokens_in_span = doc.get_tokens_or_tags_at_span((start, end), seq)\n",
    "        # NOTE: lower-casing the ngrams, come back to this if you want to encode the casing\n",
    "        tokens_in_span = [token.lower() for token in tokens_in_span]\n",
    "        for n in range(ngram_window[0], ngram_window[1] + 1):\n",
    "            # Now sort the ngrams so that it doesn't matter what order they occur in\n",
    "            grams = list(nltk_ngrams(tokens_in_span, n))\n",
    "            grams = self.sort_ngrams(grams)# + [' '.join(sorted(tup)) for tup in list(nltk_ngrams(tokens_in_span, n))]\n",
    "            all_grams.extend(set(grams))\n",
    "        all_grams = [self.normalize_grams(x) for x in set(all_grams)]\n",
    "        all_grams = [x if x in vocab else 'OOV' for x in all_grams]\n",
    "        return set(all_grams)\n",
    "\n",
    "\n",
    "    def get_grams_before(self, relat,doc, seq='tokens', ngram_window=None):\n",
    "        \"\"\"\n",
    "        Returns the n-grams before the first entity.\n",
    "        \"\"\"\n",
    "        if seq == 'tokens':\n",
    "            vocab = self.vocab\n",
    "        elif seq == 'tags':\n",
    "            vocab = self.pos_vocab\n",
    "        if not ngram_window:\n",
    "            ngram_window = self.ngram_window\n",
    "\n",
    "        all_grams = []\n",
    "        offset = relat.span[0]\n",
    "        tokens_before = doc.get_tokens_or_tags_before_or_after(offset, delta=-1,\n",
    "            n=self.context_window[0], seq=seq, padding=True)\n",
    "        tokens_before = [token.lower() for token in tokens_before]\n",
    "        for n in range(ngram_window[0], ngram_window[1] + 1):\n",
    "            grams = list(nltk_ngrams(tokens_before, n))\n",
    "            grams = self.sort_ngrams(grams)# + [' '.join(sorted(tup)) for tup in list(nltk_ngrams(tokens_in_span, n))]\n",
    "            all_grams.extend(set(grams))\n",
    "            #grams = grams + [' '.join(sorted(tup)) for tup in list(nltk_ngrams(tokens_before, n))]\n",
    "        all_grams = [self.normalize_grams(x) for x in set(all_grams)]\n",
    "        all_grams = [x if x in vocab else 'OOV' for x in all_grams]\n",
    "        return set(all_grams)\n",
    "\n",
    "    def get_grams_after(self, relat, doc, seq='tokens', ngram_window=None):\n",
    "        \"\"\"\n",
    "        Returns the n-grams after the final entity.\n",
    "        \"\"\"\n",
    "        if seq == 'tokens':\n",
    "            vocab = self.vocab\n",
    "        elif seq == 'tags':\n",
    "            vocab = self.pos_vocab\n",
    "        if not ngram_window:\n",
    "            ngram_window = self.ngram_window\n",
    "\n",
    "        all_grams = []\n",
    "        offset = relat.span[1]\n",
    "        tokens_after = doc.get_tokens_or_tags_before_or_after(offset, delta=1,\n",
    "                                        n=self.context_window[1], seq=seq)\n",
    "        tokens_after = [token.lower() for token in tokens_after]\n",
    "        for n in range(ngram_window[0], ngram_window[1] + 1):\n",
    "            grams = list(nltk_ngrams(tokens_after, n))\n",
    "            grams = self.sort_ngrams(grams)# + [' '.join(sorted(tup)) for tup in list(nltk_ngrams(tokens_in_span, n))]\n",
    "            all_grams.extend(set(grams))\n",
    "            #grams = grams + [' '.join(sorted(tup)) for tup in list(nltk_ngrams(tokens_after, n))]\n",
    "        all_grams = [self.normalize_grams(x) for x in set(all_grams)]\n",
    "        all_grams = [x if x in vocab else 'OOV' for x in all_grams]\n",
    "        return set(all_grams)\n",
    "\n",
    "    def sort_ngrams(self, ngrams):\n",
    "        return [' '.join(sorted(tup)) for tup in ngrams]\n",
    "\n",
    "    def normalize_grams(self, ngram_string):\n",
    "        \"\"\"\n",
    "        Normalizes the values in a string of joined ngrams\n",
    "        \"\"\"\n",
    "        # Substitute numbers\n",
    "        return normalize_grams(ngram_string)\n",
    "\n",
    "    def get_pos_tags(self):\n",
    "        pass\n",
    "\n",
    "    def get_entities_between(self, relat, doc):\n",
    "        \"\"\"\n",
    "        Returns a list of entities that occur between entity1 and entity2\n",
    "        \"\"\"\n",
    "        offset, end = relat.get_span()\n",
    "        overlapping_entities = []\n",
    "        # Index the entity in doc by span\n",
    "        offset_to_entity = {entity.span[0]: entity for entity in doc.get_annotations()\n",
    "                    if entity.id not in (\n",
    "                        relat.annotation_1.id, relat.annotation_2.id)\n",
    "                        }\n",
    "\n",
    "        while offset < end:\n",
    "            if offset in offset_to_entity:\n",
    "                overlapping_entities.append(offset_to_entity[offset])\n",
    "            offset += 1\n",
    "\n",
    "        return overlapping_entities\n",
    "\n",
    "\n",
    "    def get_sent_with_anno(self, anno, doc, entity_type):\n",
    "        \"\"\"\n",
    "        Returns the sentence that contains a given annotation.\n",
    "        Replaces the text of the annotations with a tag <ENTITY-TYPE>\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        # Step back some window\n",
    "        offset = anno.start_index\n",
    "\n",
    "        while offset not in doc._sentences:\n",
    "            offset -= 1\n",
    "            if offset < 0:\n",
    "                break\n",
    "            if offset in doc._tokens:\n",
    "                tokens.insert(0, doc._tokens[offset].lower())\n",
    "\n",
    "        # Now add an entity\n",
    "        tokens.append(entity_type)\n",
    "\n",
    "        # Now add all the tokens between them\n",
    "        offset = anno.start_index\n",
    "\n",
    "        while offset not in doc._sentences:\n",
    "            if offset > max(doc._tokens.keys()):\n",
    "                break\n",
    "            if offset in doc._tokens:\n",
    "                tokens.append(doc._tokens[offset].lower())\n",
    "            offset += 1\n",
    "\n",
    "\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"LexicalFeatureExtractor Ngram Window: {} Vocab: {} terms\".format(\n",
    "                self.ngram_window, len(self.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's remove the validation hold-out set\n",
    "# import glob\n",
    "\n",
    "# held_out = [os.path.basename(x) for x in glob.glob(os.path.join('..', 'data', 'heldout_xmls', 'corpus', '*'))]\n",
    "# docs = {fname: doc for (fname, doc) in docs.items() if fname not in held_out}\n",
    "# len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_annotations_in_doc(doc, legal_edges=[], max_sent_length=3):\n",
    "    \"\"\"\n",
    "    Takes a single AnnotatedDocument that contains annotations.\n",
    "    All annotations that have a legal edge between them\n",
    "    and are have an overlapping sentence length <= max_sent_length,\n",
    "        ie., they are in either the same sentence or n adjancent sentences,\n",
    "    are paired to create RelationAnnotations.\n",
    "    Takes an optional list legal_edges that defines which edges should be allowed.\n",
    "\n",
    "    Returns a list of new RelationAnnotations with annotation type 'none'.\n",
    "    \"\"\"\n",
    "    if legal_edges == []:\n",
    "        legal_edges = [('Drug', 'Route'),\n",
    "                         ('Drug', 'Indication'),\n",
    "                         ('SSLIF', 'Severity'),\n",
    "                         ('Drug', 'Dose'),\n",
    "                         ('Drug', 'Frequency'),\n",
    "                         ('Drug', 'Duration'),\n",
    "                         ('Drug', 'ADE'),\n",
    "                         ('ADE', 'Severity'),\n",
    "                         ('Indication', 'Severity'),\n",
    "                         ('SSLIF', 'ADE')]\n",
    "    true_annotations = doc.get_annotations()\n",
    "    true_relations = doc.get_relations()\n",
    "    generated_relations = []\n",
    "    edges = defaultdict(list)\n",
    "    edges = set()\n",
    "\n",
    "    # Map all annotation_1's to annotation_2's\n",
    "    # in order to identify all positive examples of relations\n",
    "    # If this is testing data, it may not actually have these\n",
    "    for relat in true_relations:\n",
    "        anno1, anno2 = relat.get_annotations()\n",
    "        edges.add((anno1.id, anno2.id))\n",
    "\n",
    "    for anno1 in true_annotations:\n",
    "        for anno2 in true_annotations:\n",
    "\n",
    "            # Don't pair the same annotation with itself\n",
    "            if anno1.id == anno2.id:\n",
    "                continue\n",
    "\n",
    "            if anno1.span == anno2.span:\n",
    "                continue\n",
    "\n",
    "            # Don't generate paris that have already been paried\n",
    "            if (anno1.id, anno2.id) in edges:\n",
    "                continue\n",
    "\n",
    "            # Exclude illegal relations\n",
    "            if len(legal_edges) and (anno1.type, anno2.type) not in legal_edges:\n",
    "                continue\n",
    "\n",
    "            # Check the span between them, make sure it's either 1 or 2\n",
    "            start1, end1 = anno1.span\n",
    "            start2, end2 = anno2.span\n",
    "            sorted_spans = list(sorted([start1, end1, start2, end2]))\n",
    "            span = (sorted_spans[0], sorted_spans[-1])\n",
    "            overlapping_sentences = doc.get_sentences_overlap_span(span)\n",
    "            if len(overlapping_sentences) > max_sent_length:\n",
    "                continue\n",
    "\n",
    "            # If they haven't already been paired, pair them\n",
    "            else:\n",
    "                generated_relation = annotation.RelationAnnotation.from_null_rel(\n",
    "                    anno1, anno2, doc.file_name\n",
    "                )\n",
    "                edges.add((anno1.id, anno2.id))\n",
    "                generated_relations.append(generated_relation)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return list(set(generated_relations + true_relations))\n",
    "\n",
    "def sample_negative_examples(relations, neg_prop=1.0):\n",
    "    \"\"\"\n",
    "    Takes a list of Relationannotations and\n",
    "    neg_prop, a float that specifies the proportion of negative\n",
    "    to positive examples.\n",
    "\n",
    "    In the future, a more sophisticated method of sampling might be used,\n",
    "    ie., sampling by the probability of the Annotation types in the nodes.\n",
    "    \"\"\"\n",
    "    pos_relations = []\n",
    "    neg_relations = []\n",
    "    for relat in relations:\n",
    "        if relat.type == 'none':\n",
    "            neg_relations.append(relat)\n",
    "        else:\n",
    "            pos_relations.append(relat)\n",
    "\n",
    "    pos_size = len(pos_relations)\n",
    "    neg_sample_size = int(neg_prop * pos_size)\n",
    "\n",
    "    neg_sample = random.sample(neg_relations, neg_sample_size)\n",
    "    print(\"Original Distribution: {} positive relations, {} negative relations\".format(\n",
    "                len(pos_relations),\n",
    "                len(neg_relations)))\n",
    "    print(\"{} positive relations, {} negative relations\".format(len(pos_relations),\n",
    "                len(neg_sample)))\n",
    "    return pos_relations + neg_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/876\n",
      "100/876\n",
      "200/876\n",
      "300/876\n",
      "400/876\n",
      "500/876\n",
      "600/876\n",
      "700/876\n",
      "800/876\n"
     ]
    }
   ],
   "source": [
    "reader = made_utils.TextAndBioCParser(TRAINDIR)\n",
    "docs = reader.read_texts_and_xmls(-1) # TODO: Change to -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = docs['12_123']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0: 12_123 \n",
      "96\n",
      "256\n",
      "-10: 10_729 \n",
      "71\n",
      "213\n",
      "-20: 3_586 \n",
      "24\n",
      "72\n",
      "-30: 3_120 \n",
      "9\n",
      "11\n",
      "-40: 7_293 \n",
      "42\n",
      "126\n",
      "-50: 16_201 \n",
      "14\n",
      "38\n",
      "-60: 7_666 \n",
      "16\n",
      "34\n",
      "-70: 10_1 \n",
      "18\n",
      "54\n",
      "-80: 6_656 \n",
      "15\n",
      "45\n",
      "-90: 5_496 \n",
      "9\n",
      "27\n",
      "-100: 6_895 \n",
      "7\n",
      "13\n",
      "-110: 6_40 \n",
      "53\n",
      "144\n",
      "-120: 12_943 \n",
      "22\n",
      "66\n",
      "-130: 14_920 \n",
      "21\n",
      "63\n",
      "-140: 1_260 \n",
      "47\n",
      "141\n",
      "-150: 10_691 \n",
      "3\n",
      "7\n",
      "-160: 12_433 \n",
      "0\n",
      "0\n",
      "-170: 13_435 \n",
      "10\n",
      "30\n",
      "-180: 6_323 \n",
      "14\n",
      "39\n",
      "-190: 1_859 \n",
      "63\n",
      "189\n",
      "-200: 4_144 \n",
      "18\n",
      "54\n",
      "-210: 12_74 \n",
      "3\n",
      "9\n",
      "-220: 3_256 \n",
      "8\n",
      "17\n",
      "-230: 14_350 \n",
      "21\n",
      "63\n",
      "-240: 14_966 \n",
      "12\n",
      "23\n",
      "-250: 19_8 \n",
      "7\n",
      "21\n",
      "-260: 1_424 \n",
      "91\n",
      "273\n",
      "-270: 1_415 \n",
      "21\n",
      "49\n",
      "-280: 10_1014 \n",
      "24\n",
      "55\n",
      "-290: 12_671 \n",
      "0\n",
      "0\n",
      "-300: 8_329 \n",
      "30\n",
      "90\n",
      "-310: 7_797 \n",
      "11\n",
      "33\n",
      "-320: 10_248 \n",
      "9\n",
      "17\n",
      "-330: 3_689 \n",
      "15\n",
      "33\n",
      "-340: 17_1036 \n",
      "30\n",
      "90\n",
      "-350: 13_599 \n",
      "1\n",
      "2\n",
      "-360: 19_202 \n",
      "41\n",
      "123\n",
      "-370: 1_1041 \n",
      "29\n",
      "87\n",
      "-380: 6_286 \n",
      "0\n",
      "0\n",
      "-390: 10_960 \n",
      "6\n",
      "13\n",
      "-400: 10_190 \n",
      "0\n",
      "0\n",
      "-410: 10_367 \n",
      "20\n",
      "60\n",
      "-420: 14_411 \n",
      "1\n",
      "2\n",
      "-430: 7_447 \n",
      "5\n",
      "5\n",
      "-440: 1_996 \n",
      "25\n",
      "75\n",
      "-450: 10_162 \n",
      "19\n",
      "45\n",
      "-460: 14_408 \n",
      "10\n",
      "30\n",
      "-470: 6_609 \n",
      "1\n",
      "3\n",
      "-480: 1_720 \n",
      "44\n",
      "132\n",
      "-490: 10_518 \n",
      "18\n",
      "44\n",
      "-500: 5_207 \n",
      "6\n",
      "6\n",
      "-510: 8_218 \n",
      "13\n",
      "39\n",
      "-520: 17_605 \n",
      "12\n",
      "36\n",
      "-530: 1_728 \n",
      "28\n",
      "84\n",
      "-540: 13_511 \n",
      "28\n",
      "84\n",
      "-550: 1_1034 \n",
      "77\n",
      "231\n",
      "-560: 14_521 \n",
      "20\n",
      "44\n",
      "-570: 1_239 \n",
      "2\n",
      "6\n",
      "-580: 14_924 \n",
      "61\n",
      "183\n",
      "-590: 5_744 \n",
      "2\n",
      "2\n",
      "-600: 6_917 \n",
      "48\n",
      "144\n",
      "-610: 6_184 \n",
      "68\n",
      "204\n",
      "-620: 7_911 \n",
      "19\n",
      "57\n",
      "-630: 7_318 \n",
      "9\n",
      "26\n",
      "-640: 19_198 \n",
      "12\n",
      "36\n",
      "-650: 7_719 \n",
      "20\n",
      "48\n",
      "-660: 14_119 \n",
      "41\n",
      "123\n",
      "-670: 19_41 \n",
      "3\n",
      "6\n",
      "-680: 14_763 \n",
      "26\n",
      "78\n",
      "-690: 21_807 \n",
      "6\n",
      "18\n",
      "-700: 4_564 \n",
      "25\n",
      "75\n",
      "-710: 17_709 \n",
      "24\n",
      "68\n",
      "-720: 3_417 \n",
      "35\n",
      "91\n",
      "-730: 6_136 \n",
      "61\n",
      "183\n",
      "-740: 7_191 \n",
      "2\n",
      "6\n",
      "-750: 10_1018 \n",
      "46\n",
      "138\n",
      "-760: 6_702 \n",
      "5\n",
      "10\n",
      "-770: 7_303 \n",
      "69\n",
      "207\n"
     ]
    }
   ],
   "source": [
    "relations = []\n",
    "for i, (fname, doc) in enumerate(docs.items()):\n",
    "    if i  % 10 == 0:\n",
    "        print('-{}: {} '.format(i, fname))\n",
    "        print(len(doc.relations))\n",
    "    new_relations = pair_annotations_in_doc(doc)\n",
    "    \n",
    "    # Add Fake relations for training\n",
    "    neg_relations = set(new_relations).difference(set(doc.relations))\n",
    "    # Sample them\n",
    "    if len(neg_relations) >= 2 * len(doc.relations):\n",
    "        neg_relations = random.sample(neg_relations, 2 * len(doc.relations))\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "    doc.add_relations(neg_relations)\n",
    "    \n",
    "    relations.extend(doc.get_relations())\n",
    "    if i  % 10 == 0:\n",
    "        print(len(doc.get_relations()))\n",
    "   "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('tmp_data/all_training_docs_and_relations.pkl', 'wb') as f:\n",
    "    pickle.dump((docs, relations), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../final_system/data/vocab.pkl', 'rb') as f:\n",
    "    vocab, pos_vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "relations = []\n",
    "for doc in docs.values():\n",
    "    relations += doc.relations\n",
    "random.shuffle(relations)\n",
    "len(relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67021"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('tmp_data/all_training_docs_and_relations.pkl', 'rb') as f:\n",
    "    docs, relations = pickle.load(f)\n",
    "len(relations)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "with open('tmp_data/val_docs_and_relations.pkl', 'rb') as f:\n",
    "    test_docs, test_relations = pickle.load(f)\n",
    "\n",
    "# Take out validation relations\n",
    "relations = [r for r in relations if r.file_name not in test_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['12_123', '4_857', '17_839', '10_988', '13_513'])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171\n"
     ]
    }
   ],
   "source": [
    "relat = relations[0]\n",
    "print(len([r for r in relations if r.type == 'none']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "283"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226\n",
      "57\n"
     ]
    }
   ],
   "source": [
    "print(len(train_relats))\n",
    "print(len(val_relats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len({r.id for r in train_relats}.intersection({r.id for r in val_relats})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(548, 637)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'atenolol':'oral', Drug:Route, type=manner/route"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = relations[7]\n",
    "doc = docs[r.file_name]\n",
    "print(r.span)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283\n",
      "283\n"
     ]
    }
   ],
   "source": [
    "rids = [r.id for r in relations]\n",
    "print(len(rids))\n",
    "print(len(set(rids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "283"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(relations))\n",
    "len(set(relations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations = list(set(relations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'do': 5177,\n",
       "         'fr': 4419,\n",
       "         'manner/route': 2551,\n",
       "         'reason': 4554,\n",
       "         'none': 43856,\n",
       "         'du': 906,\n",
       "         'severity_type': 3476,\n",
       "         'adverse': 2082})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rtypes = [r.type for r in relations]\n",
    "from collections import Counter\n",
    "c = Counter(rtypes)\n",
    "c"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Train-val split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_relats, val_relats = train_test_split(relations, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_dicts(relations, docs):\n",
    "    feat_dicts = []\n",
    "    for i, r in enumerate(relations):\n",
    "        doc = docs[r.file_name]\n",
    "        if i % 100 == 0:\n",
    "            print(\"{}/{}\".format(i, len(relations)))\n",
    "        feat_dict = feature_extractor.create_feature_dict(r, doc, entities=True, entities_between=True, surface=True)\n",
    "        feat_dicts.append(feat_dict)\n",
    "        \n",
    "        \n",
    "    return feat_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/67021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alec/anaconda/envs/made/lib/python3.6/site-packages/ipykernel_launcher.py:182: DeprecationWarning: generator 'ngrams' raised StopIteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/67021\n",
      "200/67021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alec/anaconda/envs/made/lib/python3.6/site-packages/ipykernel_launcher.py:232: DeprecationWarning: generator 'ngrams' raised StopIteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/67021\n",
      "400/67021\n",
      "500/67021\n",
      "600/67021\n",
      "700/67021\n",
      "800/67021\n",
      "900/67021\n",
      "1000/67021\n",
      "1100/67021\n",
      "1200/67021\n",
      "1300/67021\n",
      "1400/67021\n",
      "1500/67021\n",
      "1600/67021\n",
      "1700/67021\n",
      "1800/67021\n",
      "1900/67021\n",
      "2000/67021\n",
      "2100/67021\n",
      "2200/67021\n",
      "2300/67021\n",
      "2400/67021\n",
      "2500/67021\n",
      "2600/67021\n",
      "2700/67021\n",
      "2800/67021\n",
      "2900/67021\n",
      "3000/67021\n",
      "3100/67021\n",
      "3200/67021\n",
      "3300/67021\n",
      "3400/67021\n",
      "3500/67021\n",
      "3600/67021\n",
      "3700/67021\n",
      "3800/67021\n",
      "3900/67021\n",
      "4000/67021\n",
      "4100/67021\n",
      "4200/67021\n",
      "4300/67021\n",
      "4400/67021\n",
      "4500/67021\n",
      "4600/67021\n",
      "4700/67021\n",
      "4800/67021\n",
      "4900/67021\n",
      "5000/67021\n",
      "5100/67021\n",
      "5200/67021\n",
      "5300/67021\n",
      "5400/67021\n",
      "5500/67021\n",
      "5600/67021\n",
      "5700/67021\n",
      "5800/67021\n",
      "5900/67021\n",
      "6000/67021\n",
      "6100/67021\n",
      "6200/67021\n",
      "6300/67021\n",
      "6400/67021\n",
      "6500/67021\n",
      "6600/67021\n",
      "6700/67021\n",
      "6800/67021\n",
      "6900/67021\n",
      "7000/67021\n",
      "7100/67021\n",
      "7200/67021\n",
      "7300/67021\n",
      "7400/67021\n",
      "7500/67021\n",
      "7600/67021\n",
      "7700/67021\n",
      "7800/67021\n",
      "7900/67021\n",
      "8000/67021\n",
      "8100/67021\n",
      "8200/67021\n",
      "8300/67021\n",
      "8400/67021\n",
      "8500/67021\n",
      "8600/67021\n",
      "8700/67021\n",
      "8800/67021\n",
      "8900/67021\n",
      "9000/67021\n",
      "9100/67021\n",
      "9200/67021\n",
      "9300/67021\n",
      "9400/67021\n",
      "9500/67021\n",
      "9600/67021\n",
      "9700/67021\n",
      "9800/67021\n",
      "9900/67021\n",
      "10000/67021\n",
      "10100/67021\n",
      "10200/67021\n",
      "10300/67021\n",
      "10400/67021\n",
      "10500/67021\n",
      "10600/67021\n",
      "10700/67021\n",
      "10800/67021\n",
      "10900/67021\n",
      "11000/67021\n",
      "11100/67021\n",
      "11200/67021\n",
      "11300/67021\n",
      "11400/67021\n",
      "11500/67021\n",
      "11600/67021\n",
      "11700/67021\n",
      "11800/67021\n",
      "11900/67021\n",
      "12000/67021\n",
      "12100/67021\n",
      "12200/67021\n",
      "12300/67021\n",
      "12400/67021\n",
      "12500/67021\n",
      "12600/67021\n",
      "12700/67021\n",
      "12800/67021\n",
      "12900/67021\n",
      "13000/67021\n",
      "13100/67021\n",
      "13200/67021\n",
      "13300/67021\n",
      "13400/67021\n",
      "13500/67021\n",
      "13600/67021\n",
      "13700/67021\n",
      "13800/67021\n",
      "13900/67021\n",
      "14000/67021\n",
      "14100/67021\n",
      "14200/67021\n",
      "14300/67021\n",
      "14400/67021\n",
      "14500/67021\n",
      "14600/67021\n",
      "14700/67021\n",
      "14800/67021\n",
      "14900/67021\n",
      "15000/67021\n",
      "15100/67021\n",
      "15200/67021\n",
      "15300/67021\n",
      "15400/67021\n",
      "15500/67021\n",
      "15600/67021\n",
      "15700/67021\n",
      "15800/67021\n",
      "15900/67021\n",
      "16000/67021\n",
      "16100/67021\n",
      "16200/67021\n",
      "16300/67021\n",
      "16400/67021\n",
      "16500/67021\n",
      "16600/67021\n",
      "16700/67021\n",
      "16800/67021\n",
      "16900/67021\n",
      "17000/67021\n",
      "17100/67021\n",
      "17200/67021\n",
      "17300/67021\n",
      "17400/67021\n",
      "17500/67021\n",
      "17600/67021\n",
      "17700/67021\n",
      "17800/67021\n",
      "17900/67021\n",
      "18000/67021\n",
      "18100/67021\n",
      "18200/67021\n",
      "18300/67021\n",
      "18400/67021\n",
      "18500/67021\n",
      "18600/67021\n",
      "18700/67021\n",
      "18800/67021\n",
      "18900/67021\n",
      "19000/67021\n",
      "19100/67021\n",
      "19200/67021\n",
      "19300/67021\n",
      "19400/67021\n",
      "19500/67021\n",
      "19600/67021\n",
      "19700/67021\n",
      "19800/67021\n",
      "19900/67021\n",
      "20000/67021\n",
      "20100/67021\n",
      "20200/67021\n",
      "20300/67021\n",
      "20400/67021\n",
      "20500/67021\n",
      "20600/67021\n",
      "20700/67021\n",
      "20800/67021\n",
      "20900/67021\n",
      "21000/67021\n",
      "21100/67021\n",
      "21200/67021\n",
      "21300/67021\n",
      "21400/67021\n",
      "21500/67021\n",
      "21600/67021\n",
      "21700/67021\n",
      "21800/67021\n",
      "21900/67021\n",
      "22000/67021\n",
      "22100/67021\n",
      "22200/67021\n",
      "22300/67021\n",
      "22400/67021\n",
      "22500/67021\n",
      "22600/67021\n",
      "22700/67021\n",
      "22800/67021\n",
      "22900/67021\n",
      "23000/67021\n",
      "23100/67021\n",
      "23200/67021\n",
      "23300/67021\n",
      "23400/67021\n",
      "23500/67021\n",
      "23600/67021\n",
      "23700/67021\n",
      "23800/67021\n",
      "23900/67021\n",
      "24000/67021\n",
      "24100/67021\n",
      "24200/67021\n",
      "24300/67021\n",
      "24400/67021\n",
      "24500/67021\n",
      "24600/67021\n",
      "24700/67021\n",
      "24800/67021\n",
      "24900/67021\n",
      "25000/67021\n",
      "25100/67021\n",
      "25200/67021\n",
      "25300/67021\n",
      "25400/67021\n",
      "25500/67021\n",
      "25600/67021\n",
      "25700/67021\n",
      "25800/67021\n",
      "25900/67021\n",
      "26000/67021\n",
      "26100/67021\n",
      "26200/67021\n",
      "26300/67021\n",
      "26400/67021\n",
      "26500/67021\n",
      "26600/67021\n",
      "26700/67021\n",
      "26800/67021\n",
      "26900/67021\n",
      "27000/67021\n",
      "27100/67021\n",
      "27200/67021\n",
      "27300/67021\n",
      "27400/67021\n",
      "27500/67021\n",
      "27600/67021\n",
      "27700/67021\n",
      "27800/67021\n",
      "27900/67021\n",
      "28000/67021\n",
      "28100/67021\n",
      "28200/67021\n",
      "28300/67021\n",
      "28400/67021\n",
      "28500/67021\n",
      "28600/67021\n",
      "28700/67021\n",
      "28800/67021\n",
      "28900/67021\n",
      "29000/67021\n",
      "29100/67021\n",
      "29200/67021\n",
      "29300/67021\n",
      "29400/67021\n",
      "29500/67021\n",
      "29600/67021\n",
      "29700/67021\n",
      "29800/67021\n",
      "29900/67021\n",
      "30000/67021\n",
      "30100/67021\n",
      "30200/67021\n",
      "30300/67021\n",
      "30400/67021\n",
      "30500/67021\n",
      "30600/67021\n",
      "30700/67021\n",
      "30800/67021\n",
      "30900/67021\n",
      "31000/67021\n",
      "31100/67021\n",
      "31200/67021\n",
      "31300/67021\n",
      "31400/67021\n",
      "31500/67021\n",
      "31600/67021\n",
      "31700/67021\n",
      "31800/67021\n",
      "31900/67021\n",
      "32000/67021\n",
      "32100/67021\n",
      "32200/67021\n",
      "32300/67021\n",
      "32400/67021\n",
      "32500/67021\n",
      "32600/67021\n",
      "32700/67021\n",
      "32800/67021\n",
      "32900/67021\n",
      "33000/67021\n",
      "33100/67021\n",
      "33200/67021\n",
      "33300/67021\n",
      "33400/67021\n",
      "33500/67021\n",
      "33600/67021\n",
      "33700/67021\n",
      "33800/67021\n",
      "33900/67021\n",
      "34000/67021\n",
      "34100/67021\n",
      "34200/67021\n",
      "34300/67021\n",
      "34400/67021\n",
      "34500/67021\n",
      "34600/67021\n",
      "34700/67021\n",
      "34800/67021\n",
      "34900/67021\n",
      "35000/67021\n",
      "35100/67021\n",
      "35200/67021\n",
      "35300/67021\n",
      "35400/67021\n",
      "35500/67021\n",
      "35600/67021\n",
      "35700/67021\n",
      "35800/67021\n",
      "35900/67021\n",
      "36000/67021\n",
      "36100/67021\n",
      "36200/67021\n",
      "36300/67021\n",
      "36400/67021\n",
      "36500/67021\n",
      "36600/67021\n",
      "36700/67021\n",
      "36800/67021\n",
      "36900/67021\n",
      "37000/67021\n",
      "37100/67021\n",
      "37200/67021\n",
      "37300/67021\n",
      "37400/67021\n",
      "37500/67021\n",
      "37600/67021\n",
      "37700/67021\n",
      "37800/67021\n",
      "37900/67021\n",
      "38000/67021\n",
      "38100/67021\n",
      "38200/67021\n",
      "38300/67021\n",
      "38400/67021\n",
      "38500/67021\n",
      "38600/67021\n",
      "38700/67021\n",
      "38800/67021\n",
      "38900/67021\n",
      "39000/67021\n",
      "39100/67021\n",
      "39200/67021\n",
      "39300/67021\n",
      "39400/67021\n",
      "39500/67021\n",
      "39600/67021\n",
      "39700/67021\n",
      "39800/67021\n",
      "39900/67021\n",
      "40000/67021\n",
      "40100/67021\n",
      "40200/67021\n",
      "40300/67021\n",
      "40400/67021\n",
      "40500/67021\n",
      "40600/67021\n",
      "40700/67021\n",
      "40800/67021\n",
      "40900/67021\n",
      "41000/67021\n",
      "41100/67021\n",
      "41200/67021\n",
      "41300/67021\n",
      "41400/67021\n",
      "41500/67021\n",
      "41600/67021\n",
      "41700/67021\n",
      "41800/67021\n",
      "41900/67021\n",
      "42000/67021\n",
      "42100/67021\n",
      "42200/67021\n",
      "42300/67021\n",
      "42400/67021\n",
      "42500/67021\n",
      "42600/67021\n",
      "42700/67021\n",
      "42800/67021\n",
      "42900/67021\n",
      "43000/67021\n",
      "43100/67021\n",
      "43200/67021\n",
      "43300/67021\n",
      "43400/67021\n",
      "43500/67021\n",
      "43600/67021\n",
      "43700/67021\n",
      "43800/67021\n",
      "43900/67021\n",
      "44000/67021\n",
      "44100/67021\n",
      "44200/67021\n",
      "44300/67021\n",
      "44400/67021\n",
      "44500/67021\n",
      "44600/67021\n",
      "44700/67021\n",
      "44800/67021\n",
      "44900/67021\n",
      "45000/67021\n",
      "45100/67021\n",
      "45200/67021\n",
      "45300/67021\n",
      "45400/67021\n",
      "45500/67021\n",
      "45600/67021\n",
      "45700/67021\n",
      "45800/67021\n",
      "45900/67021\n",
      "46000/67021\n",
      "46100/67021\n",
      "46200/67021\n",
      "46300/67021\n",
      "46400/67021\n",
      "46500/67021\n",
      "46600/67021\n",
      "46700/67021\n",
      "46800/67021\n",
      "46900/67021\n",
      "47000/67021\n",
      "47100/67021\n",
      "47200/67021\n",
      "47300/67021\n",
      "47400/67021\n",
      "47500/67021\n",
      "47600/67021\n",
      "47700/67021\n",
      "47800/67021\n",
      "47900/67021\n",
      "48000/67021\n",
      "48100/67021\n",
      "48200/67021\n",
      "48300/67021\n",
      "48400/67021\n",
      "48500/67021\n",
      "48600/67021\n",
      "48700/67021\n",
      "48800/67021\n",
      "48900/67021\n",
      "49000/67021\n",
      "49100/67021\n",
      "49200/67021\n",
      "49300/67021\n",
      "49400/67021\n",
      "49500/67021\n",
      "49600/67021\n",
      "49700/67021\n",
      "49800/67021\n",
      "49900/67021\n",
      "50000/67021\n",
      "50100/67021\n",
      "50200/67021\n",
      "50300/67021\n",
      "50400/67021\n",
      "50500/67021\n",
      "50600/67021\n",
      "50700/67021\n",
      "50800/67021\n",
      "50900/67021\n",
      "51000/67021\n",
      "51100/67021\n",
      "51200/67021\n",
      "51300/67021\n",
      "51400/67021\n",
      "51500/67021\n",
      "51600/67021\n",
      "51700/67021\n",
      "51800/67021\n",
      "51900/67021\n",
      "52000/67021\n",
      "52100/67021\n",
      "52200/67021\n",
      "52300/67021\n",
      "52400/67021\n",
      "52500/67021\n",
      "52600/67021\n",
      "52700/67021\n",
      "52800/67021\n",
      "52900/67021\n",
      "53000/67021\n",
      "53100/67021\n",
      "53200/67021\n",
      "53300/67021\n",
      "53400/67021\n",
      "53500/67021\n",
      "53600/67021\n",
      "53700/67021\n",
      "53800/67021\n",
      "53900/67021\n",
      "54000/67021\n",
      "54100/67021\n",
      "54200/67021\n",
      "54300/67021\n",
      "54400/67021\n",
      "54500/67021\n",
      "54600/67021\n",
      "54700/67021\n",
      "54800/67021\n",
      "54900/67021\n",
      "55000/67021\n",
      "55100/67021\n",
      "55200/67021\n",
      "55300/67021\n",
      "55400/67021\n",
      "55500/67021\n",
      "55600/67021\n",
      "55700/67021\n",
      "55800/67021\n",
      "55900/67021\n",
      "56000/67021\n",
      "56100/67021\n",
      "56200/67021\n",
      "56300/67021\n",
      "56400/67021\n",
      "56500/67021\n",
      "56600/67021\n",
      "56700/67021\n",
      "56800/67021\n",
      "56900/67021\n",
      "57000/67021\n",
      "57100/67021\n",
      "57200/67021\n",
      "57300/67021\n",
      "57400/67021\n",
      "57500/67021\n",
      "57600/67021\n",
      "57700/67021\n",
      "57800/67021\n",
      "57900/67021\n",
      "58000/67021\n",
      "58100/67021\n",
      "58200/67021\n",
      "58300/67021\n",
      "58400/67021\n",
      "58500/67021\n",
      "58600/67021\n",
      "58700/67021\n",
      "58800/67021\n",
      "58900/67021\n",
      "59000/67021\n",
      "59100/67021\n",
      "59200/67021\n",
      "59300/67021\n",
      "59400/67021\n",
      "59500/67021\n",
      "59600/67021\n",
      "59700/67021\n",
      "59800/67021\n",
      "59900/67021\n",
      "60000/67021\n",
      "60100/67021\n",
      "60200/67021\n",
      "60300/67021\n",
      "60400/67021\n",
      "60500/67021\n",
      "60600/67021\n",
      "60700/67021\n",
      "60800/67021\n",
      "60900/67021\n",
      "61000/67021\n",
      "61100/67021\n",
      "61200/67021\n",
      "61300/67021\n",
      "61400/67021\n",
      "61500/67021\n",
      "61600/67021\n",
      "61700/67021\n",
      "61800/67021\n",
      "61900/67021\n",
      "62000/67021\n",
      "62100/67021\n",
      "62200/67021\n",
      "62300/67021\n",
      "62400/67021\n",
      "62500/67021\n",
      "62600/67021\n",
      "62700/67021\n",
      "62800/67021\n",
      "62900/67021\n",
      "63000/67021\n",
      "63100/67021\n",
      "63200/67021\n",
      "63300/67021\n",
      "63400/67021\n",
      "63500/67021\n",
      "63600/67021\n",
      "63700/67021\n",
      "63800/67021\n",
      "63900/67021\n",
      "64000/67021\n",
      "64100/67021\n",
      "64200/67021\n",
      "64300/67021\n",
      "64400/67021\n",
      "64500/67021\n",
      "64600/67021\n",
      "64700/67021\n",
      "64800/67021\n",
      "64900/67021\n",
      "65000/67021\n",
      "65100/67021\n",
      "65200/67021\n",
      "65300/67021\n",
      "65400/67021\n",
      "65500/67021\n",
      "65600/67021\n",
      "65700/67021\n",
      "65800/67021\n",
      "65900/67021\n",
      "66000/67021\n",
      "66100/67021\n",
      "66200/67021\n",
      "66300/67021\n",
      "66400/67021\n",
      "66500/67021\n",
      "66600/67021\n",
      "66700/67021\n",
      "66800/67021\n",
      "66900/67021\n",
      "67000/67021\n",
      "67021\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = LexicalFeatureExtractor(context_window=(2, 2),\n",
    "                            ngram_window=(1, 3), vocab=vocab, pos_vocab=pos_vocab,\n",
    "                            min_vocab_count=20, min_pos_count=20)\n",
    "feat_dicts = create_feature_dicts(relations, docs)\n",
    "\n",
    "print(len(feat_dicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3_703'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relations[-1].file_name"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('tmp_data/feat_dicts.pkl', 'wb') as f:\n",
    "    pickle.dump(feat_dicts, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tmp_data/feat_dicts.pkl', 'rb') as f:\n",
    "    feat_dicts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "source": [
    "feature_extractor = LexicalFeatureExtractor(context_window=(2, 2),\n",
    "                            ngram_window=(1, 3), vocab=vocab, pos_vocab=pos_vocab,\n",
    "                            min_vocab_count=20, min_pos_count=20)\n",
    "feat_dicts = {feature_set_name: [] for feature_set_name in \n",
    "              ('entities', 'entities_between', 'surface', \n",
    "               'entities+entities_between', 'entities+entities_between+surface')}\n",
    "for i, r in enumerate(relations):\n",
    "    if i % 100 == 0:\n",
    "        print(\"{}/{}\".format(i, len(relations)))\n",
    "    doc = docs[r.file_name]\n",
    "    # Single feature sets\n",
    "    entities_feat_dict = feature_extractor.create_feature_dict(r, doc, entities=True, entities_between=False, surface=False)\n",
    "    entities_between_dict = feature_extractor.create_feature_dict(r, doc, entities=False, entities_between=True, surface=False)\n",
    "    surface_dict = feature_extractor.create_feature_dict(r, doc, entities=False, entities_between=False, surface=True)\n",
    "    feat_dicts['entities'].append(entities_feat_dict)\n",
    "    feat_dicts['entities_between'].append(entities_between_dict)\n",
    "    feat_dicts['surface'].append(surface_dict)\n",
    "    \n",
    "    # Now create the combinations\n",
    "    combo = {}\n",
    "    combo.update(entities_feat_dict)\n",
    "    combo.update(entities_between_dict)\n",
    "    \n",
    "    feat_dicts['entities+entities_between'].append(combo)\n",
    "    combo2 = {}\n",
    "    combo2.update(combo)\n",
    "    combo2.update(surface_dict)\n",
    "#     feat_dicts['entities+entities_between'].append(combo)\n",
    "    feat_dicts['entities+entities_between+surface'].append(combo2)\n",
    "#     break\n",
    "    continue\n",
    "    # Original code\n",
    "    \n",
    "    feat_dicts['entities'].append(feature_extractor.create_feature_dict(r, doc, entities=True, entities_between=False, surface=False))\n",
    "    feat_dicts['entities_between'].append(feature_extractor.create_feature_dict(r, doc, entities=False, entities_between=True, surface=False))\n",
    "    feat_dicts['surface'].append(feature_extractor.create_feature_dict(r, doc, entities=False, entities_between=False, surface=True))\n",
    "    \n",
    "    # Combinations\n",
    "    feat_dicts['entities+entities_between'].append(feature_extractor.create_feature_dict(r, doc, entities=True, entities_between=True, surface=False))\n",
    "    feat_dicts['entities+entities_between+surface'].append(feature_extractor.create_feature_dict(r, doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('tmp_data/feat_dicts', 'rb') as f:\n",
    "    feat_dicts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_in_anno1': 'atenolol',\n",
       " 'text_in_anno2': 'oral',\n",
       " 'concat_text': 'atenolol:oral',\n",
       " 'first_entity_type:<DRUG>': 1,\n",
       " 'second_entity_type:<ROUTE>': 1,\n",
       " 'entity_types_concat': '<DRUG><=><ROUTE>',\n",
       " 'entities_between:<DOSE>': 1,\n",
       " 'num_entities_between': 2,\n",
       " 'num_sentences_overlap': 1,\n",
       " 'num_tokens_between': 11,\n",
       " 'grams_between:<-- : by>': 1,\n",
       " 'grams_between:<by ordered>': 1,\n",
       " 'grams_between:<tablet>': 1,\n",
       " 'grams_between:<mg>': 1,\n",
       " 'grams_between:<, tablet>': 1,\n",
       " 'grams_between:<directions>': 1,\n",
       " 'grams_between:<- -- :>': 1,\n",
       " 'grams_between:<<NUMBER> mg tablet>': 1,\n",
       " 'grams_between:<<NUMBER> : tablet>': 1,\n",
       " 'grams_between:<: by>': 1,\n",
       " 'grams_between:<- -- -->': 1,\n",
       " 'grams_between:<, mg tablet>': 1,\n",
       " 'grams_between:<- - -->': 1,\n",
       " 'grams_between:<- : directions>': 1,\n",
       " 'grams_between:<<NUMBER> :>': 1,\n",
       " 'grams_between:<- directions>': 1,\n",
       " 'grams_between:<by>': 1,\n",
       " 'grams_between:<:>': 1,\n",
       " 'grams_between:<mg tablet>': 1,\n",
       " 'grams_between:<ordered>': 1,\n",
       " 'grams_between:<- name>': 1,\n",
       " 'grams_between:<<NUMBER> : directions>': 1,\n",
       " 'grams_between:<, ordered tablet>': 1,\n",
       " 'grams_between:<-- name>': 1,\n",
       " 'grams_between:<: directions>': 1,\n",
       " 'grams_between:<, ordered>': 1,\n",
       " 'grams_between:<,>': 1,\n",
       " 'grams_between:<-- :>': 1,\n",
       " 'grams_between:<<NUMBER> mg>': 1,\n",
       " 'grams_between:<->': 1,\n",
       " 'grams_between:<- -->': 1,\n",
       " 'grams_between:<- -- directions>': 1,\n",
       " 'grams_between:<<NUMBER>>': 1,\n",
       " 'grams_between:<name>': 1,\n",
       " 'grams_between:<-->': 1,\n",
       " 'grams_between:<- -- name>': 1,\n",
       " 'grams_between:<: by ordered>': 1,\n",
       " 'grams_between:<, by ordered>': 1,\n",
       " 'grams_between:<<NUMBER> tablet>': 1,\n",
       " 'grams_before:<oral>': 1,\n",
       " 'grams_before:<daily>': 1,\n",
       " 'grams_before:<daily oral>': 1,\n",
       " 'grams_after:<daily>': 1,\n",
       " 'grams_after:<additional>': 1,\n",
       " 'grams_after:<OOV>': 1,\n",
       " 'tags_between:<cd>': 1,\n",
       " 'tags_between:<nn nnp>': 1,\n",
       " 'tags_between:<: in>': 1,\n",
       " 'tags_between:<: nn>': 1,\n",
       " 'tags_between:<, nn nnp>': 1,\n",
       " 'tags_between:<, vbn>': 1,\n",
       " 'tags_between:<in vbn>': 1,\n",
       " 'tags_between:<cd nn>': 1,\n",
       " 'tags_between:<cd nn nnp>': 1,\n",
       " 'tags_between:<, nnp>': 1,\n",
       " 'tags_between:<: : :>': 1,\n",
       " 'tags_between:<nn>': 1,\n",
       " 'tags_between:<: : in>': 1,\n",
       " 'tags_between:<: :>': 1,\n",
       " 'tags_between:<:>': 1,\n",
       " 'tags_between:<: cd>': 1,\n",
       " 'tags_between:<in>': 1,\n",
       " 'tags_between:<, nnp vbn>': 1,\n",
       " 'tags_between:<: in vbn>': 1,\n",
       " 'tags_between:<, in vbn>': 1,\n",
       " 'tags_between:<: : nns>': 1,\n",
       " 'tags_between:<,>': 1,\n",
       " 'tags_between:<: : nn>': 1,\n",
       " 'tags_between:<: cd nns>': 1,\n",
       " 'tags_between:<: nns>': 1,\n",
       " 'tags_between:<vbn>': 1,\n",
       " 'tags_between:<: cd nn>': 1,\n",
       " 'tags_between:<nns>': 1,\n",
       " 'tags_between:<nnp>': 1,\n",
       " 'tags_before:<jj nnp>': 1,\n",
       " 'tags_before:<jj>': 1,\n",
       " 'tags_before:<nnp>': 1,\n",
       " 'tags_after:<nnp>': 1,\n",
       " 'tags_after:<nnp nnp>': 1,\n",
       " 'same_sentence': 1}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_dicts[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'atenolol':'oral', Drug:Route, type=manner/route"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relations[7]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Read in test data\n",
    "test_reader = made_utils.TextAndBioCParser(VALDIR)\n",
    "test_docs = test_reader.read_texts_and_xmls(num_docs=-1, include_relations=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "test_relations = []\n",
    "for i, (fname, doc) in enumerate(test_docs.items()):\n",
    "    if i % 10 == 0:\n",
    "        print('-{}: {} '.format(i, fname))\n",
    "        print(len(doc.relations))\n",
    "    new_relations = pair_annotations_in_doc(doc)\n",
    "    \n",
    "    # Add Fake relations for training\n",
    "    neg_relations = set(new_relations).difference(set(doc.relations))\n",
    "    doc.add_relations(neg_relations)\n",
    "    if i % 10 == 0:\n",
    "        print(len(doc.relations))\n",
    "        \n",
    "    test_relations += doc.relations\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open('tmp_data/val_docs_and_relations.pkl', 'wb') as f:\n",
    "    pickle.dump((test_docs, test_relations), f)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "source": [
    "# feature_extractor = LexicalFeatureExtractor(context_window=(2, 2),\n",
    "#                             ngram_window=(1, 3), vocab=vocab, pos_vocab=pos_vocab,\n",
    "#                             min_vocab_count=20, min_pos_count=20)\n",
    "feat_dicts_test = {feature_set_name: [] for feature_set_name in \n",
    "              ('entities', 'entities_between', 'surface', \n",
    "               'entities+entities_between', 'entities+entities_between+surface')}\n",
    "for i, r in enumerate(test_relations):\n",
    "    if i % 100 == 0:\n",
    "        print(\"{}/{}\".format(i, len(test_relations)))\n",
    "    doc = test_docs[r.file_name]\n",
    "    # Single feature sets\n",
    "    entities_feat_dict = feature_extractor.create_feature_dict(r, doc, entities=True, entities_between=False, surface=False)\n",
    "    entities_between_dict = feature_extractor.create_feature_dict(r, doc, entities=False, entities_between=True, surface=False)\n",
    "    surface_dict = feature_extractor.create_feature_dict(r, doc, entities=False, entities_between=False, surface=True)\n",
    "    feat_dicts_test['entities'].append(entities_feat_dict)\n",
    "    feat_dicts_test['entities_between'].append(entities_between_dict)\n",
    "    feat_dicts_test['surface'].append(surface_dict)\n",
    "    \n",
    "    # Now create the combinations\n",
    "    combo = {}\n",
    "    combo.update(entities_feat_dict)\n",
    "    combo.update(entities_between_dict)\n",
    "    \n",
    "    feat_dicts_test['entities+entities_between'].append(combo)\n",
    "    combo2 = {}\n",
    "    combo2.update(combo)\n",
    "    combo2.update(surface_dict)\n",
    "#     feat_dicts['entities+entities_between'].append(combo)\n",
    "    feat_dicts_test['entities+entities_between+surface'].append(combo2)\n",
    "#     break\n",
    "    continue\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "relations[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "feat_dicts['entities+entities_between+surface'][0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_relations[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "feat_dicts_test['entities+entities_between+surface'][0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "len(set([r.id for r in relations]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vectorizer = DictVectorizer(sparse=True, sort=True)\n",
    "# TODO: Freeze the 1000 features that we use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bin': None, 'full': None}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_full = [r.type for r in relations]\n",
    "y_bin = ['any' if y_ != 'none' else y_ for y_ in y_full]\n",
    "y_dict = {'bin': y_bin,\n",
    "    'full': y_full}\n",
    "X = {'bin': None, 'full': None}\n",
    "X"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y_full_val = [r.type for r in test_relations]\n",
    "y_bin_test = ['any' if y_ != 'none' else y_ for y_ in y_full_test]\n",
    "y_dict_test = {'bin': y_bin_test,\n",
    "    'full': y_full_test}\n",
    "X_dict_test = {feature_set_name: {'bin': None, 'full': None} for feature_set_name in feat_dicts_test.keys()}\n",
    "X_dict_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selectors = {'bin': None, 'full': None}\n",
    "# vectorizers = {'bin': None, 'full': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67021, 59479)\n",
      "(67021, 1000) (67021, 1000)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = DictVectorizer(sparse=True, sort=True)\n",
    "k=1000\n",
    "binary_feature_selector = base_feature.MyFeatureSelector(vectorizer, k=k)\n",
    "full_feature_selector = base_feature.MyFeatureSelector(vectorizer, k=k)\n",
    "\n",
    "# Fit the vectorizer and feature selectors, transform X\n",
    "X_vector = vectorizer.fit_transform(feat_dicts)\n",
    "print(X_vector.shape)\n",
    "try:\n",
    "    binary_feature_selector = base_feature.MyFeatureSelector(vectorizer, k=k)\n",
    "    X_bin = binary_feature_selector.fit_transform(X_vector, y_dict['bin'])\n",
    "\n",
    "\n",
    "    full_feature_selector = base_feature.MyFeatureSelector(vectorizer, k=k)\n",
    "    X_full = full_feature_selector.fit_transform(X_vector, y_dict['full']) \n",
    "except ValueError as e: # Not enough features\n",
    "#         vectorizer = DictVectorizer(sparse=True, sort=True)\n",
    "#         X_vector = vectorizer.fit_transform(features)\n",
    "    binary_feature_selector = base_feature.MyFeatureSelector(vectorizer, k='all')\n",
    "    X_bin = binary_feature_selector.fit_transform(X_vector, y_dict['bin'])\n",
    "\n",
    "    full_feature_selector = base_feature.MyFeatureSelector(vectorizer, k='all')\n",
    "    X_full = full_feature_selector.fit_transform(X_vector, y_dict['full']) \n",
    "    \n",
    "print(X_bin.shape, X_full.shape)\n",
    "\n",
    "\n",
    "feature_selectors['bin'] = binary_feature_selector\n",
    "feature_selectors['full'] = full_feature_selector"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Now transform the test feature dicts\n",
    "\n",
    "for feature_set_name, features in feat_dicts_test.items():\n",
    "    print(feature_set_name)\n",
    "\n",
    "    binary_feature_selector = feature_selectors[feature_set_name]['bin']\n",
    "    full_feature_selector = feature_selectors[feature_set_name]['full']\n",
    "    print(len(binary_feature_selector.vectorizer.get_feature_names()))\n",
    "#     break\n",
    "    \n",
    "    try:\n",
    "        X_bin = binary_feature_selector.vectorizer.transform(features)\n",
    "        print(X_bin.shape)\n",
    "        X_bin = binary_feature_selector.transform(X_bin)\n",
    "        X_full = full_feature_selector.vectorizer.transform(features)\n",
    "        X_full = full_feature_selector.transform(X_full) \n",
    "    except ValueError as e: # Not enough features\n",
    "        raise e\n",
    "       \n",
    "    \n",
    "    \n",
    "    X_dict_test[feature_set_name]['bin'] = X_bin\n",
    "    X_dict_test[feature_set_name]['full'] = X_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open('tmp_data/X_dicts.pkl', 'wb') as f:\n",
    "    pickle.dump((X_dict, X_dict_test), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open('tmp_data/X_dict_test.pkl', 'wb') as f:\n",
    "    pickle.dump(X_dict, f)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "with open('tmp_data/X_dicts.pkl', 'rb') as f:\n",
    "    X_dict, X_dict_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['none',\n",
       " 'do',\n",
       " 'none',\n",
       " 'severity_type',\n",
       " 'none',\n",
       " 'du',\n",
       " 'manner/route',\n",
       " 'none',\n",
       " 'fr',\n",
       " 'do',\n",
       " 'none',\n",
       " 'du',\n",
       " 'manner/route',\n",
       " 'do',\n",
       " 'fr',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'fr',\n",
       " 'do',\n",
       " 'manner/route',\n",
       " 'du',\n",
       " 'du',\n",
       " 'manner/route',\n",
       " 'fr',\n",
       " 'do',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'none',\n",
       " 'do',\n",
       " 'do',\n",
       " 'none',\n",
       " 'do',\n",
       " 'fr',\n",
       " 'reason',\n",
       " 'fr',\n",
       " 'du',\n",
       " 'fr',\n",
       " 'fr',\n",
       " 'none',\n",
       " 'manner/route',\n",
       " 'severity_type',\n",
       " 'manner/route',\n",
       " 'none',\n",
       " 'fr',\n",
       " 'do',\n",
       " 'do',\n",
       " 'none',\n",
       " 'do',\n",
       " 'do',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'severity_type',\n",
       " 'do',\n",
       " 'none',\n",
       " 'manner/route',\n",
       " 'none',\n",
       " 'do',\n",
       " 'fr',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'do',\n",
       " 'du',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'adverse',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'du',\n",
       " 'reason',\n",
       " 'reason',\n",
       " 'manner/route',\n",
       " 'none',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'severity_type',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'do',\n",
       " 'do',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'none',\n",
       " 'du',\n",
       " 'do',\n",
       " 'none',\n",
       " 'du',\n",
       " 'reason',\n",
       " 'do',\n",
       " 'severity_type',\n",
       " 'fr',\n",
       " 'du',\n",
       " 'du',\n",
       " 'manner/route',\n",
       " 'none',\n",
       " 'none',\n",
       " 'severity_type',\n",
       " 'manner/route',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'none',\n",
       " 'do',\n",
       " 'none',\n",
       " 'severity_type',\n",
       " 'none',\n",
       " 'severity_type',\n",
       " 'none',\n",
       " 'do',\n",
       " 'fr',\n",
       " 'none',\n",
       " 'adverse',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'severity_type',\n",
       " 'severity_type',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'manner/route',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'reason',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'none',\n",
       " 'severity_type',\n",
       " 'do',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'fr',\n",
       " 'severity_type',\n",
       " 'none',\n",
       " 'none',\n",
       " 'severity_type',\n",
       " 'manner/route',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'severity_type',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'severity_type',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'du',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'du',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'du',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'du',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'severity_type',\n",
       " 'severity_type',\n",
       " 'none',\n",
       " 'severity_type',\n",
       " 'none',\n",
       " 'severity_type',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none']"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can train and evaluate each set\n",
    "def train_clf(X, y, cross_val=False):\n",
    "    \"\"\"\n",
    "    Trains and validates a model using cross-validation.\n",
    "    \"\"\"\n",
    "    clf = RandomForestClassifier(max_depth = None,\n",
    "                            max_features = None,\n",
    "                            min_samples_leaf = 2,\n",
    "                            min_samples_split = 2,\n",
    "                            n_estimators = 10,\n",
    "                            n_jobs = 1)\n",
    "    print(X.shape)\n",
    "    if cross_val:\n",
    "        # Cross-validate to make sure this is going right\n",
    "        pred = cross_val_predict(clf, X, y, verbose=1)\n",
    "        print(classification_report(y, pred))\n",
    "#     clf.fit(X, y)\n",
    "    else:\n",
    "        clf.fit(X, y)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67021, 1000)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        any       0.95      0.94      0.94     23165\n",
      "       none       0.97      0.97      0.97     43856\n",
      "\n",
      "avg / total       0.96      0.96      0.96     67021\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  3.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=2, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Binary\n",
    "train_clf(X_bin, y_bin, cross_val=True)\n",
    "\n",
    "# Full\n",
    "train_clf(X_full, y_full, cross_val=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save clfs, vectorizers, feature_selectors\n",
    "with open('tmp_data/training_data.pkl', 'wb') as f:\n",
    "    pickle.dump(((X_bin, y_bin), (X_full, y_full)), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tmp_data/vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "    \n",
    "with open('tmp_data/feature_selectors.pkl', 'wb') as f:\n",
    "    pickle.dump((binary_feature_selector, full_feature_selector), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7038\n",
      "'BCNU':'infusion', Drug:Route, type=manner/route\n",
      "'chemotherapy':'infusion', Drug:Route, type=none\n",
      "\n",
      "840\n",
      "'chemotherapy':'bilateral shingles', Drug:Indication, type=none\n",
      "'Ativan':'nausea', Drug:Indication, type=reason\n",
      "\n",
      "1951\n",
      "'posaconazole':'neutropenia', Drug:Indication, type=none\n",
      "'ACETAMINOPHEN':'325MG', Drug:Dose, type=do\n",
      "\n",
      "4546\n",
      "'white count of about 27,800, which decreased':'mild', SSLIF:Severity, type=none\n",
      "'Folic acid':'daily', Drug:Frequency, type=fr\n",
      "\n",
      "2220\n",
      "'ciprofloxacin':'200 mg per 5 mL', Drug:Dose, type=none\n",
      "'magnesium oxide':'daily', Drug:Frequency, type=fr\n",
      "\n",
      "2221\n",
      "'ciprofloxacin':'3 times daily', Drug:Frequency, type=none\n",
      "'Remeron':'15 mg', Drug:Dose, type=do\n",
      "\n",
      "1918\n",
      "'posaconazole':'3 times daily', Drug:Frequency, type=none\n",
      "'gammaglobulin':'intravenous', Drug:Route, type=manner/route\n",
      "\n",
      "2251\n",
      "'ciprofloxacin':'neutropenia', Drug:Indication, type=none\n",
      "'decrease \\nsensation to LLE':'mild', SSLIF:Severity, type=severity_type\n",
      "\n",
      "1620\n",
      "'Valtrex':'200 mg per 5 mL', Drug:Dose, type=none\n",
      "'OXYCODONE HCL SR':'Twice a Day', Drug:Frequency, type=fr\n",
      "\n",
      "1623\n",
      "'Valtrex':'500 mg', Drug:Dose, type=none\n",
      "'CALAN SR':'Every Day', Drug:Frequency, type=fr\n",
      "\n",
      "1624\n",
      "'Valtrex':'daily', Drug:Frequency, type=none\n",
      "'CALAN SR':'PO', Drug:Route, type=manner/route\n",
      "\n",
      "1625\n",
      "'Valtrex':'p.r.n.', Drug:Frequency, type=none\n",
      "'VERAPAMIL HCL SR':'240 MG', Drug:Dose, type=do\n",
      "\n",
      "3735\n",
      "'hyper-CVAD':'mantle cell lymphoma', Drug:Indication, type=none\n",
      "'DILTIAZEM HCL XR':'Every Day', Drug:Frequency, type=fr\n",
      "\n",
      "4442\n",
      "'hyper-CVAD':'mantle cell lymphoma', Drug:Indication, type=none\n",
      "'ACETYLCYSTEINE':'3 ML', Drug:Dose, type=do\n",
      "\n",
      "4474\n",
      "'hyper-CVAD':'prophylaxis', Drug:Indication, type=none\n",
      "'PEPCID':'20 MG', Drug:Dose, type=do\n",
      "\n",
      "4475\n",
      "'hyper-CVAD':'tumor \\nlysis syndrome', Drug:Indication, type=none\n",
      "'PEPCID':'Twice a Day', Drug:Frequency, type=fr\n",
      "\n",
      "7242\n",
      "'uric acid':'mantle cell lymphoma', Drug:Indication, type=none\n",
      "'Dilaudid':'2 mg', Drug:Dose, type=do\n",
      "\n",
      "873\n",
      "'chemotherapy':'daily', Drug:Frequency, type=none\n",
      "'steroid':'pain', Drug:Indication, type=reason\n",
      "\n",
      "874\n",
      "'chemotherapy':'prophylaxis', Drug:Indication, type=none\n",
      "'steroid':'injection', Drug:Route, type=manner/route\n",
      "\n",
      "1046\n",
      "'vomiting':'mild', SSLIF:Severity, type=none\n",
      "'gammaglobulin':'intravenous', Drug:Route, type=manner/route\n",
      "\n",
      "1520\n",
      "'neutropenia medication':'200 mg per 5 mL', Drug:Dose, type=none\n",
      "'non-Hodgkin's lymphoma Burkitt':'non-Hodgkin's lymphoma Burkitt', Indication:Indication, type=reason\n",
      "\n",
      "1524\n",
      "'neutropenia medication':'daily', Drug:Frequency, type=none\n",
      "'Burkitt non-Hodgkin's lymphoma':'stage III', Indication:Severity, type=severity_type\n",
      "\n",
      "1525\n",
      "'neutropenia medication':'p.r.n.', Drug:Frequency, type=none\n",
      "'chemotherapy':'Burkitt non-Hodgkin's lymphoma', Drug:Indication, type=reason\n",
      "\n",
      "7142\n",
      "'chemotherapy':'mantle cell lymphoma', Drug:Indication, type=none\n",
      "'VITAMIN B COMPLEX':'PO', Drug:Route, type=manner/route\n",
      "\n",
      "7174\n",
      "'chemotherapy':'prophylaxis', Drug:Indication, type=none\n",
      "'OXYCODONE HCL/APAP':'PRN', Drug:Frequency, type=fr\n",
      "\n",
      "1923\n",
      "'posaconazole':'500 mg', Drug:Dose, type=none\n",
      "'Ritaxan':'4 weeks', Drug:Duration, type=du\n",
      "\n",
      "1924\n",
      "'posaconazole':'daily', Drug:Frequency, type=none\n",
      "'Ritaxan':'weekly', Drug:Frequency, type=fr\n",
      "\n"
     ]
    }
   ],
   "source": [
    "acc = {}\n",
    "for i, r in enumerate(relations):\n",
    "    if r.id in acc:\n",
    "        print(r.id)\n",
    "        print(acc[r.id])\n",
    "        print(r)\n",
    "        print()\n",
    "    else:\n",
    "        acc[r.id] = r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67021, 1000)\n",
      "(67021, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Now let's train on all of the data\n",
    "# Binary\n",
    "clf_bin = train_clf(X_bin, y_bin, cross_val=False)\n",
    "\n",
    "# Full\n",
    "clf_full = train_clf(X_full, y_full, cross_val=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=2, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/213\n",
      "100/213\n",
      "200/213\n"
     ]
    }
   ],
   "source": [
    "# Read in the test data\n",
    "test_reader = made_utils.TextAndBioCParser(TESTDIR)\n",
    "test_docs = test_reader.read_texts_and_xmls(-1, include_relations=False) # TODO: Change to -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tmp_data/test_docs.pkl', 'wb') as f:\n",
    "    pickle.dump(test_docs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0: 18_821 \n",
      "347\n",
      "347\n",
      "-10: 3_922 \n",
      "8\n",
      "8\n",
      "-20: 21_952 \n",
      "44\n",
      "44\n",
      "-30: 18_999 \n",
      "412\n",
      "412\n",
      "-40: 20_888 \n",
      "12\n",
      "12\n",
      "-50: 18_313 \n",
      "0\n",
      "0\n",
      "-60: 14_1072 \n",
      "206\n",
      "206\n",
      "-70: 11_1024 \n",
      "0\n",
      "0\n",
      "-80: 18_454 \n",
      "324\n",
      "324\n",
      "-90: 18_693 \n",
      "3\n",
      "3\n",
      "-100: 20_935 \n",
      "0\n",
      "0\n",
      "-110: 11_587 \n",
      "119\n",
      "119\n",
      "-120: 1_1069 \n",
      "349\n",
      "349\n",
      "-130: 18_1064 \n",
      "213\n",
      "213\n",
      "-140: 13_1085 \n",
      "13\n",
      "13\n",
      "-150: 18_926 \n",
      "519\n",
      "519\n",
      "-160: 11_243 \n",
      "3\n",
      "3\n",
      "-170: 4_1082 \n",
      "28\n",
      "28\n",
      "-180: 18_698 \n",
      "324\n",
      "324\n",
      "-190: 20_775 \n",
      "36\n",
      "36\n",
      "-200: 20_975 \n",
      "34\n",
      "34\n",
      "-210: 11_534 \n",
      "13\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "test_relations = []\n",
    "for i, (fname, doc) in enumerate(test_docs.items()):\n",
    "    if i  % 10 == 0:\n",
    "        print('-{}: {} '.format(i, fname))\n",
    "        print(len(doc.relations))\n",
    "    doc.relations = []\n",
    "    possible_relations = pair_annotations_in_doc(doc)\n",
    "    \n",
    "        \n",
    "    doc.add_relations(possible_relations)\n",
    "    \n",
    "    test_relations.extend(doc.get_relations())\n",
    "    if i  % 10 == 0:\n",
    "        print(len(doc.get_relations()))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tmp_data/test_docs_and_relations.pkl', 'wb') as f:\n",
    "    pickle.dump((test_relations, test_docs), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/23266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alec/anaconda/envs/made/lib/python3.6/site-packages/ipykernel_launcher.py:182: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
      "/Users/alec/anaconda/envs/made/lib/python3.6/site-packages/ipykernel_launcher.py:232: DeprecationWarning: generator 'ngrams' raised StopIteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/23266\n",
      "200/23266\n",
      "300/23266\n",
      "400/23266\n",
      "500/23266\n",
      "600/23266\n",
      "700/23266\n",
      "800/23266\n",
      "900/23266\n",
      "1000/23266\n",
      "1100/23266\n",
      "1200/23266\n",
      "1300/23266\n",
      "1400/23266\n",
      "1500/23266\n",
      "1600/23266\n",
      "1700/23266\n",
      "1800/23266\n",
      "1900/23266\n",
      "2000/23266\n",
      "2100/23266\n",
      "2200/23266\n",
      "2300/23266\n",
      "2400/23266\n",
      "2500/23266\n",
      "2600/23266\n",
      "2700/23266\n",
      "2800/23266\n",
      "2900/23266\n",
      "3000/23266\n",
      "3100/23266\n",
      "3200/23266\n",
      "3300/23266\n",
      "3400/23266\n",
      "3500/23266\n",
      "3600/23266\n",
      "3700/23266\n",
      "3800/23266\n",
      "3900/23266\n",
      "4000/23266\n",
      "4100/23266\n",
      "4200/23266\n",
      "4300/23266\n",
      "4400/23266\n",
      "4500/23266\n",
      "4600/23266\n",
      "4700/23266\n",
      "4800/23266\n",
      "4900/23266\n",
      "5000/23266\n",
      "5100/23266\n",
      "5200/23266\n",
      "5300/23266\n",
      "5400/23266\n",
      "5500/23266\n",
      "5600/23266\n",
      "5700/23266\n",
      "5800/23266\n",
      "5900/23266\n",
      "6000/23266\n",
      "6100/23266\n",
      "6200/23266\n",
      "6300/23266\n",
      "6400/23266\n",
      "6500/23266\n",
      "6600/23266\n",
      "6700/23266\n",
      "6800/23266\n",
      "6900/23266\n",
      "7000/23266\n",
      "7100/23266\n",
      "7200/23266\n",
      "7300/23266\n",
      "7400/23266\n",
      "7500/23266\n",
      "7600/23266\n",
      "7700/23266\n",
      "7800/23266\n",
      "7900/23266\n",
      "8000/23266\n",
      "8100/23266\n",
      "8200/23266\n",
      "8300/23266\n",
      "8400/23266\n",
      "8500/23266\n",
      "8600/23266\n",
      "8700/23266\n",
      "8800/23266\n",
      "8900/23266\n",
      "9000/23266\n",
      "9100/23266\n",
      "9200/23266\n",
      "9300/23266\n",
      "9400/23266\n",
      "9500/23266\n",
      "9600/23266\n",
      "9700/23266\n",
      "9800/23266\n",
      "9900/23266\n",
      "10000/23266\n",
      "10100/23266\n",
      "10200/23266\n",
      "10300/23266\n",
      "10400/23266\n",
      "10500/23266\n",
      "10600/23266\n",
      "10700/23266\n",
      "10800/23266\n",
      "10900/23266\n",
      "11000/23266\n",
      "11100/23266\n",
      "11200/23266\n",
      "11300/23266\n",
      "11400/23266\n",
      "11500/23266\n",
      "11600/23266\n",
      "11700/23266\n",
      "11800/23266\n",
      "11900/23266\n",
      "12000/23266\n",
      "12100/23266\n",
      "12200/23266\n",
      "12300/23266\n",
      "12400/23266\n",
      "12500/23266\n",
      "12600/23266\n",
      "12700/23266\n",
      "12800/23266\n",
      "12900/23266\n",
      "13000/23266\n",
      "13100/23266\n",
      "13200/23266\n",
      "13300/23266\n",
      "13400/23266\n",
      "13500/23266\n",
      "13600/23266\n",
      "13700/23266\n",
      "13800/23266\n",
      "13900/23266\n",
      "14000/23266\n",
      "14100/23266\n",
      "14200/23266\n",
      "14300/23266\n",
      "14400/23266\n",
      "14500/23266\n",
      "14600/23266\n",
      "14700/23266\n",
      "14800/23266\n",
      "14900/23266\n",
      "15000/23266\n",
      "15100/23266\n",
      "15200/23266\n",
      "15300/23266\n",
      "15400/23266\n",
      "15500/23266\n",
      "15600/23266\n",
      "15700/23266\n",
      "15800/23266\n",
      "15900/23266\n",
      "16000/23266\n",
      "16100/23266\n",
      "16200/23266\n",
      "16300/23266\n",
      "16400/23266\n",
      "16500/23266\n",
      "16600/23266\n",
      "16700/23266\n",
      "16800/23266\n",
      "16900/23266\n",
      "17000/23266\n",
      "17100/23266\n",
      "17200/23266\n",
      "17300/23266\n",
      "17400/23266\n",
      "17500/23266\n",
      "17600/23266\n",
      "17700/23266\n",
      "17800/23266\n",
      "17900/23266\n",
      "18000/23266\n",
      "18100/23266\n",
      "18200/23266\n",
      "18300/23266\n",
      "18400/23266\n",
      "18500/23266\n",
      "18600/23266\n",
      "18700/23266\n",
      "18800/23266\n",
      "18900/23266\n",
      "19000/23266\n",
      "19100/23266\n",
      "19200/23266\n",
      "19300/23266\n",
      "19400/23266\n",
      "19500/23266\n",
      "19600/23266\n",
      "19700/23266\n",
      "19800/23266\n",
      "19900/23266\n",
      "20000/23266\n",
      "20100/23266\n",
      "20200/23266\n",
      "20300/23266\n",
      "20400/23266\n",
      "20500/23266\n",
      "20600/23266\n",
      "20700/23266\n",
      "20800/23266\n",
      "20900/23266\n",
      "21000/23266\n",
      "21100/23266\n",
      "21200/23266\n",
      "21300/23266\n",
      "21400/23266\n",
      "21500/23266\n",
      "21600/23266\n",
      "21700/23266\n",
      "21800/23266\n",
      "21900/23266\n",
      "22000/23266\n",
      "22100/23266\n",
      "22200/23266\n",
      "22300/23266\n",
      "22400/23266\n",
      "22500/23266\n",
      "22600/23266\n",
      "22700/23266\n",
      "22800/23266\n",
      "22900/23266\n",
      "23000/23266\n",
      "23100/23266\n",
      "23200/23266\n"
     ]
    }
   ],
   "source": [
    "# Create feature_dicts\n",
    "test_feat_dicts = create_feature_dicts(test_relations, test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = test_relations[-1]\n",
    "feat_dict = test_feat_dicts[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DIPHENHYDRAMINE HCL':'DAY OF CHEMOTHERAPY', Drug:Frequency, type=none"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_in_anno1': 'diphenhydramine hcl',\n",
       " 'text_in_anno2': 'day of chemotherapy',\n",
       " 'concat_text': 'diphenhydramine hcl:day of chemotherapy',\n",
       " 'first_entity_type:<DRUG>': 1,\n",
       " 'second_entity_type:<FREQUENCY>': 1,\n",
       " 'entity_types_concat': '<FREQUENCY><=><DRUG>',\n",
       " 'entities_between:<DRUG>': 1,\n",
       " 'entities_between:<DOSE>': 1,\n",
       " 'entities_between:<FREQUENCY>': 1,\n",
       " 'entities_between:<ROUTE>': 1,\n",
       " 'entities_between:<INDICATION>': 1,\n",
       " 'num_entities_between': 17,\n",
       " 'num_sentences_overlap': 3,\n",
       " 'num_tokens_between': 53,\n",
       " 'grams_between:<to>': 1,\n",
       " 'grams_between:<: medication>': 1,\n",
       " 'grams_between:<# mr>': 1,\n",
       " 'grams_between:<: patient>': 1,\n",
       " 'grams_between:<- -- patient>': 1,\n",
       " 'grams_between:<every>': 1,\n",
       " 'grams_between:<ud>': 1,\n",
       " 'grams_between:<hospital medicine>': 1,\n",
       " 'grams_between:<morning>': 1,\n",
       " 'grams_between:<-<NUMBER>>': 1,\n",
       " 'grams_between:<<NUMBER> mg po>': 1,\n",
       " 'grams_between:<-- medical_record_number>': 1,\n",
       " 'grams_between:<#>': 1,\n",
       " 'grams_between:<-- :>': 1,\n",
       " 'grams_between:<,>': 1,\n",
       " 'grams_between:<po>': 1,\n",
       " 'grams_between:<<NUMBER> mg>': 1,\n",
       " 'grams_between:<chemotherapy>': 1,\n",
       " 'grams_between:<.>': 1,\n",
       " 'grams_between:<hcl>': 1,\n",
       " 'grams_between:<medical_record_number>': 1,\n",
       " 'grams_between:<- medical_record_number>': 1,\n",
       " 'grams_between:<- : patient>': 1,\n",
       " 'grams_between:<once>': 1,\n",
       " 'grams_between:<for this>': 1,\n",
       " 'grams_between:<<NUMBER> tablet>': 1,\n",
       " 'grams_between:<mg po>': 1,\n",
       " 'grams_between:<that>': 1,\n",
       " 'grams_between:<tablet>': 1,\n",
       " 'grams_between:<reason>': 1,\n",
       " 'grams_between:<note>': 1,\n",
       " 'grams_between:<discharge summary>': 1,\n",
       " 'grams_between:<-- : patient>': 1,\n",
       " 'grams_between:<, ->': 1,\n",
       " 'grams_between:<<NUMBER>mg>': 1,\n",
       " 'grams_between:<month>': 1,\n",
       " 'grams_between:<- patient>': 1,\n",
       " 'grams_between:<# :>': 1,\n",
       " 'grams_between:<for>': 1,\n",
       " 'grams_between:<first>': 1,\n",
       " 'grams_between:<for medication this>': 1,\n",
       " 'grams_between:<- hospital>': 1,\n",
       " 'grams_between:<. chemotherapy>': 1,\n",
       " 'grams_between:<has increased>': 1,\n",
       " 'grams_between:<- name>': 1,\n",
       " 'grams_between:<medication this>': 1,\n",
       " 'grams_between:<# -- :>': 1,\n",
       " 'grams_between:<-->': 1,\n",
       " 'grams_between:<take>': 1,\n",
       " 'grams_between:<days>': 1,\n",
       " 'grams_between:<patient>': 1,\n",
       " 'grams_between:<then>': 1,\n",
       " 'grams_between:<mr>': 1,\n",
       " 'grams_between:<mg>': 1,\n",
       " 'grams_between:<- -- hospital>': 1,\n",
       " 'grams_between:<that your>': 1,\n",
       " 'grams_between:<medicine>': 1,\n",
       " 'grams_between:<increased>': 1,\n",
       " 'grams_between:<on>': 1,\n",
       " 'grams_between:<day>': 1,\n",
       " 'grams_between:<birth>': 1,\n",
       " 'grams_between:<days <NUMBER>>': 1,\n",
       " 'grams_between:<: medication this>': 1,\n",
       " 'grams_between:<your>': 1,\n",
       " 'grams_between:<for reason>': 1,\n",
       " 'grams_between:<has>': 1,\n",
       " 'grams_between:<discharge>': 1,\n",
       " 'grams_between:<# : mr>': 1,\n",
       " 'grams_between:<daily>': 1,\n",
       " 'grams_between:<following>': 1,\n",
       " 'grams_between:<<NUMBER>>': 1,\n",
       " 'grams_between:<- -- name>': 1,\n",
       " 'grams_between:<increased to>': 1,\n",
       " 'grams_between:<- -- medical_record_number>': 1,\n",
       " 'grams_between:<for reason this>': 1,\n",
       " 'grams_between:<: note>': 1,\n",
       " 'grams_between:<control>': 1,\n",
       " 'grams_between:<, -->': 1,\n",
       " 'grams_between:<this>': 1,\n",
       " 'grams_between:<hospital>': 1,\n",
       " 'grams_between:<- -- :>': 1,\n",
       " 'grams_between:<<NUMBER> mg tablet>': 1,\n",
       " 'grams_between:<, - -->': 1,\n",
       " 'grams_between:<comment>': 1,\n",
       " 'grams_between:<day every>': 1,\n",
       " 'grams_between:<per>': 1,\n",
       " 'grams_between:<: comment>': 1,\n",
       " 'grams_between:<tablet ud>': 1,\n",
       " 'grams_between:<dose>': 1,\n",
       " 'grams_between:<-- hospital>': 1,\n",
       " 'grams_between:<:>': 1,\n",
       " 'grams_between:<in>': 1,\n",
       " 'grams_between:<-- name>': 1,\n",
       " 'grams_between:<- -->': 1,\n",
       " 'grams_between:<->': 1,\n",
       " 'grams_between:<name>': 1,\n",
       " 'grams_between:<summary>': 1,\n",
       " 'grams_between:<- hospital medicine>': 1,\n",
       " 'grams_between:<: birth>': 1,\n",
       " 'grams_between:<OOV>': 1,\n",
       " 'grams_between:<daps<NUMBER>>': 1,\n",
       " 'grams_between:<medication>': 1,\n",
       " 'grams_between:<. then>': 1,\n",
       " 'grams_before:<<NUMBER>mg>': 1,\n",
       " 'grams_before:<take>': 1,\n",
       " 'grams_before:<OOV>': 1,\n",
       " 'grams_after:<every>': 1,\n",
       " 'grams_after:<<NUMBER>>': 1,\n",
       " 'grams_after:<every <NUMBER>>': 1,\n",
       " 'tags_between:<cd>': 1,\n",
       " 'tags_between:<nn nnp>': 1,\n",
       " 'tags_between:<cd nnp>': 1,\n",
       " 'tags_between:<# nnp nnp>': 1,\n",
       " 'tags_between:<: nn>': 1,\n",
       " 'tags_between:<: dt nn>': 1,\n",
       " 'tags_between:<: nnp>': 1,\n",
       " 'tags_between:<in nnp nnp>': 1,\n",
       " 'tags_between:<# :>': 1,\n",
       " 'tags_between:<# : nnp>': 1,\n",
       " 'tags_between:<rb>': 1,\n",
       " 'tags_between:<dt nn>': 1,\n",
       " 'tags_between:<, :>': 1,\n",
       " 'tags_between:<: : :>': 1,\n",
       " 'tags_between:<nn>': 1,\n",
       " 'tags_between:<jj nnp nnp>': 1,\n",
       " 'tags_between:<nnp nnp nnp>': 1,\n",
       " 'tags_between:<nnp nnp rb>': 1,\n",
       " 'tags_between:<: :>': 1,\n",
       " 'tags_between:<. nnp nnp>': 1,\n",
       " 'tags_between:<dt in nnp>': 1,\n",
       " 'tags_between:<:>': 1,\n",
       " 'tags_between:<in nn>': 1,\n",
       " 'tags_between:<in nnp>': 1,\n",
       " 'tags_between:<: nn nnp>': 1,\n",
       " 'tags_between:<in>': 1,\n",
       " 'tags_between:<: nnp nnp>': 1,\n",
       " 'tags_between:<cd jj nnp>': 1,\n",
       " 'tags_between:<nnp nnp>': 1,\n",
       " 'tags_between:<#>': 1,\n",
       " 'tags_between:<jj>': 1,\n",
       " 'tags_between:<nnp rb>': 1,\n",
       " 'tags_between:<# nnp>': 1,\n",
       " 'tags_between:<,>': 1,\n",
       " 'tags_between:<: in nn>': 1,\n",
       " 'tags_between:<: : nn>': 1,\n",
       " 'tags_between:<cd rb>': 1,\n",
       " 'tags_between:<dt in nn>': 1,\n",
       " 'tags_between:<jj nnp>': 1,\n",
       " 'tags_between:<: jj>': 1,\n",
       " 'tags_between:<.>': 1,\n",
       " 'tags_between:<nn nnp nnp>': 1,\n",
       " 'tags_between:<. cd nnp>': 1,\n",
       " 'tags_between:<cd nnp rb>': 1,\n",
       " 'tags_between:<cd nnp nnp>': 1,\n",
       " 'tags_between:<dt>': 1,\n",
       " 'tags_between:<in nn nnp>': 1,\n",
       " 'tags_between:<dt in>': 1,\n",
       " 'tags_between:<: : nnp>': 1,\n",
       " 'tags_between:<: : jj>': 1,\n",
       " 'tags_between:<, : :>': 1,\n",
       " 'tags_between:<. nnp>': 1,\n",
       " 'tags_between:<# : :>': 1,\n",
       " 'tags_between:<nnp>': 1,\n",
       " 'tags_before:<cd>': 1,\n",
       " 'tags_before:<nnp>': 1,\n",
       " 'tags_before:<cd nnp>': 1,\n",
       " 'tags_after:<nnp>': 1,\n",
       " 'tags_after:<nnp nnp>': 1,\n",
       " 'same_sentence': 0}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23266, 1000)\n",
      "(23266, 1000)\n"
     ]
    }
   ],
   "source": [
    "X_test = vectorizer.transform(test_feat_dicts)\n",
    "X_test_bin = binary_feature_selector.transform(X_test)\n",
    "X_test_full = full_feature_selector.transform(X_test)\n",
    "print(X_test_bin.shape)\n",
    "print(X_test_full.shape)\n",
    "\n",
    "\n",
    "\n",
    "y_pred_bin = clf_bin.predict(X_test_bin)\n",
    "y_pred_full = clf_full.predict(X_test_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['none', 'none', 'none', ..., 'any', 'none', 'none'], dtype='<U4')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['none', 'none', 'none', ..., 'do', 'none', 'none'], dtype='<U13')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tmp_data/preds.pkl', 'wb') as f:\n",
    "    pickle.dump((y_pred_bin, y_pred_full), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'do',\n",
       " 'manner/route',\n",
       " 'fr',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'fr',\n",
       " 'none',\n",
       " 'manner/route',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'manner/route',\n",
       " 'fr',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'do',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'do',\n",
       " 'none',\n",
       " 'none',\n",
       " 'manner/route',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'do',\n",
       " 'none',\n",
       " 'fr',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'do',\n",
       " 'manner/route',\n",
       " 'fr',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'do',\n",
       " 'manner/route',\n",
       " 'fr',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'fr',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'manner/route',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'severity_type',\n",
       " 'none',\n",
       " 'none',\n",
       " 'fr',\n",
       " 'none',\n",
       " 'manner/route',\n",
       " 'du',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'manner/route',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'do',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'adverse',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'none',\n",
       " 'adverse',\n",
       " 'adverse',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'adverse',\n",
       " 'severity_type',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'fr',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'none',\n",
       " 'adverse',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'reason',\n",
       " 'adverse',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'none',\n",
       " 'adverse',\n",
       " 'none',\n",
       " 'none',\n",
       " 'adverse',\n",
       " 'reason',\n",
       " 'fr',\n",
       " 'none',\n",
       " 'do',\n",
       " 'manner/route',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'adverse',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'severity_type',\n",
       " 'none',\n",
       " 'severity_type',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'adverse',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'du',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'severity_type',\n",
       " 'none',\n",
       " 'none',\n",
       " 'severity_type',\n",
       " 'du',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'do',\n",
       " 'manner/route',\n",
       " 'none',\n",
       " 'fr',\n",
       " 'do',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'fr',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'adverse',\n",
       " 'none',\n",
       " 'adverse',\n",
       " 'none',\n",
       " 'adverse',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'severity_type',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'severity_type',\n",
       " 'do',\n",
       " 'none',\n",
       " 'manner/route',\n",
       " 'none',\n",
       " 'fr',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'severity_type',\n",
       " 'none',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'du',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'severity_type',\n",
       " 'adverse',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'none',\n",
       " 'severity_type',\n",
       " 'do',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'du',\n",
       " 'du',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'none',\n",
       " 'adverse',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'severity_type',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'do',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'do',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'fr',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'fr',\n",
       " 'none',\n",
       " 'do',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'do',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'fr',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'do',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'adverse',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'manner/route',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'adverse',\n",
       " 'manner/route',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'do',\n",
       " 'none',\n",
       " 'fr',\n",
       " 'none',\n",
       " 'do',\n",
       " 'none',\n",
       " 'fr',\n",
       " 'none',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'do',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'do',\n",
       " 'adverse',\n",
       " 'fr',\n",
       " 'none',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'adverse',\n",
       " 'none',\n",
       " 'adverse',\n",
       " 'none',\n",
       " 'adverse',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'do',\n",
       " 'fr',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'du',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'do',\n",
       " 'fr',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'manner/route',\n",
       " 'severity_type',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'severity_type',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'do',\n",
       " 'fr',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'fr',\n",
       " 'do',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'severity_type',\n",
       " 'none',\n",
       " 'du',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'do',\n",
       " 'none',\n",
       " 'fr',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'adverse',\n",
       " ...]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test = [y_pred_full[i] if y_pred_bin[i] != 'none' else 'none' for  i in range(len(y_pred_full))]\n",
    "y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_bioc_xml(doc, outdir):\n",
    "    outpath = os.path.join(outdir, doc.file_name + '.bioc.xml')\n",
    "    writer = bioc.BioCXMLWriter()\n",
    "    writer.collection = bioc.BioCCollection()\n",
    "    \n",
    "    collection = writer.collection\n",
    "    document = bioc.BioCDocument()\n",
    "    document.id = doc.file_name\n",
    "\n",
    "    passage = bioc.BioCPassage()\n",
    "    passage.offset = '0'\n",
    "    document.add_passage(passage)\n",
    "    collection.add_document(document)\n",
    "\n",
    "    # Add annotations that already have bioc annotations\n",
    "    for anno in doc.get_annotations():\n",
    "        passage.add_annotation(anno.bioc_anno)\n",
    "\n",
    "    for relat in doc.get_relations():\n",
    "        # Create new BioCRelation\n",
    "        relation = bioc.bioc_relation.BioCRelation()\n",
    "        relation.id = relat.id\n",
    "        relation.put_infon('type', relat.type)\n",
    "\n",
    "        # Reference that nodes that contain the annotations\n",
    "        node1 = bioc.bioc_node.BioCNode()\n",
    "        node1.role = 'annotation 1'\n",
    "        node1.refid = relat.annotation_1.id\n",
    "        relation.add_node(node1)\n",
    "\n",
    "        node2 = bioc.bioc_node.BioCNode()\n",
    "        node2.role = 'annotation 2'\n",
    "        node2.refid = relat.annotation_2.id\n",
    "        relation.add_node(node2)\n",
    "\n",
    "        passage.add_relation(relation)\n",
    "\n",
    "    writer.write(outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc ={}\n",
    "for relation in test_relations:\n",
    "    if relation.id in acc:\n",
    "        print(acc[relation.id])\n",
    "        print(relation)\n",
    "    else:\n",
    "        acc[relation.id] = relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'18_1076'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc['114309114310'].file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Write out bioc annotations\n",
    "# Remove any duplicates that somehow got in\n",
    "\n",
    "for doc in test_docs.values():\n",
    "    doc.relations = []\n",
    "    existing_annos = set()\n",
    "    to_add = []\n",
    "    for i, anno in enumerate(doc.annotations):\n",
    "        if anno.id not in existing_annos:\n",
    "            to_add.append(anno)\n",
    "            existing_annos.add(anno.id)\n",
    "    doc.annotations = to_add\n",
    "\n",
    "from collections import defaultdict\n",
    "relations_already_seen = []\n",
    "\n",
    "for i in range(len(y_pred_test)):\n",
    "    p = y_pred_test[i]\n",
    "#     print(p); break\n",
    "    r = test_relations[i]\n",
    "    r.type = p\n",
    "    doc = test_docs[r.file_name]\n",
    "    if r.type != 'none':\n",
    "        doc.relations.append(r)\n",
    "\n",
    "\n",
    "for doc in test_docs.values():\n",
    "    existing_relats = set()\n",
    "    to_add = []\n",
    "    for i, relat in enumerate(doc.relations):\n",
    "        if relat.id not in existing_relats:\n",
    "            to_add.append(relat)\n",
    "            existing_relats.add(relat.id)\n",
    "    doc.relations = to_add\n",
    "\n",
    "OUTDIR = 'tmp_data/output_{}'.format('test_set_task_one')\n",
    "if not os.path.exists(OUTDIR):\n",
    "    os.mkdir(OUTDIR)\n",
    "for d in test_docs.values():\n",
    "    to_bioc_xml(d, OUTDIR)\n",
    "        \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bioc\n",
    "bioc.BioCXMLWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/213\n",
      "100/213\n",
      "200/213\n"
     ]
    }
   ],
   "source": [
    "# Now do the same with Kelly's output\n",
    "# Read in the test data\n",
    "KELLYDIR = '/Users/alec/Data/NLP_Challenge/task1_test_set_predictions'\n",
    "task_3_reader = made_utils.TextAndBioCParser(KELLYDIR)\n",
    "task_3_docs = task_3_reader.read_texts_and_xmls(-1, include_relations=False) # TODO: Change to -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lymphoplasmacytoid lymphoma involving bone marrow and spleen], 397:457, type=[SSLIF]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = task_3_docs['1_1069']\n",
    "doc.annotations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/18637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alec/anaconda/envs/made/lib/python3.6/site-packages/ipykernel_launcher.py:232: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
      "/Users/alec/anaconda/envs/made/lib/python3.6/site-packages/ipykernel_launcher.py:182: DeprecationWarning: generator 'ngrams' raised StopIteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/18637\n",
      "200/18637\n",
      "300/18637\n",
      "400/18637\n",
      "500/18637\n",
      "600/18637\n",
      "700/18637\n",
      "800/18637\n",
      "900/18637\n",
      "1000/18637\n",
      "1100/18637\n",
      "1200/18637\n",
      "1300/18637\n",
      "1400/18637\n",
      "1500/18637\n",
      "1600/18637\n",
      "1700/18637\n",
      "1800/18637\n",
      "1900/18637\n",
      "2000/18637\n",
      "2100/18637\n",
      "2200/18637\n",
      "2300/18637\n",
      "2400/18637\n",
      "2500/18637\n",
      "2600/18637\n",
      "2700/18637\n",
      "2800/18637\n",
      "2900/18637\n",
      "3000/18637\n",
      "3100/18637\n",
      "3200/18637\n",
      "3300/18637\n",
      "3400/18637\n",
      "3500/18637\n",
      "3600/18637\n",
      "3700/18637\n",
      "3800/18637\n",
      "3900/18637\n",
      "4000/18637\n",
      "4100/18637\n",
      "4200/18637\n",
      "4300/18637\n",
      "4400/18637\n",
      "4500/18637\n",
      "4600/18637\n",
      "4700/18637\n",
      "4800/18637\n",
      "4900/18637\n",
      "5000/18637\n",
      "5100/18637\n",
      "5200/18637\n",
      "5300/18637\n",
      "5400/18637\n",
      "5500/18637\n",
      "5600/18637\n",
      "5700/18637\n",
      "5800/18637\n",
      "5900/18637\n",
      "6000/18637\n",
      "6100/18637\n",
      "6200/18637\n",
      "6300/18637\n",
      "6400/18637\n",
      "6500/18637\n",
      "6600/18637\n",
      "6700/18637\n",
      "6800/18637\n",
      "6900/18637\n",
      "7000/18637\n",
      "7100/18637\n",
      "7200/18637\n",
      "7300/18637\n",
      "7400/18637\n",
      "7500/18637\n",
      "7600/18637\n",
      "7700/18637\n",
      "7800/18637\n",
      "7900/18637\n",
      "8000/18637\n",
      "8100/18637\n",
      "8200/18637\n",
      "8300/18637\n",
      "8400/18637\n",
      "8500/18637\n",
      "8600/18637\n",
      "8700/18637\n",
      "8800/18637\n",
      "8900/18637\n",
      "9000/18637\n",
      "9100/18637\n",
      "9200/18637\n",
      "9300/18637\n",
      "9400/18637\n",
      "9500/18637\n",
      "9600/18637\n",
      "9700/18637\n",
      "9800/18637\n",
      "9900/18637\n",
      "10000/18637\n",
      "10100/18637\n",
      "10200/18637\n",
      "10300/18637\n",
      "10400/18637\n",
      "10500/18637\n",
      "10600/18637\n",
      "10700/18637\n",
      "10800/18637\n",
      "10900/18637\n",
      "11000/18637\n",
      "11100/18637\n",
      "11200/18637\n",
      "11300/18637\n",
      "11400/18637\n",
      "11500/18637\n",
      "11600/18637\n",
      "11700/18637\n",
      "11800/18637\n",
      "11900/18637\n",
      "12000/18637\n",
      "12100/18637\n",
      "12200/18637\n",
      "12300/18637\n",
      "12400/18637\n",
      "12500/18637\n",
      "12600/18637\n",
      "12700/18637\n",
      "12800/18637\n",
      "12900/18637\n",
      "13000/18637\n",
      "13100/18637\n",
      "13200/18637\n",
      "13300/18637\n",
      "13400/18637\n",
      "13500/18637\n",
      "13600/18637\n",
      "13700/18637\n",
      "13800/18637\n",
      "13900/18637\n",
      "14000/18637\n",
      "14100/18637\n",
      "14200/18637\n",
      "14300/18637\n",
      "14400/18637\n",
      "14500/18637\n",
      "14600/18637\n",
      "14700/18637\n",
      "14800/18637\n",
      "14900/18637\n",
      "15000/18637\n",
      "15100/18637\n",
      "15200/18637\n",
      "15300/18637\n",
      "15400/18637\n",
      "15500/18637\n",
      "15600/18637\n",
      "15700/18637\n",
      "15800/18637\n",
      "15900/18637\n",
      "16000/18637\n",
      "16100/18637\n",
      "16200/18637\n",
      "16300/18637\n",
      "16400/18637\n",
      "16500/18637\n",
      "16600/18637\n",
      "16700/18637\n",
      "16800/18637\n",
      "16900/18637\n",
      "17000/18637\n",
      "17100/18637\n",
      "17200/18637\n",
      "17300/18637\n",
      "17400/18637\n",
      "17500/18637\n",
      "17600/18637\n",
      "17700/18637\n",
      "17800/18637\n",
      "17900/18637\n",
      "18000/18637\n",
      "18100/18637\n",
      "18200/18637\n",
      "18300/18637\n",
      "18400/18637\n",
      "18500/18637\n",
      "18600/18637\n"
     ]
    }
   ],
   "source": [
    "def create_feature_dicts(relations, docs):\n",
    "    feat_dicts = []\n",
    "    for i, r in enumerate(relations):\n",
    "        doc = docs[r.file_name]\n",
    "        if i % 100 == 0:\n",
    "            print(\"{}/{}\".format(i, len(relations)))\n",
    "        feat_dict = feature_extractor.create_feature_dict(r, doc, entities=True, entities_between=True, surface=True)\n",
    "        feat_dicts.append(feat_dict)\n",
    "#         print(r, feat_dict)\n",
    "        \n",
    "        \n",
    "        \n",
    "    return feat_dicts\n",
    "task_3_feat_dicts = create_feature_dicts(task_3_relations, task_3_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bleomycin':'TID', Drug:Frequency, type=none"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_3_relations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_in_anno1': 'bleomycin',\n",
       " 'text_in_anno2': 'tid',\n",
       " 'concat_text': 'bleomycin:tid',\n",
       " 'first_entity_type:<DRUG>': 1,\n",
       " 'second_entity_type:<FREQUENCY>': 1,\n",
       " 'entity_types_concat': '<DRUG><=><FREQUENCY>',\n",
       " 'entities_between:<ADE>': 1,\n",
       " 'entities_between:<DRUG>': 1,\n",
       " 'entities_between:<DOSE>': 1,\n",
       " 'entities_between:<ROUTE>': 1,\n",
       " 'num_entities_between': 4,\n",
       " 'num_sentences_overlap': 3,\n",
       " 'num_tokens_between': 35,\n",
       " 'grams_between:<: diet regular>': 1,\n",
       " 'grams_between:<- .>': 1,\n",
       " 'grams_between:<with>': 1,\n",
       " 'grams_between:<home>': 1,\n",
       " 'grams_between:<of>': 1,\n",
       " 'grams_between:<meds>': 1,\n",
       " 'grams_between:<mg>': 1,\n",
       " 'grams_between:<- date>': 1,\n",
       " 'grams_between:<<NUMBER> mg oral>': 1,\n",
       " 'grams_between:<date>': 1,\n",
       " 'grams_between:<right upper>': 1,\n",
       " 'grams_between:<history surgical>': 1,\n",
       " 'grams_between:<: diet>': 1,\n",
       " 'grams_between:<on>': 1,\n",
       " 'grams_between:<past>': 1,\n",
       " 'grams_between:<no with>': 1,\n",
       " 'grams_between:<reconciliation>': 1,\n",
       " 'grams_between:<history>': 1,\n",
       " 'grams_between:<for>': 1,\n",
       " 'grams_between:<: history>': 1,\n",
       " 'grams_between:<: regular>': 1,\n",
       " 'grams_between:<see>': 1,\n",
       " 'grams_between:<- -- .>': 1,\n",
       " 'grams_between:<. past>': 1,\n",
       " 'grams_between:<restrictions>': 1,\n",
       " 'grams_between:<:>': 1,\n",
       " 'grams_between:<medication reconciliation>': 1,\n",
       " 'grams_between:<diet>': 1,\n",
       " 'grams_between:<. diet>': 1,\n",
       " 'grams_between:<regular>': 1,\n",
       " 'grams_between:<: history surgical>': 1,\n",
       " 'grams_between:<oral>': 1,\n",
       " 'grams_between:<-- date>': 1,\n",
       " 'grams_between:<no>': 1,\n",
       " 'grams_between:<history past surgical>': 1,\n",
       " 'grams_between:<order>': 1,\n",
       " 'grams_between:<- -- on>': 1,\n",
       " 'grams_between:<medications>': 1,\n",
       " 'grams_between:<extremity upper>': 1,\n",
       " 'grams_between:<past surgical>': 1,\n",
       " 'grams_between:<: medications>': 1,\n",
       " 'grams_between:<prednis<NUMBER>>': 1,\n",
       " 'grams_between:<- -->': 1,\n",
       " 'grams_between:<<NUMBER> mg>': 1,\n",
       " 'grams_between:<form>': 1,\n",
       " 'grams_between:<pulmonary>': 1,\n",
       " 'grams_between:<->': 1,\n",
       " 'grams_between:<mg oral>': 1,\n",
       " 'grams_between:<<NUMBER>>': 1,\n",
       " 'grams_between:<placement>': 1,\n",
       " 'grams_between:<.>': 1,\n",
       " 'grams_between:<surgical>': 1,\n",
       " 'grams_between:<-->': 1,\n",
       " 'grams_between:<. past surgical>': 1,\n",
       " 'grams_between:<right>': 1,\n",
       " 'grams_between:<- -- date>': 1,\n",
       " 'grams_between:<OOV>': 1,\n",
       " 'grams_between:<extremity>': 1,\n",
       " 'grams_between:<-- on>': 1,\n",
       " 'grams_between:<upper>': 1,\n",
       " 'grams_between:<of right>': 1,\n",
       " 'grams_between:<line>': 1,\n",
       " 'grams_between:<medication>': 1,\n",
       " 'grams_before:<possible>': 1,\n",
       " 'grams_before:<OOV>': 1,\n",
       " 'grams_after:<<NUMBER> for>': 1,\n",
       " 'grams_after:<for>': 1,\n",
       " 'grams_after:<<NUMBER>>': 1,\n",
       " 'tags_between:<cd>': 1,\n",
       " 'tags_between:<jj nn nnp>': 1,\n",
       " 'tags_between:<dt nnp nns>': 1,\n",
       " 'tags_between:<nn nnp>': 1,\n",
       " 'tags_between:<: in>': 1,\n",
       " 'tags_between:<dt nns>': 1,\n",
       " 'tags_between:<. jj jj>': 1,\n",
       " 'tags_between:<. : :>': 1,\n",
       " 'tags_between:<: nn>': 1,\n",
       " 'tags_between:<. :>': 1,\n",
       " 'tags_between:<. jj>': 1,\n",
       " 'tags_between:<: nnp vbd>': 1,\n",
       " 'tags_between:<: nnp>': 1,\n",
       " 'tags_between:<vbd>': 1,\n",
       " 'tags_between:<in nnp nnp>': 1,\n",
       " 'tags_between:<. : jj>': 1,\n",
       " 'tags_between:<cd vbd>': 1,\n",
       " 'tags_between:<nnp nns>': 1,\n",
       " 'tags_between:<nn>': 1,\n",
       " 'tags_between:<: nn nn>': 1,\n",
       " 'tags_between:<in jj>': 1,\n",
       " 'tags_between:<nnp nnp nnp>': 1,\n",
       " 'tags_between:<: jj nn>': 1,\n",
       " 'tags_between:<: :>': 1,\n",
       " 'tags_between:<: : in>': 1,\n",
       " 'tags_between:<cd jj jj>': 1,\n",
       " 'tags_between:<nn nn nnp>': 1,\n",
       " 'tags_between:<: nnp nns>': 1,\n",
       " 'tags_between:<jj jj nn>': 1,\n",
       " 'tags_between:<:>': 1,\n",
       " 'tags_between:<in nn>': 1,\n",
       " 'tags_between:<in nnp>': 1,\n",
       " 'tags_between:<in>': 1,\n",
       " 'tags_between:<jj jj>': 1,\n",
       " 'tags_between:<in jj jj>': 1,\n",
       " 'tags_between:<cd jj vbd>': 1,\n",
       " 'tags_between:<. jj nn>': 1,\n",
       " 'tags_between:<nnp nnp>': 1,\n",
       " 'tags_between:<in jj nn>': 1,\n",
       " 'tags_between:<nnp nnp nns>': 1,\n",
       " 'tags_between:<jj>': 1,\n",
       " 'tags_between:<dt in nns>': 1,\n",
       " 'tags_between:<: in nn>': 1,\n",
       " 'tags_between:<: : nn>': 1,\n",
       " 'tags_between:<dt in nn>': 1,\n",
       " 'tags_between:<cd nnp vbd>': 1,\n",
       " 'tags_between:<: nns>': 1,\n",
       " 'tags_between:<: jj>': 1,\n",
       " 'tags_between:<.>': 1,\n",
       " 'tags_between:<cd jj>': 1,\n",
       " 'tags_between:<jj nn>': 1,\n",
       " 'tags_between:<nnp vbd>': 1,\n",
       " 'tags_between:<dt>': 1,\n",
       " 'tags_between:<in nn nnp>': 1,\n",
       " 'tags_between:<dt in>': 1,\n",
       " 'tags_between:<. nn>': 1,\n",
       " 'tags_between:<nns>': 1,\n",
       " 'tags_between:<: jj jj>': 1,\n",
       " 'tags_between:<nnp>': 1,\n",
       " 'tags_before:<jj>': 1,\n",
       " 'tags_before:<OOV>': 1,\n",
       " 'tags_after:<in>': 1,\n",
       " 'tags_after:<cd>': 1,\n",
       " 'tags_after:<cd in>': 1,\n",
       " 'same_sentence': 0}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_3_feat_dicts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18637, 1000)\n",
      "(18637, 1000)\n"
     ]
    }
   ],
   "source": [
    "X_test = vectorizer.transform(task_3_feat_dicts)\n",
    "X_test_bin = binary_feature_selector.transform(X_test)\n",
    "X_test_full = full_feature_selector.transform(X_test)\n",
    "print(X_test_bin.shape)\n",
    "print(X_test_full.shape)\n",
    "\n",
    "\n",
    "\n",
    "y_pred_bin = clf_bin.predict(X_test_bin)\n",
    "y_pred_full = clf_full.predict(X_test_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['none',\n",
       " 'fr',\n",
       " 'none',\n",
       " 'none',\n",
       " 'du',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'do',\n",
       " 'manner/route',\n",
       " 'do',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'adverse',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'manner/route',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'fr',\n",
       " 'none',\n",
       " 'none',\n",
       " 'do',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'manner/route',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'manner/route',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'do',\n",
       " 'none',\n",
       " 'none',\n",
       " 'fr',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'do',\n",
       " 'manner/route',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'fr',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'do',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'manner/route',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'manner/route',\n",
       " 'none',\n",
       " 'none',\n",
       " 'fr',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'severity_type',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'fr',\n",
       " 'none',\n",
       " 'adverse',\n",
       " 'none',\n",
       " 'fr',\n",
       " 'none',\n",
       " 'none',\n",
       " 'adverse',\n",
       " 'reason',\n",
       " 'adverse',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'manner/route',\n",
       " 'none',\n",
       " 'do',\n",
       " 'reason',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'adverse',\n",
       " 'none',\n",
       " 'severity_type',\n",
       " 'severity_type',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'severity_type',\n",
       " 'severity_type',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'severity_type',\n",
       " 'du',\n",
       " 'severity_type',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'du',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'adverse',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'adverse',\n",
       " 'fr',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'severity_type',\n",
       " 'do',\n",
       " 'none',\n",
       " 'adverse',\n",
       " 'manner/route',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'fr',\n",
       " 'none',\n",
       " 'manner/route',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'do',\n",
       " 'none',\n",
       " 'none',\n",
       " 'fr',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'do',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'severity_type',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'do',\n",
       " 'none',\n",
       " 'du',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'severity_type',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'fr',\n",
       " 'none',\n",
       " 'none',\n",
       " 'do',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'fr',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'do',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'du',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'do',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'fr',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'do',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'do',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'do',\n",
       " 'none',\n",
       " 'severity_type',\n",
       " 'none',\n",
       " 'fr',\n",
       " 'none',\n",
       " 'none',\n",
       " 'manner/route',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'none',\n",
       " 'manner/route',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'severity_type',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'adverse',\n",
       " 'none',\n",
       " 'none',\n",
       " 'do',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'fr',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'adverse',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'fr',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'manner/route',\n",
       " 'none',\n",
       " 'do',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'do',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'severity_type',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'do',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'do',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'do',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'fr',\n",
       " 'none',\n",
       " 'severity_type',\n",
       " 'do',\n",
       " 'none',\n",
       " 'none',\n",
       " 'fr',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'fr',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'do',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'fr',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'reason',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'severity_type',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'manner/route',\n",
       " 'fr',\n",
       " 'none',\n",
       " 'severity_type',\n",
       " 'severity_type',\n",
       " 'do',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'severity_type',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'severity_type',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'severity_type',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'severity_type',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'severity_type',\n",
       " 'none',\n",
       " 'none',\n",
       " 'fr',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'do',\n",
       " 'do',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'fr',\n",
       " 'none',\n",
       " 'adverse',\n",
       " 'adverse',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'severity_type',\n",
       " 'severity_type',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'fr',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'adverse',\n",
       " 'adverse',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'adverse',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'do',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'do',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'fr',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'do',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'adverse',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'manner/route',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'fr',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'do',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'do',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'do',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'fr',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'manner/route',\n",
       " 'severity_type',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'fr',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'do',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'severity_type',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'do',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " 'fr',\n",
       " 'none',\n",
       " 'none',\n",
       " 'none',\n",
       " ...]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test = [y_pred_full[i] if y_pred_bin[i] != 'none' else 'none' for  i in range(len(y_pred_full))]\n",
    "y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Write out bioc annotations\n",
    "# Remove any duplicates that somehow got in\n",
    "\n",
    "for doc in task_3_docs.values():\n",
    "#     test_doc = test_docs[doc.file_name]\n",
    "#     doc.annotations = test_doc.annotations\n",
    "    doc.relations = []\n",
    "    existing_annos = set()\n",
    "    to_add = []\n",
    "    for i, anno in enumerate(doc.annotations):\n",
    "#         print(anno)\n",
    "        if anno.id not in existing_annos:\n",
    "            to_add.append(anno)\n",
    "            existing_annos.add(anno.id)\n",
    "    doc.annotations = to_add\n",
    "\n",
    "from collections import defaultdict\n",
    "relations_already_seen = []\n",
    "\n",
    "for i in range(len(y_pred_test)):\n",
    "    p = y_pred_test[i]\n",
    "#     print(p); break\n",
    "    r = task_3_relations[i]\n",
    "    r.type = p\n",
    "    doc = task_3_docs[r.file_name]\n",
    "    if r.type != 'none':\n",
    "        doc.relations.append(r)\n",
    "\n",
    "\n",
    "for doc in task_3_docs.values():\n",
    "    existing_relats = set()\n",
    "    to_add = []\n",
    "    for i, relat in enumerate(doc.relations):\n",
    "        if relat.id not in existing_relats:\n",
    "            to_add.append(relat)\n",
    "            existing_relats.add(relat.id)\n",
    "    doc.relations = to_add\n",
    "#     doc.annotations = []\n",
    "\n",
    "OUTDIR = 'tmp_data/output_{}'.format('test_set_task_three')\n",
    "if not os.path.exists(OUTDIR):\n",
    "    os.mkdir(OUTDIR)\n",
    "for d in task_3_docs.values():\n",
    "    to_bioc_xml(d, OUTDIR)\n",
    "        \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_in_anno1': 'chemotherapy',\n",
       " 'text_in_anno2': 'tablet                8 mg',\n",
       " 'concat_text': 'chemotherapy:tablet                8 mg',\n",
       " 'first_entity_type:<DRUG>': 1,\n",
       " 'second_entity_type:<DOSE>': 1,\n",
       " 'entity_types_concat': '<DRUG><=><DOSE>',\n",
       " 'entities_between:<DRUG>': 1,\n",
       " 'entities_between:<DOSE>': 1,\n",
       " 'entities_between:<ROUTE>': 1,\n",
       " 'entities_between:<FREQUENCY>': 1,\n",
       " 'entities_between:<INDICATION>': 1,\n",
       " 'num_entities_between': 28,\n",
       " 'num_sentences_overlap': 2,\n",
       " 'num_tokens_between': 54,\n",
       " 'grams_between:<to>': 1,\n",
       " 'grams_between:<: medication>': 1,\n",
       " 'grams_between:<# mr>': 1,\n",
       " 'grams_between:<: patient>': 1,\n",
       " 'grams_between:<as hours needed>': 1,\n",
       " 'grams_between:<- -- patient>': 1,\n",
       " 'grams_between:<every>': 1,\n",
       " 'grams_between:<ud>': 1,\n",
       " 'grams_between:<hospital medicine>': 1,\n",
       " 'grams_between:<-<NUMBER>>': 1,\n",
       " 'grams_between:<<NUMBER> mg po>': 1,\n",
       " 'grams_between:<sustained>': 1,\n",
       " 'grams_between:<-- medical_record_number>': 1,\n",
       " 'grams_between:<#>': 1,\n",
       " 'grams_between:<-- :>': 1,\n",
       " 'grams_between:<,>': 1,\n",
       " 'grams_between:<po>': 1,\n",
       " 'grams_between:<<NUMBER> mg>': 1,\n",
       " 'grams_between:<.>': 1,\n",
       " 'grams_between:<hcl>': 1,\n",
       " 'grams_between:<medical_record_number>': 1,\n",
       " 'grams_between:<hours>': 1,\n",
       " 'grams_between:<- medical_record_number>': 1,\n",
       " 'grams_between:<- : patient>': 1,\n",
       " 'grams_between:<once>': 1,\n",
       " 'grams_between:<for this>': 1,\n",
       " 'grams_between:<<NUMBER> tablet>': 1,\n",
       " 'grams_between:<mg po>': 1,\n",
       " 'grams_between:<that>': 1,\n",
       " 'grams_between:<tablet>': 1,\n",
       " 'grams_between:<reason>': 1,\n",
       " 'grams_between:<note>': 1,\n",
       " 'grams_between:<discharge summary>': 1,\n",
       " 'grams_between:<-- : patient>': 1,\n",
       " 'grams_between:<, ->': 1,\n",
       " 'grams_between:<<NUMBER>mg>': 1,\n",
       " 'grams_between:<month>': 1,\n",
       " 'grams_between:<- patient>': 1,\n",
       " 'grams_between:<# :>': 1,\n",
       " 'grams_between:<for>': 1,\n",
       " 'grams_between:<for medication this>': 1,\n",
       " 'grams_between:<- hospital>': 1,\n",
       " 'grams_between:<has increased>': 1,\n",
       " 'grams_between:<- name>': 1,\n",
       " 'grams_between:<medication this>': 1,\n",
       " 'grams_between:<# -- :>': 1,\n",
       " 'grams_between:<-->': 1,\n",
       " 'grams_between:<patient>': 1,\n",
       " 'grams_between:<mr>': 1,\n",
       " 'grams_between:<mg>': 1,\n",
       " 'grams_between:<- -- hospital>': 1,\n",
       " 'grams_between:<that your>': 1,\n",
       " 'grams_between:<as>': 1,\n",
       " 'grams_between:<medicine>': 1,\n",
       " 'grams_between:<hours <NUMBER>>': 1,\n",
       " 'grams_between:<increased>': 1,\n",
       " 'grams_between:<day>': 1,\n",
       " 'grams_between:<birth>': 1,\n",
       " 'grams_between:<omeprazole>': 1,\n",
       " 'grams_between:<: medication this>': 1,\n",
       " 'grams_between:<your>': 1,\n",
       " 'grams_between:<constipation>': 1,\n",
       " 'grams_between:<for reason>': 1,\n",
       " 'grams_between:<as needed>': 1,\n",
       " 'grams_between:<has>': 1,\n",
       " 'grams_between:<miralax>': 1,\n",
       " 'grams_between:<discharge>': 1,\n",
       " 'grams_between:<# : mr>': 1,\n",
       " 'grams_between:<daily>': 1,\n",
       " 'grams_between:<as hours>': 1,\n",
       " 'grams_between:<<NUMBER>>': 1,\n",
       " 'grams_between:<- -- name>': 1,\n",
       " 'grams_between:<increased to>': 1,\n",
       " 'grams_between:<- -- medical_record_number>': 1,\n",
       " 'grams_between:<or>': 1,\n",
       " 'grams_between:<for reason this>': 1,\n",
       " 'grams_between:<: note>': 1,\n",
       " 'grams_between:<needed>': 1,\n",
       " 'grams_between:<control>': 1,\n",
       " 'grams_between:<ondansetron>': 1,\n",
       " 'grams_between:<, -->': 1,\n",
       " 'grams_between:<this>': 1,\n",
       " 'grams_between:<hospital>': 1,\n",
       " 'grams_between:<- -- :>': 1,\n",
       " 'grams_between:<<NUMBER> mg tablet>': 1,\n",
       " 'grams_between:<, - -->': 1,\n",
       " 'grams_between:<itching>': 1,\n",
       " 'grams_between:<comment>': 1,\n",
       " 'grams_between:<day every>': 1,\n",
       " 'grams_between:<per>': 1,\n",
       " 'grams_between:<: comment>': 1,\n",
       " 'grams_between:<tablet ud>': 1,\n",
       " 'grams_between:<dose>': 1,\n",
       " 'grams_between:<-- hospital>': 1,\n",
       " 'grams_between:<:>': 1,\n",
       " 'grams_between:<every <NUMBER>>': 1,\n",
       " 'grams_between:<every hours <NUMBER>>': 1,\n",
       " 'grams_between:<-- name>': 1,\n",
       " 'grams_between:<- -->': 1,\n",
       " 'grams_between:<->': 1,\n",
       " 'grams_between:<name>': 1,\n",
       " 'grams_between:<summary>': 1,\n",
       " 'grams_between:<- hospital medicine>': 1,\n",
       " 'grams_between:<: birth>': 1,\n",
       " 'grams_between:<OOV>': 1,\n",
       " 'grams_between:<daps<NUMBER>>': 1,\n",
       " 'grams_between:<medication>': 1,\n",
       " 'grams_before:<following>': 1,\n",
       " 'grams_before:<days>': 1,\n",
       " 'grams_before:<OOV>': 1,\n",
       " 'grams_after:<po>': 1,\n",
       " 'grams_after:<OOV>': 1,\n",
       " 'tags_between:<cd>': 1,\n",
       " 'tags_between:<nn nnp>': 1,\n",
       " 'tags_between:<cd nnp>': 1,\n",
       " 'tags_between:<# nnp nnp>': 1,\n",
       " 'tags_between:<: nn>': 1,\n",
       " 'tags_between:<: dt nn>': 1,\n",
       " 'tags_between:<: nnp>': 1,\n",
       " 'tags_between:<in nnp nnp>': 1,\n",
       " 'tags_between:<# :>': 1,\n",
       " 'tags_between:<# : nnp>': 1,\n",
       " 'tags_between:<dt nn>': 1,\n",
       " 'tags_between:<, :>': 1,\n",
       " 'tags_between:<: : :>': 1,\n",
       " 'tags_between:<nn>': 1,\n",
       " 'tags_between:<jj nnp nnp>': 1,\n",
       " 'tags_between:<: nn nn>': 1,\n",
       " 'tags_between:<nnp nnp nnp>': 1,\n",
       " 'tags_between:<: :>': 1,\n",
       " 'tags_between:<dt in nnp>': 1,\n",
       " 'tags_between:<:>': 1,\n",
       " 'tags_between:<in nn>': 1,\n",
       " 'tags_between:<in nnp>': 1,\n",
       " 'tags_between:<: nn nnp>': 1,\n",
       " 'tags_between:<in>': 1,\n",
       " 'tags_between:<: nnp nnp>': 1,\n",
       " 'tags_between:<jj jj>': 1,\n",
       " 'tags_between:<cd jj nnp>': 1,\n",
       " 'tags_between:<nnp nnp>': 1,\n",
       " 'tags_between:<#>': 1,\n",
       " 'tags_between:<jj>': 1,\n",
       " 'tags_between:<# nnp>': 1,\n",
       " 'tags_between:<,>': 1,\n",
       " 'tags_between:<: in nn>': 1,\n",
       " 'tags_between:<: : nn>': 1,\n",
       " 'tags_between:<dt in nn>': 1,\n",
       " 'tags_between:<jj nnp>': 1,\n",
       " 'tags_between:<: jj>': 1,\n",
       " 'tags_between:<.>': 1,\n",
       " 'tags_between:<jj jj nnp>': 1,\n",
       " 'tags_between:<nn nnp nnp>': 1,\n",
       " 'tags_between:<. cd nnp>': 1,\n",
       " 'tags_between:<cd nnp nnp>': 1,\n",
       " 'tags_between:<dt>': 1,\n",
       " 'tags_between:<in nn nnp>': 1,\n",
       " 'tags_between:<dt in>': 1,\n",
       " 'tags_between:<: : nnp>': 1,\n",
       " 'tags_between:<: : jj>': 1,\n",
       " 'tags_between:<: jj jj>': 1,\n",
       " 'tags_between:<, : :>': 1,\n",
       " 'tags_between:<. nnp>': 1,\n",
       " 'tags_between:<# : :>': 1,\n",
       " 'tags_between:<nnp>': 1,\n",
       " 'tags_before:<nnp>': 1,\n",
       " 'tags_before:<nnp nnp>': 1,\n",
       " 'tags_after:<nnp>': 1,\n",
       " 'tags_after:<nnp nnp>': 1,\n",
       " 'same_sentence': 0}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_3_feat_dicts[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CHEMOTHERAPY':'TABLET                8 MG', Drug:Dose, type=none"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_3_relations[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entities+entities_between+surface\n",
      "bin entities+entities_between+surface\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-63f8cf906b35>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mX_train_bin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_bin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature_set_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'bin'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'bin'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mX_test_bin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_bin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_dict_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature_set_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'bin'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_dict_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'bin'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mbin_clf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_clf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_bin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_bin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mclfs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature_set_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'bin'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbin_clf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-55-0e4831b02e70>\u001b[0m in \u001b[0;36mtrain_clf\u001b[1;34m(X, y, cross_val)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\made\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    325\u001b[0m                     \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[1;32m--> 327\u001b[1;33m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[0;32m    328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m             \u001b[1;31m# Collect newly grown trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\made\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    777\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\made\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    623\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\made\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 588\u001b[1;33m         \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\made\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\made\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\made\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\made\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\made\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'balanced'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\made\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    788\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 790\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m    791\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    792\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\made\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    360\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# for feature_set_name in X_dict.keys():\n",
    "for feature_set_name in ['entities+entities_between+surface',]:\n",
    "    print(feature_set_name)\n",
    "    # First, train and get predictions for binary classifier\n",
    "    print('bin', feature_set_name)\n",
    "    X_train_bin, y_train_bin = X_dict[feature_set_name]['bin'], y_dict['bin']\n",
    "    X_test_bin, y_test_bin = X_dict_test[feature_set_name]['bin'], y_dict_test['bin']\n",
    "    bin_clf = train_clf(X_train_bin, y_train_bin)\n",
    "    \n",
    "    clfs[feature_set_name]['bin'] = bin_clf\n",
    "    \n",
    "    \n",
    "    # Now, train and get predictions full classifier\n",
    "    X_train_full = X_dict[feature_set_name]['full']\n",
    "    y_train_full = y_dict['full']\n",
    "    \n",
    "    X_test_full, y_test_full = X_dict_test[feature_set_name]['full'], y_dict_test['full']\n",
    "    \n",
    "    print('full', feature_set_name)\n",
    "    try:\n",
    "        full_clf = train_clf(X_train_full, y_train_full)\n",
    "        clfs[feature_set_name]['full'] = full_clf\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "        print(X_dict[feature_set_name]['full'], y_dict['full'])\n",
    "        raise e\n",
    "        \n",
    "        \n",
    "    # Now predict on the test set\n",
    "    # Now get the agreed upon scores\n",
    "    from collections import Counter\n",
    "    \n",
    "    print(\"Predicting binary test: {}\".format(X_test_bin.shape))\n",
    "    pred_bin = bin_clf.predict(X_test_bin)\n",
    "    \n",
    "    print(Counter(pred_bin))\n",
    "    print(\"Predicting binary test: {}\".format(X_test_full.shape))\n",
    "    pred_full = full_clf.predict(X_test_full)\n",
    "    print(Counter(pred_full))\n",
    "    \n",
    "    \n",
    "    y_pred_dict[feature_set_name+'_no_binary'] = pred_full\n",
    "    y_pred_dict[feature_set_name] = [pred_full[i] if pred_bin[i] != 'none' else 'none' for i in range(len(pred_full))]\n",
    "print(\"Finished training and predicting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reason' 'do' 'none' ..., 'fr' 'none' 'severity_type']\n"
     ]
    }
   ],
   "source": [
    "no_filter_pred = clfs['entities+entities_between+surface']['full'].predict(X_dict_test['entities+entities_between+surface']['full'])\n",
    "print(no_filter_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('tmp_data/preds.pkl', 'wb') as f:\n",
    "    pickle.dump(y_pred_dict, f)\n",
    "with open('tmp_data/clfs.pkl', 'wb') as f:\n",
    "    pickle.dump(clfs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('tmp_data/preds.pkl', 'rb') as f:\n",
    "    y_pred_dict = pickle.load(f)\n",
    "with open('tmp_data/clfs.pkl', 'rb') as f:\n",
    "    clfs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('tmp_data/preds.pkl', 'rb') as f:\n",
    "    y_pred_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entities\n",
      "entities_between\n",
      "surface\n",
      "entities+entities_between\n",
      "entities+entities_between+surface\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Write out bioc annotations\n",
    "# Remove any duplicates that somehow got in\n",
    "\n",
    "for doc in test_docs.values():\n",
    "    doc.relations = []\n",
    "    existing_annos = set()\n",
    "    to_add = []\n",
    "    for i, anno in enumerate(doc.annotations):\n",
    "        if anno.id not in existing_annos:\n",
    "            to_add.append(anno)\n",
    "            existing_annos.add(anno.id)\n",
    "    doc.annotations = to_add\n",
    "\n",
    "from collections import defaultdict\n",
    "relations_already_seen = []\n",
    "\n",
    "for feature_set_name in y_pred_dict.keys():\n",
    "# for feature_set_name in ('entities+entities_between+surface',):\n",
    "    print(feature_set_name)\n",
    "    for i in range(len(y_pred_dict[feature_set_name])):\n",
    "        p = y_pred_dict[feature_set_name][i]\n",
    "    #     print(p); break\n",
    "        r = test_relations[i]\n",
    "        r.type = p\n",
    "        doc = test_docs[r.file_name]\n",
    "        if r.type != 'none':\n",
    "            doc.relations.append(r)\n",
    "\n",
    "\n",
    "    for doc in test_docs.values():\n",
    "        existing_relats = set()\n",
    "        to_add = []\n",
    "        for i, relat in enumerate(doc.relations):\n",
    "            if relat.id not in existing_relats:\n",
    "                to_add.append(relat)\n",
    "                existing_relats.add(relat.id)\n",
    "        doc.relations = to_add\n",
    "\n",
    "    OUTDIR = 'tmp_data/output_{}'.format(feature_set_name)\n",
    "    if not os.path.exists(OUTDIR):\n",
    "        os.mkdir(OUTDIR)\n",
    "    for d in test_docs.values():\n",
    "        d.to_bioc_xml(OUTDIR)\n",
    "        \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/176\n",
      "100/176\n"
     ]
    }
   ],
   "source": [
    "# Read the documents back in, this time with true relations\n",
    "# Read in test data\n",
    "gold_test_reader = made_utils.TextAndBioCParser(VALDIR)\n",
    "gold_test_docs = gold_test_reader.read_texts_and_xmls(num_docs=-1, include_relations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4206"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_test_relations = []\n",
    "for doc in gold_test_docs.values():\n",
    "    gold_test_relations += doc.relations\n",
    "len(gold_test_relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21053"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make sure that our predictions are using the full feature set\n",
    "for i in range(len(y_pred_dict[feature_set_name])):\n",
    "    p = y_pred_dict['entities+entities_between+surface'][i]\n",
    "#     print(p); break\n",
    "    r = test_relations[i]\n",
    "    r.type = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a set of all anno -> anno edges in gold\n",
    "gold_edges = {file_name: {} for file_name in gold_test_docs.keys()}\n",
    "for relat in gold_test_relations:\n",
    "     gold_edges[relat.file_name][(relat.annotation_1.start_index,\n",
    "                               relat.annotation_2.end_index)] = relat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_edges = {file_name: {} for file_name in test_docs.keys()}\n",
    "for relat in test_relations:\n",
    "    # Exclude any 'none' relations\n",
    "    if relat.type == 'none':\n",
    "        continue\n",
    "    pred_edges[relat.file_name][(relat.annotation_1.start_index,\n",
    "                               relat.annotation_2.end_index)] = relat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of errors\n",
    "\n",
    "- **Relation-Type Errors** - We predicted a relation between the two entities but the wrong relation\n",
    "- **False Negative** - There is a relation, but we missed it\n",
    "- **False Positive** - There is a relation between the two, but we missed it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type_errors = []\n",
    "false_negatives = []\n",
    "for f_name, anno_edges in gold_edges.items():\n",
    "    for anno_edge in anno_edges:\n",
    "        # If there's not a relation between two annotations, it's a false negative\n",
    "        if anno_edge not in pred_edges[f_name]:\n",
    "            false_negatives.append(anno_edges[anno_edge])\n",
    "        else:\n",
    "            true_relat = anno_edges[anno_edge]\n",
    "            pred_relat = pred_edges[f_name][anno_edge]\n",
    "            if true_relat.type != pred_relat.type:\n",
    "                type_errors.append((true_relat, pred_relat))\n",
    "#         if anno_edges[anno_edge] \n",
    "    \n",
    "# Now go through and find the false positives\n",
    "false_positives = []\n",
    "for f_name, anno_edges in pred_edges.items():\n",
    "    for anno_edge in anno_edges:\n",
    "        if anno_edge not in gold_edges[f_name]:\n",
    "            false_positives.append(anno_edges[anno_edge])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Type Errors: 12\n",
      "Number of False Negatives: 363\n",
      "Number of False Positives: 332\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Type Errors: {}\".format(len(type_errors)))\n",
    "print(\"Number of False Negatives: {}\".format(len(false_negatives)))\n",
    "print(\"Number of False Positives: {}\".format(len(false_positives)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('chemotherapy':'shingles', Drug:ADE, type=adverse,\n",
       "  'chemotherapy':'shingles', Drug:Indication, type=reason),\n",
       " ('Carafate':'mucositis', Drug:Indication, type=reason,\n",
       "  'Carafate':'mucositis', Drug:ADE, type=adverse),\n",
       " ('chemo':'shingles', Drug:ADE, type=adverse,\n",
       "  'chemo':'shingles', Drug:Indication, type=reason),\n",
       " ('steroids':'decreased bone density', Drug:ADE, type=adverse,\n",
       "  'steroids':'decreased bone density', Drug:Indication, type=reason),\n",
       " ('steroids':'low testosterone level', Drug:ADE, type=adverse,\n",
       "  'steroids':'low testosterone level', Drug:Indication, type=reason),\n",
       " ('Zofran':'vomiting', Drug:Indication, type=reason,\n",
       "  'Zofran':'vomiting', Drug:ADE, type=adverse),\n",
       " ('scopolamine':'nausea', Drug:Indication, type=reason,\n",
       "  'scopolamine':'nausea', Drug:ADE, type=adverse),\n",
       " ('Benadryl':'rash', Drug:Indication, type=reason,\n",
       "  'Benadryl':'rash', Drug:ADE, type=adverse),\n",
       " ('steroid':'dyspnea', Drug:ADE, type=adverse,\n",
       "  'steroid':'dyspnea', Drug:Indication, type=reason),\n",
       " ('steroid':'blood pressure going up', Drug:ADE, type=adverse,\n",
       "  'steroid':'blood pressure going up', Drug:Indication, type=reason),\n",
       " ('steroid':'edema', Drug:ADE, type=adverse,\n",
       "  'steroid':'edema', Drug:Indication, type=reason),\n",
       " ('glucocorticoids':'Vitamin D deficiency', Drug:ADE, type=adverse,\n",
       "  'glucocorticoids':'Vitamin D deficiency', Drug:Indication, type=reason)]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (Truth, Pred)\n",
    "type_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6_917\n",
      "Truth: reason\n",
      "Pred: adverse\n",
      "-    o monitored. --- Name --- did have some <REASON><INDICATION>nausea</INDICATION> that was refractory to these <DRUG>scopolamine</DRUG></REASON>  patch and Zofran and was additionally g\n",
      "\n",
      "10_758\n",
      "Truth: adverse\n",
      "Pred: reason\n",
      "-    c  fever with the previous 5 cycles of <ADVERSE><DRUG>chemo</DRUG>, but did have one episode of <ADE>shingles</ADE></ADVERSE>   and has been treated on valacyclovir.\n",
      "\n",
      "7_410\n",
      "Truth: adverse\n",
      "Pred: reason\n",
      "-    ompression fractures. I also think the <ADVERSE><DRUG>steroid</DRUG> regimen may have increased her volume and that has resulted in <ADE>blood pressure going up</ADE></ADVERSE>  as well as increased edema. I therefor\n",
      "\n",
      "3_405\n",
      "Truth: reason\n",
      "Pred: adverse\n",
      "-     and drink, has occasional nausea  and <REASON><INDICATION>vomiting</INDICATION>, which was adequately controlled by <DRUG>Zofran</DRUG></REASON> . The patient has  been fever-free sin\n",
      "\n",
      "10_197\n",
      "Truth: adverse\n",
      "Pred: reason\n",
      "-    fever with either of the cycles of this <ADVERSE><DRUG>chemotherapy</DRUG>, but he did have 1 episode of <ADE>shingles</ADE></ADVERSE> , after which he has been put on valacyc\n",
      "\n",
      "7_410\n",
      "Truth: adverse\n",
      "Pred: reason\n",
      "-    - Name ---, I suspect that --- Name --- <ADVERSE><ADE>dyspnea</ADE> is likely due to the combined effects of worsening restrictive lung disease as by exam her kyphosis and scoliosis seems to be worsening significantly in the setting of her multiple compression fractures. I also think the <DRUG>steroid</DRUG></ADVERSE>  regimen may have increased her volume a\n",
      "\n",
      "7_410\n",
      "Truth: adverse\n",
      "Pred: reason\n",
      "-    ompression fractures. I also think the <ADVERSE><DRUG>steroid</DRUG> regimen may have increased her volume and that has resulted in blood pressure going up as well as increased <ADE>edema</ADE></ADVERSE> . I therefore had to double her furosem\n",
      "\n",
      "1_437\n",
      "Truth: adverse\n",
      "Pred: reason\n",
      "-    re could be additional risk factors for <ADVERSE><ADE>decreased bone density</ADE>. He has  been on <DRUG>steroids</DRUG></ADVERSE>  and he has now newly found low testoste\n",
      "\n",
      "7_472\n",
      "Truth: adverse\n",
      "Pred: reason\n",
      "-     taking extra calcium supplements.  2. <ADVERSE><ADE>Vitamin D deficiency</ADE>. Since at least --- Date ---. Most likely aggravated by the <DRUG>glucocorticoids</DRUG></ADVERSE> . Her achiness in her leg may be associ\n",
      "\n",
      "6_917\n",
      "Truth: reason\n",
      "Pred: adverse\n",
      "-    is believed that this is a methotrexate <REASON><INDICATION>rash</INDICATION> and he was given <DRUG>Benadryl</DRUG></REASON>  after which his symptoms resolved. ---\n",
      "\n",
      "10_508\n",
      "Truth: reason\n",
      "Pred: adverse\n",
      "-     well. The patient  had a few days of <REASON><INDICATION>mucositis</INDICATION> which resolved quickly with <DRUG>Carafate</DRUG></REASON>  and Miracle  Mouthwash. Currently, th\n",
      "\n",
      "1_437\n",
      "Truth: adverse\n",
      "Pred: reason\n",
      "-    creased bone density. He has  been on <ADVERSE><DRUG>steroids</DRUG> and he has now newly found <ADE>low testosterone level</ADE></ADVERSE> .  At this time, I will add a vitamin D\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for (truth_relat, pred_relat) in random.sample(type_errors, len(type_errors)):\n",
    "    doc = gold_test_docs[truth_relat.file_name]\n",
    "    print(doc.file_name)\n",
    "    print(\"Truth: {}\".format(truth_relat.type))\n",
    "    print(\"Pred: {}\".format(pred_relat.type))\n",
    "    print('-    ' + truth_relat.get_example_string(doc))\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "for truth_relat in random.sample(false_negatives, 100):\n",
    "    doc = gold_test_docs[truth_relat.file_name]\n",
    "    print(doc.file_name)\n",
    "    print(\"Truth: {}\".format(truth_relat))\n",
    "    print('-    ' + truth_relat.get_example_string(doc))\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for pred_relat in random.sample(false_positives, 100):\n",
    "    doc = test_docs[pred_relat.file_name]\n",
    "    print(doc.file_name)\n",
    "    print(\"Pred: {}\".format(pred_relat))\n",
    "    print('-    ' + pred_relat.get_example_string(doc))\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_docs['19_566'].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "false_negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "false_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'concat_text=acyclovir:2 capsule': 1.0,\n",
       " 'concat_text=air hunger:some': 1.0,\n",
       " 'concat_text=antivirals:prophylactic': 1.0,\n",
       " 'concat_text=aspirin:few cycles': 6.0,\n",
       " 'concat_text=aspirin:in the a.m': 1.0,\n",
       " 'concat_text=back pain:80%': 1.0,\n",
       " 'concat_text=back pain:adverse reaction': 1.0,\n",
       " 'concat_text=back pain:constipation': 1.0,\n",
       " 'concat_text=bactrim:10 meq': 1.0}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.inverse_transform(X_dict_test['entities+entities_between+surface']['full'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instead of computing metrics this way,\n",
    "# let's use their script to make sure we get the same number.\n",
    "\n",
    "# For now, let's start with just the full feature_set\n",
    "y_pred_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For some reasons there are duplicates in the training data\n",
    "# which probably messed up the cross-validation.\n",
    "\n",
    "new_docs = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-4e4f39427681>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnew_docs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0manno\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_annotations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manno\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'new_docs' is not defined"
     ]
    }
   ],
   "source": [
    "for fname, doc in new_docs.items():\n",
    "    for anno in doc.get_annotations():\n",
    "        print(anno.id)\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adverse\n",
      "{'text_in_anno1': 'lidocaine', 'text_in_anno2': 'anesthesia', 'concat_text': 'lidocaine:anesthesia', 'first_entity_type:<DRUG>': 1, 'second_entity_type:<INDICATION>': 1, 'entity_types_concat': '<DRUG><=><INDICATION>', 'num_entities_between': 0, 'num_sentences_overlap': 1, 'num_tokens_between': 6, 'grams_between:<OOV>': 1, 'grams_between:<local>': 1, 'grams_between:<then was>': 1, 'grams_between:<injected>': 1, 'grams_between:<then>': 1, 'grams_between:<to>': 1, 'grams_between:<was>': 1, 'grams_before:<%>': 1, 'grams_before:<<NUMBER>>': 1, 'grams_before:<% <NUMBER>>': 1, 'grams_after:<OOV>': 1, 'tags_between:<to vb vbn>': 1, 'tags_between:<vb>': 1, 'tags_between:<vbn>': 1, 'tags_between:<rb vbd vbn>': 1, 'tags_between:<rb>': 1, 'tags_between:<rb vbd>': 1, 'tags_between:<rb vbn>': 1, 'tags_between:<jj to vb>': 1, 'tags_between:<rb to vbn>': 1, 'tags_between:<to vbn>': 1, 'tags_between:<to>': 1, 'tags_between:<to vb>': 1, 'tags_between:<jj>': 1, 'tags_between:<vbd>': 1, 'tags_between:<jj vb>': 1, 'tags_before:<cd>': 1, 'tags_before:<nn>': 1, 'tags_before:<cd nn>': 1, 'tags_after:<OOV>': 1, 'same_sentence': 1}\n",
      "'lidocaine':'anesthesia', Drug:Indication, type=adverse\n",
      "10_1010\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    print(y_pred_dict['entities+entities_between+surface'][i])\n",
    "    print(feat_dicts_test['entities+entities_between+surface'][i])\n",
    "    print(test_relations[i])\n",
    "    print(test_relations[i].file_name)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doxorubicin':'infusion', Drug:Route, type=manner/route,\n",
       " 'valacyclovir':'bilateral shingles', Drug:Indication, type=reason,\n",
       " 'bloating':'mild', SSLIF:Severity, type=severity_type,\n",
       " 'neutropenia medication':'neutropenia', Drug:Indication, type=reason,\n",
       " 'Valtrex':'500 mg', Drug:Dose, type=do,\n",
       " 'Valtrex':'3 times daily', Drug:Frequency, type=fr,\n",
       " 'posaconazole':'200 mg per 5 mL', Drug:Dose, type=do,\n",
       " 'posaconazole':'3 times daily', Drug:Frequency, type=fr,\n",
       " 'ciprofloxacin':'500 mg', Drug:Dose, type=do,\n",
       " 'ciprofloxacin':'daily', Drug:Frequency, type=fr,\n",
       " 'antiemetics':'p.r.n.', Drug:Frequency, type=fr,\n",
       " 'uric acid':'daily', Drug:Frequency, type=fr,\n",
       " 'uric acid':'prophylaxis', Drug:Indication, type=reason,\n",
       " 'uric acid':'tumor \\nlysis syndrome', Drug:Indication, type=reason,\n",
       " 'doxorubicin':'infusion', Drug:Route, type=manner/route,\n",
       " 'valacyclovir':'bilateral shingles', Drug:Indication, type=reason,\n",
       " 'bloating':'mild', SSLIF:Severity, type=severity_type,\n",
       " 'neutropenia medication':'neutropenia', Drug:Indication, type=reason,\n",
       " 'Valtrex':'500 mg', Drug:Dose, type=do,\n",
       " 'Valtrex':'3 times daily', Drug:Frequency, type=fr,\n",
       " 'posaconazole':'200 mg per 5 mL', Drug:Dose, type=do,\n",
       " 'posaconazole':'3 times daily', Drug:Frequency, type=fr,\n",
       " 'ciprofloxacin':'500 mg', Drug:Dose, type=do,\n",
       " 'ciprofloxacin':'daily', Drug:Frequency, type=fr,\n",
       " 'antiemetics':'p.r.n.', Drug:Frequency, type=fr,\n",
       " 'uric acid':'daily', Drug:Frequency, type=fr,\n",
       " 'uric acid':'prophylaxis', Drug:Indication, type=reason,\n",
       " 'uric acid':'tumor \\nlysis syndrome', Drug:Indication, type=reason,\n",
       " 'Valtrex':'neutropenia', Drug:Indication, type=reason,\n",
       " 'hyper-CVAD':'daily', Drug:Frequency, type=fr]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[r for r in new_docs['10_1'].relations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[a.id for a in new_docs['10_1'].annotations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "def compute_metrics(y, pred, average='micro'):\n",
    "    metrics = {}\n",
    "    labels = set(y)\n",
    "    labels.remove('none')\n",
    "    labels = list(sorted(labels))\n",
    "    metrics['precision'] = precision_score(y, pred, average=average, labels=labels)\n",
    "    metrics['recall'] = recall_score(y, pred, average=average, labels=labels)\n",
    "    metrics['f1'] = f1_score(y, pred, average=average, labels=labels)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now compute the metrics\n",
    "metrics = {feature_set_name: None for feature_set_name in y_pred_dict.keys()}\n",
    "\n",
    "for feature_set_name, y_pred in y_pred_dict.items():\n",
    "    metrics[feature_set_name] = compute_metrics(y_dict['full'], y_pred, 'micro')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('entities+entities_between+surface',\n",
       "  {'f1': 0.9318378304591165,\n",
       "   'precision': 0.98508826783395065,\n",
       "   'recall': 0.88404921217353771}),\n",
       " ('entities+entities_between',\n",
       "  {'f1': 0.9058706875918896,\n",
       "   'precision': 0.95967255077153413,\n",
       "   'recall': 0.85778113533347722}),\n",
       " ('surface',\n",
       "  {'f1': 0.6509934013629971,\n",
       "   'precision': 0.73451951503973523,\n",
       "   'recall': 0.58452406647960287}),\n",
       " ('entities',\n",
       "  {'f1': 0.52739580229043381,\n",
       "   'precision': 0.76985540870862157,\n",
       "   'recall': 0.40107921433196631}),\n",
       " ('entities_between',\n",
       "  {'f1': 0.1727288185373545,\n",
       "   'precision': 0.30428625891835959,\n",
       "   'recall': 0.12059140945391755})]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(metrics.items(), key=lambda x:x[1]['f1'], reverse=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
