{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will explore different combinations of the features that were used in submission and save the results/evaluations to compare the different scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bioc import BioCXMLReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../final_system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import annotation\n",
    "import base_feature\n",
    "import made_utils\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "\n",
    "from nltk import ngrams as nltk_ngrams\n",
    "import re\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "DATADIR = '/Users/alec/Data/NLP_Challenge'\n",
    "# ALLDIR = os.path.join(DATADIR, 'original_data')\n",
    "TRAINDIR = os.path.join(DATADIR, 'MADE-1.0')\n",
    "TESTDIR = os.path.join(DATADIR, 'made_test_data')\n",
    "print(os.path.exists(TRAINDIR))\n",
    "print(os.path.exists(TESTDIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_grams(ngram_string):\n",
    "    \"\"\"\n",
    "    Normalizes the values in a string of joined ngrams\n",
    "    \"\"\"\n",
    "    # Substitute numbers\n",
    "    ngram_string = re.sub('[\\d]+|one|two|three|four|five|six|seven|eight|nine|ten', '<NUMBER>', ngram_string)\n",
    "    return ngram_string\n",
    "\n",
    "\n",
    "\n",
    "class LexicalFeatureExtractor(base_feature.BaseFeatureExtractor):\n",
    "    \"\"\"\n",
    "    ngram_window - the length of ngrams to include in the vocabulary.\n",
    "    context_window - the number of ngrams to include before and after the entity.\n",
    "    \"\"\"\n",
    "    def __init__(self, ngram_window=(1, 1), context_window=(2, 2),\n",
    "                vocab=None, pos_vocab=None, min_vocab_count=5, min_pos_count=5):\n",
    "        super().__init__()\n",
    "        self.ngram_window = ngram_window\n",
    "        if min(ngram_window) < 1 or max(ngram_window) > 3:\n",
    "            raise NotImplementedError(\"Ngram Window must be between one and 3\")\n",
    "        self.context_window = context_window\n",
    "        self.min_vocab_count = min_vocab_count\n",
    "        self.min_pos_count = min_pos_count\n",
    "\n",
    "        # Set vocab and POS vocab\n",
    "        self._unfiltered_vocab = vocab # Contains unigrams-trigrams, no count threshold\n",
    "        self._unfiltered_pos_vocab = pos_vocab\n",
    "\n",
    "        self.vocab = self.create_vocab(vocab, min_vocab_count, self.ngram_window) # Only contains ngrams defined by context_window\n",
    "        #print(self.vocab); exit()\n",
    "        self.pos_vocab =  self.create_vocab(pos_vocab, min_pos_count, self.ngram_window)\n",
    "        #self.tokens = [gram for (gram, idx) in self.vocab.items() if len(gram.split()) == 1] # Only unigrams\n",
    "        self.pos = {} # Will eventually contain mapping for POS tags\n",
    "\n",
    "        # pyConText tools\n",
    "        #self.modifiers = itemData.instantiateFromCSVtoitemData(\"https://raw.githubusercontent.com/chapmanbe/pyConTextNLP/master/KB/lexical_kb_05042016.tsv\")\n",
    "        #self.targets = itemData.instantiateFromCSVtoitemData(\"https://raw.githubusercontent.com/abchapman93/MADE_relations/master/feature_extraction/targets.tsv?token=AUOYx9rYHO6A5fiZS3mB9e_3DP83Uws8ks5aownVwA%3D%3D\")\n",
    "\n",
    "\n",
    "        #self.all_features_values = self.create_base_features()\n",
    "\n",
    "\n",
    "\n",
    "    def create_base_features(self):\n",
    "        \"\"\"\n",
    "        Enumerates possible feature values from the vocab, as well as an OOV value.\n",
    "        Any features that are binary should only get one index and are encoded as 0.\n",
    "        \"\"\"\n",
    "        # This will be a dictionary that contains all possible values for each feature\n",
    "        all_features_values = {\n",
    "            'same_sentence': 0,\n",
    "            'num_tokens_between': 0,\n",
    "            'grams_between': ['OOV'] + list(self.vocab),\n",
    "            'grams_before': ['OOV'] + list(self.vocab),\n",
    "            'grams_after': ['OOV'] + list(self.vocab),\n",
    "            'pos_grams_between': ['OOV'] + list(self.pos_vocab),\n",
    "            #'pos_grams_before': ['OOV'] + list(self.pos_vocab),\n",
    "            #'pos_grams_after': ['OOV'] + list(self.pos_vocab),\n",
    "            'first_entity_type': 0,#list(ENTITY_TYPES_MAPPING.values()),\n",
    "            'second_entity_type': 0,#list(ENTITY_TYPES_MAPPING.values()),\n",
    "\n",
    "            }\n",
    "        return all_features_values\n",
    "\n",
    "    def create_feature_dict(self, relat, doc, entities=True, entities_between=True, surface=True):\n",
    "        \"\"\"\n",
    "        Takes a RelationAnnotation and an AnnotatedDocument.\n",
    "        Returns the a dictionary containing the defined lexical features.\n",
    "        \"\"\"\n",
    "\n",
    "        lex_features = {}\n",
    "\n",
    "        if entities:\n",
    "            lex_features.update(self.get_entity_features(relat, doc))\n",
    "        if entities_between:\n",
    "            lex_features.update(self.get_entities_between_features(relat, doc))\n",
    "        if surface:\n",
    "            lex_features.update(self.get_surface_features(relat, doc))\n",
    "        return lex_features\n",
    "    \n",
    "    \n",
    "    def get_entity_features(self, relat, doc):\n",
    "        features = {}\n",
    "        \n",
    "        # The full string of the entities\n",
    "        anno1, anno2 = relat.get_annotations()\n",
    "        features['text_in_anno1'] = anno1.text.lower()\n",
    "        features['text_in_anno2'] = anno2.text.lower()\n",
    "        features['concat_text'] = anno1.text.lower() + ':' + anno2.text.lower()\n",
    "        \n",
    "        # Features for types of the entities\n",
    "        features['first_entity_type:<{}>'.format(relat.entity_types[0].upper())] = 1\n",
    "        features['second_entity_type:<{}>'.format(relat.entity_types[1].upper())] = 1\n",
    "        \n",
    "        # Feature types for entities, left to right\n",
    "        sorted_entities = sorted((relat.annotation_1, relat.annotation_2), key=lambda a: a.span[0])\n",
    "        features['entity_types_concat'] = '<=>'.join(['<{}>'.format(a.type.upper()) for a in sorted_entities])\n",
    "        return features\n",
    "    \n",
    "    \n",
    "    def get_entities_between_features(self, relat, doc):\n",
    "       \n",
    "        features = {}\n",
    "        # One binary feature for every type of entity between\n",
    "        entities_between = self.get_entities_between(relat, doc)\n",
    "        # TODO: Maybe change this to a count\n",
    "        features.update({\n",
    "            'entities_between:<{}>'.format(v.type.upper()): 1 for v in entities_between\n",
    "            })\n",
    "        features['num_entities_between'] = len(entities_between)\n",
    "\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_surface_features(self, relat, doc):        \n",
    "        \n",
    "        features = {}\n",
    "        \n",
    "        # Same sentence\n",
    "        features['num_sentences_overlap'] = len(doc.get_sentences_overlap_span(relat.get_span()))\n",
    "        # Get the number of tokens between\n",
    "        # NOTE: only unigrams\n",
    "        \n",
    "        features['num_tokens_between'] = len(self.get_grams_between(relat, doc, ngram_window=(1, 1)))\n",
    "        # Get all tokens/POS tags in between\n",
    "        # Create one feature for each ngram/tag\n",
    "        features.update({\n",
    "            'grams_between:<{}>'.format(v): 1 for v in self.get_grams_between(relat, doc)\n",
    "            })\n",
    "        features.update({\n",
    "            'grams_before:<{}>'.format(v): 1 for v in self.get_grams_before(relat, doc)\n",
    "            })\n",
    "        features.update({\n",
    "            'grams_after:<{}>'.format(v): 1 for v in self.get_grams_after(relat, doc)\n",
    "            })\n",
    "\n",
    "        features.update({\n",
    "            'tags_between:<{}>'.format(v): 1 for v in self.get_grams_between(relat, doc, seq='tags')\n",
    "            })\n",
    "        features.update({\n",
    "            'tags_before:<{}>'.format(v): 1 for v in self.get_grams_before(relat, doc, seq='tags')\n",
    "            })\n",
    "        features.update({\n",
    "            'tags_after:<{}>'.format(v): 1 for v in self.get_grams_after(relat, doc, seq='tags')\n",
    "            })\n",
    "\n",
    "        # Get features for information about entities/context between\n",
    "        # Binary feature: Are they in the same sentence?\n",
    "        features['same_sentence'] = doc.in_same_sentence(relat.get_span())\n",
    "        return features\n",
    "        \n",
    "\n",
    "    def get_grams_between(self, relat, doc, seq='tokens', ngram_window=None):\n",
    "        \"\"\"\n",
    "        Returns the N-grams between the two entities connected in relat.\n",
    "        Represents it as OOV if it's not in the vocabulary.\n",
    "        Returns a unique set.\n",
    "        \"\"\"\n",
    "\n",
    "        if seq == 'tokens':\n",
    "            vocab = self.vocab\n",
    "        elif seq == 'tags':\n",
    "            vocab = self.pos_vocab\n",
    "        else:\n",
    "            raise ValueError(\"Must specify seq: {}\".format(seq))\n",
    "\n",
    "        if not ngram_window:\n",
    "            ngram_window = self.ngram_window\n",
    "\n",
    "        all_grams = []\n",
    "        span1, span2 = relat.spans\n",
    "        # Fixed this: get the start and span of the middle, not of the entire relation\n",
    "        _, start, end, _ = sorted(span1 +span2)\n",
    "        tokens_in_span = doc.get_tokens_or_tags_at_span((start, end), seq)\n",
    "        # NOTE: lower-casing the ngrams, come back to this if you want to encode the casing\n",
    "        tokens_in_span = [token.lower() for token in tokens_in_span]\n",
    "        for n in range(ngram_window[0], ngram_window[1] + 1):\n",
    "            # Now sort the ngrams so that it doesn't matter what order they occur in\n",
    "            grams = list(nltk_ngrams(tokens_in_span, n))\n",
    "            grams = self.sort_ngrams(grams)# + [' '.join(sorted(tup)) for tup in list(nltk_ngrams(tokens_in_span, n))]\n",
    "            all_grams.extend(set(grams))\n",
    "        all_grams = [self.normalize_grams(x) for x in set(all_grams)]\n",
    "        all_grams = [x if x in vocab else 'OOV' for x in all_grams]\n",
    "        return set(all_grams)\n",
    "\n",
    "\n",
    "    def get_grams_before(self, relat,doc, seq='tokens', ngram_window=None):\n",
    "        \"\"\"\n",
    "        Returns the n-grams before the first entity.\n",
    "        \"\"\"\n",
    "        if seq == 'tokens':\n",
    "            vocab = self.vocab\n",
    "        elif seq == 'tags':\n",
    "            vocab = self.pos_vocab\n",
    "        if not ngram_window:\n",
    "            ngram_window = self.ngram_window\n",
    "\n",
    "        all_grams = []\n",
    "        offset = relat.span[0]\n",
    "        tokens_before = doc.get_tokens_or_tags_before_or_after(offset, delta=-1,\n",
    "            n=self.context_window[0], seq=seq, padding=True)\n",
    "        tokens_before = [token.lower() for token in tokens_before]\n",
    "        for n in range(ngram_window[0], ngram_window[1] + 1):\n",
    "            grams = list(nltk_ngrams(tokens_before, n))\n",
    "            grams = self.sort_ngrams(grams)# + [' '.join(sorted(tup)) for tup in list(nltk_ngrams(tokens_in_span, n))]\n",
    "            all_grams.extend(set(grams))\n",
    "            #grams = grams + [' '.join(sorted(tup)) for tup in list(nltk_ngrams(tokens_before, n))]\n",
    "        all_grams = [self.normalize_grams(x) for x in set(all_grams)]\n",
    "        all_grams = [x if x in vocab else 'OOV' for x in all_grams]\n",
    "        return set(all_grams)\n",
    "\n",
    "    def get_grams_after(self, relat, doc, seq='tokens', ngram_window=None):\n",
    "        \"\"\"\n",
    "        Returns the n-grams after the final entity.\n",
    "        \"\"\"\n",
    "        if seq == 'tokens':\n",
    "            vocab = self.vocab\n",
    "        elif seq == 'tags':\n",
    "            vocab = self.pos_vocab\n",
    "        if not ngram_window:\n",
    "            ngram_window = self.ngram_window\n",
    "\n",
    "        all_grams = []\n",
    "        offset = relat.span[1]\n",
    "        tokens_after = doc.get_tokens_or_tags_before_or_after(offset, delta=1,\n",
    "                                        n=self.context_window[1], seq=seq)\n",
    "        tokens_after = [token.lower() for token in tokens_after]\n",
    "        for n in range(ngram_window[0], ngram_window[1] + 1):\n",
    "            grams = list(nltk_ngrams(tokens_after, n))\n",
    "            grams = self.sort_ngrams(grams)# + [' '.join(sorted(tup)) for tup in list(nltk_ngrams(tokens_in_span, n))]\n",
    "            all_grams.extend(set(grams))\n",
    "            #grams = grams + [' '.join(sorted(tup)) for tup in list(nltk_ngrams(tokens_after, n))]\n",
    "        all_grams = [self.normalize_grams(x) for x in set(all_grams)]\n",
    "        all_grams = [x if x in vocab else 'OOV' for x in all_grams]\n",
    "        return set(all_grams)\n",
    "\n",
    "    def sort_ngrams(self, ngrams):\n",
    "        return [' '.join(sorted(tup)) for tup in ngrams]\n",
    "\n",
    "    def normalize_grams(self, ngram_string):\n",
    "        \"\"\"\n",
    "        Normalizes the values in a string of joined ngrams\n",
    "        \"\"\"\n",
    "        # Substitute numbers\n",
    "        return normalize_grams(ngram_string)\n",
    "\n",
    "    def get_pos_tags(self):\n",
    "        pass\n",
    "\n",
    "    def get_entities_between(self, relat, doc):\n",
    "        \"\"\"\n",
    "        Returns a list of entities that occur between entity1 and entity2\n",
    "        \"\"\"\n",
    "        offset, end = relat.get_span()\n",
    "        overlapping_entities = []\n",
    "        # Index the entity in doc by span\n",
    "        offset_to_entity = {entity.span[0]: entity for entity in doc.get_annotations()\n",
    "                    if entity.id not in (\n",
    "                        relat.annotation_1.id, relat.annotation_2.id)\n",
    "                        }\n",
    "\n",
    "        while offset < end:\n",
    "            if offset in offset_to_entity:\n",
    "                overlapping_entities.append(offset_to_entity[offset])\n",
    "            offset += 1\n",
    "\n",
    "        return overlapping_entities\n",
    "\n",
    "\n",
    "    def get_sent_with_anno(self, anno, doc, entity_type):\n",
    "        \"\"\"\n",
    "        Returns the sentence that contains a given annotation.\n",
    "        Replaces the text of the annotations with a tag <ENTITY-TYPE>\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        # Step back some window\n",
    "        offset = anno.start_index\n",
    "\n",
    "        while offset not in doc._sentences:\n",
    "            offset -= 1\n",
    "            if offset < 0:\n",
    "                break\n",
    "            if offset in doc._tokens:\n",
    "                tokens.insert(0, doc._tokens[offset].lower())\n",
    "\n",
    "        # Now add an entity\n",
    "        tokens.append(entity_type)\n",
    "\n",
    "        # Now add all the tokens between them\n",
    "        offset = anno.start_index\n",
    "\n",
    "        while offset not in doc._sentences:\n",
    "            if offset > max(doc._tokens.keys()):\n",
    "                break\n",
    "            if offset in doc._tokens:\n",
    "                tokens.append(doc._tokens[offset].lower())\n",
    "            offset += 1\n",
    "\n",
    "\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"LexicalFeatureExtractor Ngram Window: {} Vocab: {} terms\".format(\n",
    "                self.ngram_window, len(self.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let's remove the validation hold-out set\n",
    "# import glob\n",
    "\n",
    "# held_out = [os.path.basename(x) for x in glob.glob(os.path.join('..', 'data', 'heldout_xmls', 'corpus', '*'))]\n",
    "# docs = {fname: doc for (fname, doc) in docs.items() if fname not in held_out}\n",
    "# len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_annotations_in_doc(doc, legal_edges=[], max_sent_length=3):\n",
    "    \"\"\"\n",
    "    Takes a single AnnotatedDocument that contains annotations.\n",
    "    All annotations that have a legal edge between them\n",
    "    and are have an overlapping sentence length <= max_sent_length,\n",
    "        ie., they are in either the same sentence or n adjancent sentences,\n",
    "    are paired to create RelationAnnotations.\n",
    "    Takes an optional list legal_edges that defines which edges should be allowed.\n",
    "\n",
    "    Returns a list of new RelationAnnotations with annotation type 'none'.\n",
    "    \"\"\"\n",
    "    if legal_edges == []:\n",
    "        legal_edges = [('Drug', 'Route'),\n",
    "                         ('Drug', 'Indication'),\n",
    "                         ('SSLIF', 'Severity'),\n",
    "                         ('Drug', 'Dose'),\n",
    "                         ('Drug', 'Frequency'),\n",
    "                         ('Drug', 'Duration'),\n",
    "                         ('Drug', 'ADE'),\n",
    "                         ('ADE', 'Severity'),\n",
    "                         ('Indication', 'Severity'),\n",
    "                         ('SSLIF', 'ADE')]\n",
    "    true_annotations = doc.get_annotations()\n",
    "    true_relations = doc.get_relations()\n",
    "    generated_relations = []\n",
    "    edges = defaultdict(list)\n",
    "    edges = set()\n",
    "\n",
    "    # Map all annotation_1's to annotation_2's\n",
    "    # in order to identify all positive examples of relations\n",
    "    # If this is testing data, it may not actually have these\n",
    "    for relat in true_relations:\n",
    "        anno1, anno2 = relat.get_annotations()\n",
    "        edges.add((anno1.id, anno2.id))\n",
    "\n",
    "    for anno1 in true_annotations:\n",
    "        for anno2 in true_annotations:\n",
    "\n",
    "            # Don't pair the same annotation with itself\n",
    "            if anno1.id == anno2.id:\n",
    "                continue\n",
    "\n",
    "            if anno1.span == anno2.span:\n",
    "                continue\n",
    "\n",
    "            # Don't generate paris that have already been paried\n",
    "            if (anno1.id, anno2.id) in edges:\n",
    "                continue\n",
    "\n",
    "            # Exclude illegal relations\n",
    "            if len(legal_edges) and (anno1.type, anno2.type) not in legal_edges:\n",
    "                continue\n",
    "\n",
    "            # Check the span between them, make sure it's either 1 or 2\n",
    "            start1, end1 = anno1.span\n",
    "            start2, end2 = anno2.span\n",
    "            sorted_spans = list(sorted([start1, end1, start2, end2]))\n",
    "            span = (sorted_spans[0], sorted_spans[-1])\n",
    "            overlapping_sentences = doc.get_sentences_overlap_span(span)\n",
    "            if len(overlapping_sentences) > max_sent_length:\n",
    "                continue\n",
    "\n",
    "            # If they haven't already been paired, pair them\n",
    "            else:\n",
    "                generated_relation = annotation.RelationAnnotation.from_null_rel(\n",
    "                    anno1, anno2, doc.file_name\n",
    "                )\n",
    "                edges.add((anno1.id, anno2.id))\n",
    "                generated_relations.append(generated_relation)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return list(set(generated_relations + true_relations))\n",
    "\n",
    "def sample_negative_examples(relations, neg_prop=1.0):\n",
    "    \"\"\"\n",
    "    Takes a list of Relationannotations and\n",
    "    neg_prop, a float that specifies the proportion of negative\n",
    "    to positive examples.\n",
    "\n",
    "    In the future, a more sophisticated method of sampling might be used,\n",
    "    ie., sampling by the probability of the Annotation types in the nodes.\n",
    "    \"\"\"\n",
    "    pos_relations = []\n",
    "    neg_relations = []\n",
    "    for relat in relations:\n",
    "        if relat.type == 'none':\n",
    "            neg_relations.append(relat)\n",
    "        else:\n",
    "            pos_relations.append(relat)\n",
    "\n",
    "    pos_size = len(pos_relations)\n",
    "    neg_sample_size = int(neg_prop * pos_size)\n",
    "\n",
    "    neg_sample = random.sample(neg_relations, neg_sample_size)\n",
    "    print(\"Original Distribution: {} positive relations, {} negative relations\".format(\n",
    "                len(pos_relations),\n",
    "                len(neg_relations)))\n",
    "    print(\"{} positive relations, {} negative relations\".format(len(pos_relations),\n",
    "                len(neg_sample)))\n",
    "    return pos_relations + neg_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/5\n"
     ]
    }
   ],
   "source": [
    "reader = made_utils.TextAndBioCParser(TRAINDIR)\n",
    "docs = reader.read_texts_and_xmls(5) # TODO: Change to -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = docs['12_123']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0: 12_123 \n",
      "32\n",
      "96\n",
      "-1: 4_857 \n",
      "31\n",
      "59\n",
      "-2: 17_839 \n",
      "11\n",
      "33\n",
      "-3: 10_988 \n",
      "16\n",
      "48\n",
      "-4: 13_513 \n",
      "22\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "relations = []\n",
    "for i, (fname, doc) in enumerate(docs.items()):\n",
    "    if i  >= 0:\n",
    "        print('-{}: {} '.format(i, fname))\n",
    "        print(len(doc.relations))\n",
    "    new_relations = pair_annotations_in_doc(doc)\n",
    "    \n",
    "    # Add Fake relations for training\n",
    "    neg_relations = set(new_relations).difference(set(doc.relations))\n",
    "    # Sample them\n",
    "    if len(neg_relations) >= 2 * len(doc.relations):\n",
    "        neg_relations = random.sample(neg_relations, 2 * len(doc.relations))\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "    doc.add_relations(neg_relations)\n",
    "    \n",
    "    relations.extend(doc.get_relations())\n",
    "    if i >= 0:\n",
    "        print(len(doc.get_relations()))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../final_system/data/vocab.pkl', 'rb') as f:\n",
    "    vocab, pos_vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open('tmp_data/all_training_docs_and_relations.pkl', 'wb') as f:\n",
    "    pickle.dump((docs, relations), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "relations = []\n",
    "for doc in docs.values():\n",
    "    relations += doc.relations\n",
    "random.shuffle(relations)\n",
    "len(relations)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('tmp_data/all_training_docs_and_relations.pkl', 'rb') as f:\n",
    "    docs, relations = pickle.load(f)\n",
    "len(relations)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "with open('tmp_data/val_docs_and_relations.pkl', 'rb') as f:\n",
    "    test_docs, test_relations = pickle.load(f)\n",
    "\n",
    "# Take out validation relations\n",
    "relations = [r for r in relations if r.file_name not in test_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['12_123', '4_857', '17_839', '10_988', '13_513'])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171\n"
     ]
    }
   ],
   "source": [
    "relat = relations[0]\n",
    "print(len([r for r in relations if r.type == 'none']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "283"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-val split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_relats, val_relats = train_test_split(relations,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226\n",
      "57\n"
     ]
    }
   ],
   "source": [
    "print(len(train_relats))\n",
    "print(len(val_relats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len({r.id for r in train_relats}.intersection({r.id for r in val_relats})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(548, 637)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'atenolol':'oral', Drug:Route, type=manner/route"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = relations[7]\n",
    "doc = docs[r.file_name]\n",
    "print(r.span)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283\n",
      "283\n"
     ]
    }
   ],
   "source": [
    "rids = [r.id for r in relations]\n",
    "print(len(rids))\n",
    "print(len(set(rids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "283"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(relations))\n",
    "len(set(relations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations = list(set(relations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'none': 171,\n",
       "         'do': 22,\n",
       "         'severity_type': 19,\n",
       "         'du': 15,\n",
       "         'manner/route': 12,\n",
       "         'fr': 13,\n",
       "         'reason': 29,\n",
       "         'adverse': 2})"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rtypes = [r.type for r in relations]\n",
    "from collections import Counter\n",
    "c = Counter(rtypes)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alec/anaconda/envs/made/lib/python3.6/site-packages/ipykernel_launcher.py:182: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
      "/Users/alec/anaconda/envs/made/lib/python3.6/site-packages/ipykernel_launcher.py:232: DeprecationWarning: generator 'ngrams' raised StopIteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/283\n",
      "200/283\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = LexicalFeatureExtractor(context_window=(2, 2),\n",
    "                            ngram_window=(1, 3), vocab=vocab, pos_vocab=pos_vocab,\n",
    "                            min_vocab_count=20, min_pos_count=20)\n",
    "feat_dicts = {feature_set_name: [] for feature_set_name in \n",
    "              ('entities', 'entities_between', 'surface', \n",
    "               'entities+entities_between', 'entities+entities_between+surface')}\n",
    "for i, r in enumerate(relations):\n",
    "    if i % 100 == 0:\n",
    "        print(\"{}/{}\".format(i, len(relations)))\n",
    "    doc = docs[r.file_name]\n",
    "    # Single feature sets\n",
    "    entities_feat_dict = feature_extractor.create_feature_dict(r, doc, entities=True, entities_between=False, surface=False)\n",
    "    entities_between_dict = feature_extractor.create_feature_dict(r, doc, entities=False, entities_between=True, surface=False)\n",
    "    surface_dict = feature_extractor.create_feature_dict(r, doc, entities=False, entities_between=False, surface=True)\n",
    "    feat_dicts['entities'].append(entities_feat_dict)\n",
    "    feat_dicts['entities_between'].append(entities_between_dict)\n",
    "    feat_dicts['surface'].append(surface_dict)\n",
    "    \n",
    "    # Now create the combinations\n",
    "    combo = {}\n",
    "    combo.update(entities_feat_dict)\n",
    "    combo.update(entities_between_dict)\n",
    "    \n",
    "    feat_dicts['entities+entities_between'].append(combo)\n",
    "    combo2 = {}\n",
    "    combo2.update(combo)\n",
    "    combo2.update(surface_dict)\n",
    "#     feat_dicts['entities+entities_between'].append(combo)\n",
    "    feat_dicts['entities+entities_between+surface'].append(combo2)\n",
    "#     break\n",
    "    continue\n",
    "    # Original code\n",
    "    \n",
    "    feat_dicts['entities'].append(feature_extractor.create_feature_dict(r, doc, entities=True, entities_between=False, surface=False))\n",
    "    feat_dicts['entities_between'].append(feature_extractor.create_feature_dict(r, doc, entities=False, entities_between=True, surface=False))\n",
    "    feat_dicts['surface'].append(feature_extractor.create_feature_dict(r, doc, entities=False, entities_between=False, surface=True))\n",
    "    \n",
    "    # Combinations\n",
    "    feat_dicts['entities+entities_between'].append(feature_extractor.create_feature_dict(r, doc, entities=True, entities_between=True, surface=False))\n",
    "    feat_dicts['entities+entities_between+surface'].append(feature_extractor.create_feature_dict(r, doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tmp_data/feat_dicts', 'wb') as f:\n",
    "    pickle.dump((feat_dicts, relations), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('tmp_data/feat_dicts', 'rb') as f:\n",
    "    feat_dicts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_in_anno1': 'doxazosin',\n",
       " 'text_in_anno2': 'daily',\n",
       " 'concat_text': 'doxazosin:daily',\n",
       " 'first_entity_type:<DRUG>': 1,\n",
       " 'second_entity_type:<FREQUENCY>': 1,\n",
       " 'entity_types_concat': '<DRUG><=><FREQUENCY>'}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_dicts['entities'][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'doxazosin':'DAILY', Drug:Frequency, type=none"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relations[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entities_between:<DOSE>': 1,\n",
       " 'entities_between:<ROUTE>': 1,\n",
       " 'entities_between:<FREQUENCY>': 1,\n",
       " 'entities_between:<DRUG>': 1,\n",
       " 'entities_between:<INDICATION>': 1,\n",
       " 'num_entities_between': 27}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_dicts['entities_between'][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_sentences_overlap': 1,\n",
       " 'num_tokens_between': 28,\n",
       " 'grams_between:<,>': 1,\n",
       " 'grams_between:<daily oral tablet>': 1,\n",
       " 'grams_between:<hours prn>': 1,\n",
       " 'grams_between:<by ordered>': 1,\n",
       " 'grams_between:<and>': 1,\n",
       " 'grams_between:<every oral>': 1,\n",
       " 'grams_between:<<NUMBER> mg tablet>': 1,\n",
       " 'grams_between:<, mg tablet>': 1,\n",
       " 'grams_between:<-- : by>': 1,\n",
       " 'grams_between:<mg tablet>': 1,\n",
       " 'grams_between:<<NUMBER> days for>': 1,\n",
       " 'grams_between:<OOV>': 1,\n",
       " 'grams_between:<: directions>': 1,\n",
       " 'grams_between:<tablet>': 1,\n",
       " 'grams_between:<prn>': 1,\n",
       " 'grams_between:<: by ordered>': 1,\n",
       " 'grams_between:<capsule mg>': 1,\n",
       " 'grams_between:<<NUMBER> mg>': 1,\n",
       " 'grams_between:<fatty>': 1,\n",
       " 'grams_between:<->': 1,\n",
       " 'grams_between:<- -- directions>': 1,\n",
       " 'grams_between:<every>': 1,\n",
       " 'grams_between:<, tablet>': 1,\n",
       " 'grams_between:<daily>': 1,\n",
       " 'grams_between:<name>': 1,\n",
       " 'grams_between:<daily oral>': 1,\n",
       " 'grams_between:<<NUMBER> : capsule>': 1,\n",
       " 'grams_between:<oral tablet>': 1,\n",
       " 'grams_between:<-->': 1,\n",
       " 'grams_between:<oil>': 1,\n",
       " 'grams_between:<=>': 1,\n",
       " 'grams_between:<<NUMBER> for>': 1,\n",
       " 'grams_between:<<NUMBER> oral tablet>': 1,\n",
       " 'grams_between:<, ordered>': 1,\n",
       " 'grams_between:<oral>': 1,\n",
       " 'grams_between:<<NUMBER> : tablet>': 1,\n",
       " 'grams_between:<by>': 1,\n",
       " 'grams_between:<then>': 1,\n",
       " 'grams_between:<:>': 1,\n",
       " 'grams_between:<routine>': 1,\n",
       " 'grams_between:<, by ordered>': 1,\n",
       " 'grams_between:<-- name>': 1,\n",
       " 'grams_between:<- -->': 1,\n",
       " 'grams_between:<: by>': 1,\n",
       " 'grams_between:<and then>': 1,\n",
       " 'grams_between:<, capsule>': 1,\n",
       " 'grams_between:<<NUMBER> capsule>': 1,\n",
       " 'grams_between:<multivitamins>': 1,\n",
       " 'grams_between:<<NUMBER> :>': 1,\n",
       " 'grams_between:<, ordered tablet>': 1,\n",
       " 'grams_between:<every <NUMBER>>': 1,\n",
       " 'grams_between:<<NUMBER> : directions>': 1,\n",
       " 'grams_between:<days>': 1,\n",
       " 'grams_between:<every <NUMBER> hours>': 1,\n",
       " 'grams_between:<, capsule mg>': 1,\n",
       " 'grams_between:<<NUMBER> hours>': 1,\n",
       " 'grams_between:<mg>': 1,\n",
       " 'grams_between:<<NUMBER> tablet>': 1,\n",
       " 'grams_between:<-- :>': 1,\n",
       " 'grams_between:<- directions>': 1,\n",
       " 'grams_between:<<NUMBER>>': 1,\n",
       " 'grams_between:<capsule oral>': 1,\n",
       " 'grams_between:<ordered>': 1,\n",
       " 'grams_between:<daily tablet>': 1,\n",
       " 'grams_between:<- : directions>': 1,\n",
       " 'grams_between:<hours>': 1,\n",
       " 'grams_between:<<NUMBER> capsule oral>': 1,\n",
       " 'grams_between:<- -- :>': 1,\n",
       " 'grams_between:<- -- -->': 1,\n",
       " 'grams_between:<every oral tablet>': 1,\n",
       " 'grams_between:<<NUMBER> days>': 1,\n",
       " 'grams_between:<- -- name>': 1,\n",
       " 'grams_between:<pain>': 1,\n",
       " 'grams_between:<directions>': 1,\n",
       " 'grams_between:<for>': 1,\n",
       " 'grams_between:<capsule>': 1,\n",
       " 'grams_between:<- name>': 1,\n",
       " 'grams_between:<- - -->': 1,\n",
       " 'grams_before:<<>': 1,\n",
       " 'grams_before:<OOV>': 1,\n",
       " 'grams_after:<medications>': 1,\n",
       " 'grams_after:<OOV>': 1,\n",
       " 'grams_after:<stopped>': 1,\n",
       " 'tags_between:<,>': 1,\n",
       " 'tags_between:<: cd>': 1,\n",
       " 'tags_between:<: nnps>': 1,\n",
       " 'tags_between:<, vbn>': 1,\n",
       " 'tags_between:<jj nn>': 1,\n",
       " 'tags_between:<cd jj nn>': 1,\n",
       " 'tags_between:<: :>': 1,\n",
       " 'tags_between:<nnp nnp nns>': 1,\n",
       " 'tags_between:<OOV>': 1,\n",
       " 'tags_between:<nn>': 1,\n",
       " 'tags_between:<cd nnp vbd>': 1,\n",
       " 'tags_between:<dt jj nn>': 1,\n",
       " 'tags_between:<nnp vbd>': 1,\n",
       " 'tags_between:<: nns>': 1,\n",
       " 'tags_between:<, in vbn>': 1,\n",
       " 'tags_between:<vbn>': 1,\n",
       " 'tags_between:<cd nn vbd>': 1,\n",
       " 'tags_between:<nns>': 1,\n",
       " 'tags_between:<nnp nns>': 1,\n",
       " 'tags_between:<jj nnp>': 1,\n",
       " 'tags_between:<: : nn>': 1,\n",
       " 'tags_between:<, nn nnp>': 1,\n",
       " 'tags_between:<in vbn>': 1,\n",
       " 'tags_between:<nn nnp>': 1,\n",
       " 'tags_between:<in nnp>': 1,\n",
       " 'tags_between:<nn nn nnp>': 1,\n",
       " 'tags_between:<nnp nnp nnp>': 1,\n",
       " 'tags_between:<jj nnp nnp>': 1,\n",
       " 'tags_between:<jj jj jj>': 1,\n",
       " 'tags_between:<nnps>': 1,\n",
       " 'tags_between:<cd nnp>': 1,\n",
       " 'tags_between:<jj jj nn>': 1,\n",
       " 'tags_between:<cd nns>': 1,\n",
       " 'tags_between:<cc>': 1,\n",
       " 'tags_between:<: in vbn>': 1,\n",
       " 'tags_between:<: : nns>': 1,\n",
       " 'tags_between:<, nnp vbn>': 1,\n",
       " 'tags_between:<, nnp>': 1,\n",
       " 'tags_between:<cd in>': 1,\n",
       " 'tags_between:<:>': 1,\n",
       " 'tags_between:<dt jj>': 1,\n",
       " 'tags_between:<cd in nns>': 1,\n",
       " 'tags_between:<: in>': 1,\n",
       " 'tags_between:<cd nnp nns>': 1,\n",
       " 'tags_between:<in>': 1,\n",
       " 'tags_between:<nnp nnp>': 1,\n",
       " 'tags_between:<jj nnp vbd>': 1,\n",
       " 'tags_between:<nn nn>': 1,\n",
       " 'tags_between:<nnp nnp vbd>': 1,\n",
       " 'tags_between:<: nn>': 1,\n",
       " 'tags_between:<cd dt nns>': 1,\n",
       " 'tags_between:<: cd nns>': 1,\n",
       " 'tags_between:<cd in nnp>': 1,\n",
       " 'tags_between:<cc rb>': 1,\n",
       " 'tags_between:<jj>': 1,\n",
       " 'tags_between:<vbd>': 1,\n",
       " 'tags_between:<: cd nnps>': 1,\n",
       " 'tags_between:<cd dt jj>': 1,\n",
       " 'tags_between:<cd nn nnp>': 1,\n",
       " 'tags_between:<: cd nn>': 1,\n",
       " 'tags_between:<: : in>': 1,\n",
       " 'tags_between:<jj jj>': 1,\n",
       " 'tags_between:<nnp>': 1,\n",
       " 'tags_between:<dt>': 1,\n",
       " 'tags_between:<rb>': 1,\n",
       " 'tags_between:<cd nn>': 1,\n",
       " 'tags_between:<cd jj>': 1,\n",
       " 'tags_between:<cc nnp>': 1,\n",
       " 'tags_between:<jj nn nnp>': 1,\n",
       " 'tags_between:<cc nnp rb>': 1,\n",
       " 'tags_between:<cd>': 1,\n",
       " 'tags_between:<cd dt>': 1,\n",
       " 'tags_between:<cd nn nn>': 1,\n",
       " 'tags_between:<cd vbd>': 1,\n",
       " 'tags_between:<: : :>': 1,\n",
       " 'tags_between:<cd jj nnp>': 1,\n",
       " 'tags_between:<nnp rb>': 1,\n",
       " 'tags_between:<in nnp rb>': 1,\n",
       " 'tags_between:<cc nnp nnp>': 1,\n",
       " 'tags_between:<jj jj nnp>': 1,\n",
       " 'tags_between:<cd nnp nnp>': 1,\n",
       " 'tags_before:<nnp>': 1,\n",
       " 'tags_before:<nnp nnp>': 1,\n",
       " 'tags_after:<nnp>': 1,\n",
       " 'tags_after:<nnp nns>': 1,\n",
       " 'tags_after:<nns>': 1,\n",
       " 'same_sentence': 1}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_dicts['surface'][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text_in_anno1': 'doxazosin',\n",
       " 'text_in_anno2': 'daily',\n",
       " 'concat_text': 'doxazosin:daily',\n",
       " 'first_entity_type:<DRUG>': 1,\n",
       " 'second_entity_type:<FREQUENCY>': 1,\n",
       " 'entity_types_concat': '<DRUG><=><FREQUENCY>',\n",
       " 'entities_between:<DOSE>': 1,\n",
       " 'entities_between:<ROUTE>': 1,\n",
       " 'entities_between:<FREQUENCY>': 1,\n",
       " 'entities_between:<DRUG>': 1,\n",
       " 'entities_between:<INDICATION>': 1,\n",
       " 'num_entities_between': 27,\n",
       " 'num_sentences_overlap': 1,\n",
       " 'num_tokens_between': 28,\n",
       " 'grams_between:<,>': 1,\n",
       " 'grams_between:<daily oral tablet>': 1,\n",
       " 'grams_between:<hours prn>': 1,\n",
       " 'grams_between:<by ordered>': 1,\n",
       " 'grams_between:<and>': 1,\n",
       " 'grams_between:<every oral>': 1,\n",
       " 'grams_between:<<NUMBER> mg tablet>': 1,\n",
       " 'grams_between:<, mg tablet>': 1,\n",
       " 'grams_between:<-- : by>': 1,\n",
       " 'grams_between:<mg tablet>': 1,\n",
       " 'grams_between:<<NUMBER> days for>': 1,\n",
       " 'grams_between:<OOV>': 1,\n",
       " 'grams_between:<: directions>': 1,\n",
       " 'grams_between:<tablet>': 1,\n",
       " 'grams_between:<prn>': 1,\n",
       " 'grams_between:<: by ordered>': 1,\n",
       " 'grams_between:<capsule mg>': 1,\n",
       " 'grams_between:<<NUMBER> mg>': 1,\n",
       " 'grams_between:<fatty>': 1,\n",
       " 'grams_between:<->': 1,\n",
       " 'grams_between:<- -- directions>': 1,\n",
       " 'grams_between:<every>': 1,\n",
       " 'grams_between:<, tablet>': 1,\n",
       " 'grams_between:<daily>': 1,\n",
       " 'grams_between:<name>': 1,\n",
       " 'grams_between:<daily oral>': 1,\n",
       " 'grams_between:<<NUMBER> : capsule>': 1,\n",
       " 'grams_between:<oral tablet>': 1,\n",
       " 'grams_between:<-->': 1,\n",
       " 'grams_between:<oil>': 1,\n",
       " 'grams_between:<=>': 1,\n",
       " 'grams_between:<<NUMBER> for>': 1,\n",
       " 'grams_between:<<NUMBER> oral tablet>': 1,\n",
       " 'grams_between:<, ordered>': 1,\n",
       " 'grams_between:<oral>': 1,\n",
       " 'grams_between:<<NUMBER> : tablet>': 1,\n",
       " 'grams_between:<by>': 1,\n",
       " 'grams_between:<then>': 1,\n",
       " 'grams_between:<:>': 1,\n",
       " 'grams_between:<routine>': 1,\n",
       " 'grams_between:<, by ordered>': 1,\n",
       " 'grams_between:<-- name>': 1,\n",
       " 'grams_between:<- -->': 1,\n",
       " 'grams_between:<: by>': 1,\n",
       " 'grams_between:<and then>': 1,\n",
       " 'grams_between:<, capsule>': 1,\n",
       " 'grams_between:<<NUMBER> capsule>': 1,\n",
       " 'grams_between:<multivitamins>': 1,\n",
       " 'grams_between:<<NUMBER> :>': 1,\n",
       " 'grams_between:<, ordered tablet>': 1,\n",
       " 'grams_between:<every <NUMBER>>': 1,\n",
       " 'grams_between:<<NUMBER> : directions>': 1,\n",
       " 'grams_between:<days>': 1,\n",
       " 'grams_between:<every <NUMBER> hours>': 1,\n",
       " 'grams_between:<, capsule mg>': 1,\n",
       " 'grams_between:<<NUMBER> hours>': 1,\n",
       " 'grams_between:<mg>': 1,\n",
       " 'grams_between:<<NUMBER> tablet>': 1,\n",
       " 'grams_between:<-- :>': 1,\n",
       " 'grams_between:<- directions>': 1,\n",
       " 'grams_between:<<NUMBER>>': 1,\n",
       " 'grams_between:<capsule oral>': 1,\n",
       " 'grams_between:<ordered>': 1,\n",
       " 'grams_between:<daily tablet>': 1,\n",
       " 'grams_between:<- : directions>': 1,\n",
       " 'grams_between:<hours>': 1,\n",
       " 'grams_between:<<NUMBER> capsule oral>': 1,\n",
       " 'grams_between:<- -- :>': 1,\n",
       " 'grams_between:<- -- -->': 1,\n",
       " 'grams_between:<every oral tablet>': 1,\n",
       " 'grams_between:<<NUMBER> days>': 1,\n",
       " 'grams_between:<- -- name>': 1,\n",
       " 'grams_between:<pain>': 1,\n",
       " 'grams_between:<directions>': 1,\n",
       " 'grams_between:<for>': 1,\n",
       " 'grams_between:<capsule>': 1,\n",
       " 'grams_between:<- name>': 1,\n",
       " 'grams_between:<- - -->': 1,\n",
       " 'grams_before:<<>': 1,\n",
       " 'grams_before:<OOV>': 1,\n",
       " 'grams_after:<medications>': 1,\n",
       " 'grams_after:<OOV>': 1,\n",
       " 'grams_after:<stopped>': 1,\n",
       " 'tags_between:<,>': 1,\n",
       " 'tags_between:<: cd>': 1,\n",
       " 'tags_between:<: nnps>': 1,\n",
       " 'tags_between:<, vbn>': 1,\n",
       " 'tags_between:<jj nn>': 1,\n",
       " 'tags_between:<cd jj nn>': 1,\n",
       " 'tags_between:<: :>': 1,\n",
       " 'tags_between:<nnp nnp nns>': 1,\n",
       " 'tags_between:<OOV>': 1,\n",
       " 'tags_between:<nn>': 1,\n",
       " 'tags_between:<cd nnp vbd>': 1,\n",
       " 'tags_between:<dt jj nn>': 1,\n",
       " 'tags_between:<nnp vbd>': 1,\n",
       " 'tags_between:<: nns>': 1,\n",
       " 'tags_between:<, in vbn>': 1,\n",
       " 'tags_between:<vbn>': 1,\n",
       " 'tags_between:<cd nn vbd>': 1,\n",
       " 'tags_between:<nns>': 1,\n",
       " 'tags_between:<nnp nns>': 1,\n",
       " 'tags_between:<jj nnp>': 1,\n",
       " 'tags_between:<: : nn>': 1,\n",
       " 'tags_between:<, nn nnp>': 1,\n",
       " 'tags_between:<in vbn>': 1,\n",
       " 'tags_between:<nn nnp>': 1,\n",
       " 'tags_between:<in nnp>': 1,\n",
       " 'tags_between:<nn nn nnp>': 1,\n",
       " 'tags_between:<nnp nnp nnp>': 1,\n",
       " 'tags_between:<jj nnp nnp>': 1,\n",
       " 'tags_between:<jj jj jj>': 1,\n",
       " 'tags_between:<nnps>': 1,\n",
       " 'tags_between:<cd nnp>': 1,\n",
       " 'tags_between:<jj jj nn>': 1,\n",
       " 'tags_between:<cd nns>': 1,\n",
       " 'tags_between:<cc>': 1,\n",
       " 'tags_between:<: in vbn>': 1,\n",
       " 'tags_between:<: : nns>': 1,\n",
       " 'tags_between:<, nnp vbn>': 1,\n",
       " 'tags_between:<, nnp>': 1,\n",
       " 'tags_between:<cd in>': 1,\n",
       " 'tags_between:<:>': 1,\n",
       " 'tags_between:<dt jj>': 1,\n",
       " 'tags_between:<cd in nns>': 1,\n",
       " 'tags_between:<: in>': 1,\n",
       " 'tags_between:<cd nnp nns>': 1,\n",
       " 'tags_between:<in>': 1,\n",
       " 'tags_between:<nnp nnp>': 1,\n",
       " 'tags_between:<jj nnp vbd>': 1,\n",
       " 'tags_between:<nn nn>': 1,\n",
       " 'tags_between:<nnp nnp vbd>': 1,\n",
       " 'tags_between:<: nn>': 1,\n",
       " 'tags_between:<cd dt nns>': 1,\n",
       " 'tags_between:<: cd nns>': 1,\n",
       " 'tags_between:<cd in nnp>': 1,\n",
       " 'tags_between:<cc rb>': 1,\n",
       " 'tags_between:<jj>': 1,\n",
       " 'tags_between:<vbd>': 1,\n",
       " 'tags_between:<: cd nnps>': 1,\n",
       " 'tags_between:<cd dt jj>': 1,\n",
       " 'tags_between:<cd nn nnp>': 1,\n",
       " 'tags_between:<: cd nn>': 1,\n",
       " 'tags_between:<: : in>': 1,\n",
       " 'tags_between:<jj jj>': 1,\n",
       " 'tags_between:<nnp>': 1,\n",
       " 'tags_between:<dt>': 1,\n",
       " 'tags_between:<rb>': 1,\n",
       " 'tags_between:<cd nn>': 1,\n",
       " 'tags_between:<cd jj>': 1,\n",
       " 'tags_between:<cc nnp>': 1,\n",
       " 'tags_between:<jj nn nnp>': 1,\n",
       " 'tags_between:<cc nnp rb>': 1,\n",
       " 'tags_between:<cd>': 1,\n",
       " 'tags_between:<cd dt>': 1,\n",
       " 'tags_between:<cd nn nn>': 1,\n",
       " 'tags_between:<cd vbd>': 1,\n",
       " 'tags_between:<: : :>': 1,\n",
       " 'tags_between:<cd jj nnp>': 1,\n",
       " 'tags_between:<nnp rb>': 1,\n",
       " 'tags_between:<in nnp rb>': 1,\n",
       " 'tags_between:<cc nnp nnp>': 1,\n",
       " 'tags_between:<jj jj nnp>': 1,\n",
       " 'tags_between:<cd nnp nnp>': 1,\n",
       " 'tags_before:<nnp>': 1,\n",
       " 'tags_before:<nnp nnp>': 1,\n",
       " 'tags_after:<nnp>': 1,\n",
       " 'tags_after:<nnp nns>': 1,\n",
       " 'tags_after:<nns>': 1,\n",
       " 'same_sentence': 1}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_dicts['entities+entities_between+surface'][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in test data\n",
    "test_reader = made_utils.TextAndBioCParser(VALDIR)\n",
    "test_docs = test_reader.read_texts_and_xmls(num_docs=-1, include_relations=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_relations = []\n",
    "for i, (fname, doc) in enumerate(test_docs.items()):\n",
    "    if i % 10 == 0:\n",
    "        print('-{}: {} '.format(i, fname))\n",
    "        print(len(doc.relations))\n",
    "    new_relations = pair_annotations_in_doc(doc)\n",
    "    \n",
    "    # Add Fake relations for training\n",
    "    neg_relations = set(new_relations).difference(set(doc.relations))\n",
    "    doc.add_relations(neg_relations)\n",
    "    if i % 10 == 0:\n",
    "        print(len(doc.relations))\n",
    "        \n",
    "    test_relations += doc.relations\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open('tmp_data/val_docs_and_relations.pkl', 'wb') as f:\n",
    "    pickle.dump((test_docs, test_relations), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# feature_extractor = LexicalFeatureExtractor(context_window=(2, 2),\n",
    "#                             ngram_window=(1, 3), vocab=vocab, pos_vocab=pos_vocab,\n",
    "#                             min_vocab_count=20, min_pos_count=20)\n",
    "feat_dicts_test = {feature_set_name: [] for feature_set_name in \n",
    "              ('entities', 'entities_between', 'surface', \n",
    "               'entities+entities_between', 'entities+entities_between+surface')}\n",
    "for i, r in enumerate(test_relations):\n",
    "    if i % 100 == 0:\n",
    "        print(\"{}/{}\".format(i, len(test_relations)))\n",
    "    doc = test_docs[r.file_name]\n",
    "    # Single feature sets\n",
    "    entities_feat_dict = feature_extractor.create_feature_dict(r, doc, entities=True, entities_between=False, surface=False)\n",
    "    entities_between_dict = feature_extractor.create_feature_dict(r, doc, entities=False, entities_between=True, surface=False)\n",
    "    surface_dict = feature_extractor.create_feature_dict(r, doc, entities=False, entities_between=False, surface=True)\n",
    "    feat_dicts_test['entities'].append(entities_feat_dict)\n",
    "    feat_dicts_test['entities_between'].append(entities_between_dict)\n",
    "    feat_dicts_test['surface'].append(surface_dict)\n",
    "    \n",
    "    # Now create the combinations\n",
    "    combo = {}\n",
    "    combo.update(entities_feat_dict)\n",
    "    combo.update(entities_between_dict)\n",
    "    \n",
    "    feat_dicts_test['entities+entities_between'].append(combo)\n",
    "    combo2 = {}\n",
    "    combo2.update(combo)\n",
    "    combo2.update(surface_dict)\n",
    "#     feat_dicts['entities+entities_between'].append(combo)\n",
    "    feat_dicts_test['entities+entities_between+surface'].append(combo2)\n",
    "#     break\n",
    "    continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Magnesium oxide':'a day', Drug:Frequency, type=none"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'concat_text': 'magnesium oxide:a day',\n",
       " 'entity_types_concat': '<FREQUENCY><=><DRUG>',\n",
       " 'first_entity_type:<DRUG>': 1,\n",
       " 'grams_after:<<NUMBER> mg>': 1,\n",
       " 'grams_after:<<NUMBER>>': 1,\n",
       " 'grams_after:<mg>': 1,\n",
       " 'grams_before:<<NUMBER> mcg>': 1,\n",
       " 'grams_before:<<NUMBER>>': 1,\n",
       " 'grams_before:<mcg>': 1,\n",
       " 'grams_between:<. . <NUMBER>>': 1,\n",
       " 'grams_between:<. <NUMBER>>': 1,\n",
       " 'grams_between:<.>': 1,\n",
       " 'grams_between:<<NUMBER>>': 1,\n",
       " 'num_entities_between': 0,\n",
       " 'num_sentences_overlap': 3,\n",
       " 'num_tokens_between': 2,\n",
       " 'same_sentence': 0,\n",
       " 'second_entity_type:<FREQUENCY>': 1,\n",
       " 'tags_after:<cd nns>': 1,\n",
       " 'tags_after:<cd>': 1,\n",
       " 'tags_after:<nns>': 1,\n",
       " 'tags_before:<cd vbd>': 1,\n",
       " 'tags_before:<cd>': 1,\n",
       " 'tags_before:<vbd>': 1,\n",
       " 'tags_between:<. . cd>': 1,\n",
       " 'tags_between:<. cd>': 1,\n",
       " 'tags_between:<.>': 1,\n",
       " 'tags_between:<cd>': 1,\n",
       " 'text_in_anno1': 'magnesium oxide',\n",
       " 'text_in_anno2': 'a day'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_dicts['entities+entities_between+surface'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lidocaine':'anesthesia', Drug:Indication, type=none"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_relations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'concat_text': 'lidocaine:anesthesia',\n",
       " 'entity_types_concat': '<DRUG><=><INDICATION>',\n",
       " 'first_entity_type:<DRUG>': 1,\n",
       " 'grams_after:<OOV>': 1,\n",
       " 'grams_before:<% <NUMBER>>': 1,\n",
       " 'grams_before:<%>': 1,\n",
       " 'grams_before:<<NUMBER>>': 1,\n",
       " 'grams_between:<OOV>': 1,\n",
       " 'grams_between:<injected>': 1,\n",
       " 'grams_between:<local>': 1,\n",
       " 'grams_between:<then was>': 1,\n",
       " 'grams_between:<then>': 1,\n",
       " 'grams_between:<to>': 1,\n",
       " 'grams_between:<was>': 1,\n",
       " 'num_entities_between': 0,\n",
       " 'num_sentences_overlap': 1,\n",
       " 'num_tokens_between': 6,\n",
       " 'same_sentence': 1,\n",
       " 'second_entity_type:<INDICATION>': 1,\n",
       " 'tags_after:<OOV>': 1,\n",
       " 'tags_before:<cd nn>': 1,\n",
       " 'tags_before:<cd>': 1,\n",
       " 'tags_before:<nn>': 1,\n",
       " 'tags_between:<jj to vb>': 1,\n",
       " 'tags_between:<jj vb>': 1,\n",
       " 'tags_between:<jj>': 1,\n",
       " 'tags_between:<rb to vbn>': 1,\n",
       " 'tags_between:<rb vbd vbn>': 1,\n",
       " 'tags_between:<rb vbd>': 1,\n",
       " 'tags_between:<rb vbn>': 1,\n",
       " 'tags_between:<rb>': 1,\n",
       " 'tags_between:<to vb vbn>': 1,\n",
       " 'tags_between:<to vb>': 1,\n",
       " 'tags_between:<to vbn>': 1,\n",
       " 'tags_between:<to>': 1,\n",
       " 'tags_between:<vb>': 1,\n",
       " 'tags_between:<vbd>': 1,\n",
       " 'tags_between:<vbn>': 1,\n",
       " 'text_in_anno1': 'lidocaine',\n",
       " 'text_in_anno2': 'anesthesia'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_dicts_test['entities+entities_between+surface'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37666"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set([r.id for r in relations]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vectorizer = DictVectorizer(sparse=True, sort=True)\n",
    "# TODO: Freeze the 1000 features that we use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entities': {'bin': None, 'full': None},\n",
       " 'entities+entities_between': {'bin': None, 'full': None},\n",
       " 'entities+entities_between+surface': {'bin': None, 'full': None},\n",
       " 'entities_between': {'bin': None, 'full': None},\n",
       " 'surface': {'bin': None, 'full': None}}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_full = [r.type for r in relations]\n",
    "y_bin = ['any' if y_ != 'none' else y_ for y_ in y_full]\n",
    "y_dict = {'bin': y_bin,\n",
    "    'full': y_full}\n",
    "X_dict = {feature_set_name: {'bin': None, 'full': None} for feature_set_name in feat_dicts.keys()}\n",
    "X_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entities': {'bin': None, 'full': None},\n",
       " 'entities+entities_between': {'bin': None, 'full': None},\n",
       " 'entities+entities_between+surface': {'bin': None, 'full': None},\n",
       " 'entities_between': {'bin': None, 'full': None},\n",
       " 'surface': {'bin': None, 'full': None}}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_full_test = [r.type for r in test_relations]\n",
    "y_bin_test = ['any' if y_ != 'none' else y_ for y_ in y_full_test]\n",
    "y_dict_test = {'bin': y_bin_test,\n",
    "    'full': y_full_test}\n",
    "X_dict_test = {feature_set_name: {'bin': None, 'full': None} for feature_set_name in feat_dicts_test.keys()}\n",
    "X_dict_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37676"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feat_dicts['entities+entities_between'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21053"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feat_dicts_test['entities+entities_between'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_selectors = {feature_set_name: {'bin': None, 'full': None} for feature_set_name in X_dict_test.keys()}\n",
    "vectorizers = {feature_set_name: None for feature_set_name in X_dict_test.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entities\n",
      "(37676, 26577)\n",
      "(37676, 1000) (37676, 1000)\n",
      "entities_between\n",
      "(37676, 11)\n",
      "(37676, 11) (37676, 11)\n",
      "surface\n",
      "(37676, 15931)\n",
      "(37676, 1000) (37676, 1000)\n",
      "entities+entities_between\n",
      "(37676, 26588)\n",
      "(37676, 1000) (37676, 1000)\n",
      "entities+entities_between+surface\n",
      "(37676, 42519)\n",
      "(37676, 1000) (37676, 1000)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for feature_set_name, features in feat_dicts.items():\n",
    "    vectorizer = DictVectorizer(sparse=True, sort=True)\n",
    "    print(feature_set_name)\n",
    "    k = 1000\n",
    "    X_vector = vectorizer.fit_transform(features)\n",
    "    print(X_vector.shape)\n",
    "    vectorizers[feature_set_name] = vectorizer\n",
    "    \n",
    "#     print(X.shape)\n",
    "    try:\n",
    "        binary_feature_selector = base_feature.MyFeatureSelector(vectorizer, k=k)\n",
    "        X_bin = binary_feature_selector.fit_transform(X_vector, y_dict['bin'])\n",
    "        \n",
    "        \n",
    "        full_feature_selector = base_feature.MyFeatureSelector(vectorizer, k=k)\n",
    "        X_full = full_feature_selector.fit_transform(X_vector, y_dict['full']) \n",
    "        \n",
    "        \n",
    "    except ValueError as e: # Not enough features\n",
    "#         vectorizer = DictVectorizer(sparse=True, sort=True)\n",
    "#         X_vector = vectorizer.fit_transform(features)\n",
    "        binary_feature_selector = base_feature.MyFeatureSelector(vectorizer, k='all')\n",
    "        X_bin = binary_feature_selector.fit_transform(X_vector, y_dict['bin'])\n",
    "        \n",
    "        full_feature_selector = base_feature.MyFeatureSelector(vectorizer, k='all')\n",
    "        X_full = full_feature_selector.fit_transform(X_vector, y_dict['full']) \n",
    "    \n",
    "    print(X_bin.shape, X_full.shape)\n",
    "    X_dict[feature_set_name]['bin'] = X_bin\n",
    "    X_dict[feature_set_name]['full'] = X_full\n",
    "    \n",
    "    feature_selectors[feature_set_name]['bin'] = binary_feature_selector\n",
    "    feature_selectors[feature_set_name]['full'] = full_feature_selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entities\n",
      "26577\n",
      "(21053, 26577)\n",
      "entities_between\n",
      "11\n",
      "(21053, 11)\n",
      "surface\n",
      "15931\n",
      "(21053, 15931)\n",
      "entities+entities_between\n",
      "26588\n",
      "(21053, 26588)\n",
      "entities+entities_between+surface\n",
      "42519\n",
      "(21053, 42519)\n"
     ]
    }
   ],
   "source": [
    "# Now transform the test feature dicts\n",
    "\n",
    "for feature_set_name, features in feat_dicts_test.items():\n",
    "    print(feature_set_name)\n",
    "\n",
    "    binary_feature_selector = feature_selectors[feature_set_name]['bin']\n",
    "    full_feature_selector = feature_selectors[feature_set_name]['full']\n",
    "    print(len(binary_feature_selector.vectorizer.get_feature_names()))\n",
    "#     break\n",
    "    \n",
    "    try:\n",
    "        X_bin = binary_feature_selector.vectorizer.transform(features)\n",
    "        print(X_bin.shape)\n",
    "        X_bin = binary_feature_selector.transform(X_bin)\n",
    "        X_full = full_feature_selector.vectorizer.transform(features)\n",
    "        X_full = full_feature_selector.transform(X_full) \n",
    "    except ValueError as e: # Not enough features\n",
    "        raise e\n",
    "       \n",
    "    \n",
    "    \n",
    "    X_dict_test[feature_set_name]['bin'] = X_bin\n",
    "    X_dict_test[feature_set_name]['full'] = X_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open('tmp_data/X_dicts.pkl', 'wb') as f:\n",
    "    pickle.dump((X_dict, X_dict_test), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open('tmp_data/X_dict_test.pkl', 'wb') as f:\n",
    "    pickle.dump(X_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('tmp_data/X_dicts.pkl', 'rb') as f:\n",
    "    X_dict, X_dict_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entities': {'bin': <37676x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 136731 stored elements in Compressed Sparse Row format>,\n",
       "  'full': <37676x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 163994 stored elements in Compressed Sparse Row format>},\n",
       " 'entities+entities_between': {'bin': <37676x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 226551 stored elements in Compressed Sparse Row format>,\n",
       "  'full': <37676x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 253424 stored elements in Compressed Sparse Row format>},\n",
       " 'entities+entities_between+surface': {'bin': <37676x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 1661898 stored elements in Compressed Sparse Row format>,\n",
       "  'full': <37676x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 1835785 stored elements in Compressed Sparse Row format>},\n",
       " 'entities_between': {'bin': <37676x11 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 89917 stored elements in Compressed Sparse Row format>,\n",
       "  'full': <37676x11 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 89917 stored elements in Compressed Sparse Row format>},\n",
       " 'surface': {'bin': <37676x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 1545439 stored elements in Compressed Sparse Row format>,\n",
       "  'full': <37676x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 1687203 stored elements in Compressed Sparse Row format>}}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entities': {'bin': <21053x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 75417 stored elements in Compressed Sparse Row format>,\n",
       "  'full': <21053x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 89225 stored elements in Compressed Sparse Row format>},\n",
       " 'entities+entities_between': {'bin': <21053x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 142576 stored elements in Compressed Sparse Row format>,\n",
       "  'full': <21053x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 156041 stored elements in Compressed Sparse Row format>},\n",
       " 'entities+entities_between+surface': {'bin': <21053x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 1251121 stored elements in Compressed Sparse Row format>,\n",
       "  'full': <21053x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 1315580 stored elements in Compressed Sparse Row format>},\n",
       " 'entities_between': {'bin': <21053x11 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 67172 stored elements in Compressed Sparse Row format>,\n",
       "  'full': <21053x11 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 67172 stored elements in Compressed Sparse Row format>},\n",
       " 'surface': {'bin': <21053x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 1169401 stored elements in Compressed Sparse Row format>,\n",
       "  'full': <21053x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 1231452 stored elements in Compressed Sparse Row format>}}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dict_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we can train and evaluate each set\n",
    "def train_clf(X, y, cross_val=False):\n",
    "    \"\"\"\n",
    "    Trains and validates a model using cross-validation.\n",
    "    \"\"\"\n",
    "    clf = RandomForestClassifier(max_depth = None,\n",
    "                            max_features = None,\n",
    "                            min_samples_leaf = 2,\n",
    "                            min_samples_split = 2,\n",
    "                            n_estimators = 10,\n",
    "                            n_jobs = 1)\n",
    "    if cross_val:\n",
    "        # Cross-validate to make sure this is going right\n",
    "        pred = cross_val_predict(clf, X, y)\n",
    "        print(classification_report(y, pred))\n",
    "    clf.fit(X, y)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clfs = {feature_set_name: {\"bin\": None, \"full\": None} for feature_set_name in feat_dicts.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_dict = {feature_set_name: [] for feature_set_name in feat_dicts.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entities+entities_between+surface\n",
      "bin entities+entities_between+surface\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-63f8cf906b35>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mX_train_bin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_bin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature_set_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'bin'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'bin'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mX_test_bin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_bin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_dict_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature_set_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'bin'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_dict_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'bin'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mbin_clf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_clf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_bin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_bin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mclfs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature_set_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'bin'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbin_clf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-55-0e4831b02e70>\u001b[0m in \u001b[0;36mtrain_clf\u001b[1;34m(X, y, cross_val)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\made\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    325\u001b[0m                     \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[1;32m--> 327\u001b[1;33m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[0;32m    328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m             \u001b[1;31m# Collect newly grown trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\made\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    777\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\made\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    623\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\made\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 588\u001b[1;33m         \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\made\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\made\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\made\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\made\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\made\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'balanced'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\made\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    788\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 790\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m    791\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    792\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\made\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    360\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# for feature_set_name in X_dict.keys():\n",
    "for feature_set_name in ['entities+entities_between+surface',]:\n",
    "    print(feature_set_name)\n",
    "    # First, train and get predictions for binary classifier\n",
    "    print('bin', feature_set_name)\n",
    "    X_train_bin, y_train_bin = X_dict[feature_set_name]['bin'], y_dict['bin']\n",
    "    X_test_bin, y_test_bin = X_dict_test[feature_set_name]['bin'], y_dict_test['bin']\n",
    "    bin_clf = train_clf(X_train_bin, y_train_bin)\n",
    "    \n",
    "    clfs[feature_set_name]['bin'] = bin_clf\n",
    "    \n",
    "    \n",
    "    # Now, train and get predictions full classifier\n",
    "    X_train_full = X_dict[feature_set_name]['full']\n",
    "    y_train_full = y_dict['full']\n",
    "    \n",
    "    X_test_full, y_test_full = X_dict_test[feature_set_name]['full'], y_dict_test['full']\n",
    "    \n",
    "    print('full', feature_set_name)\n",
    "    try:\n",
    "        full_clf = train_clf(X_train_full, y_train_full)\n",
    "        clfs[feature_set_name]['full'] = full_clf\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "        print(X_dict[feature_set_name]['full'], y_dict['full'])\n",
    "        raise e\n",
    "        \n",
    "        \n",
    "    # Now predict on the test set\n",
    "    # Now get the agreed upon scores\n",
    "    from collections import Counter\n",
    "    \n",
    "    print(\"Predicting binary test: {}\".format(X_test_bin.shape))\n",
    "    pred_bin = bin_clf.predict(X_test_bin)\n",
    "    \n",
    "    print(Counter(pred_bin))\n",
    "    print(\"Predicting binary test: {}\".format(X_test_full.shape))\n",
    "    pred_full = full_clf.predict(X_test_full)\n",
    "    print(Counter(pred_full))\n",
    "    \n",
    "    \n",
    "    y_pred_dict[feature_set_name+'_no_binary'] = pred_full\n",
    "    y_pred_dict[feature_set_name] = [pred_full[i] if pred_bin[i] != 'none' else 'none' for i in range(len(pred_full))]\n",
    "print(\"Finished training and predicting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reason' 'do' 'none' ..., 'fr' 'none' 'severity_type']\n"
     ]
    }
   ],
   "source": [
    "no_filter_pred = clfs['entities+entities_between+surface']['full'].predict(X_dict_test['entities+entities_between+surface']['full'])\n",
    "print(no_filter_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('tmp_data/preds.pkl', 'wb') as f:\n",
    "    pickle.dump(y_pred_dict, f)\n",
    "with open('tmp_data/clfs.pkl', 'wb') as f:\n",
    "    pickle.dump(clfs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('tmp_data/preds.pkl', 'rb') as f:\n",
    "    y_pred_dict = pickle.load(f)\n",
    "with open('tmp_data/clfs.pkl', 'rb') as f:\n",
    "    clfs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('tmp_data/preds.pkl', 'rb') as f:\n",
    "    y_pred_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entities\n",
      "entities_between\n",
      "surface\n",
      "entities+entities_between\n",
      "entities+entities_between+surface\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Write out bioc annotations\n",
    "# Remove any duplicates that somehow got in\n",
    "\n",
    "for doc in test_docs.values():\n",
    "    doc.relations = []\n",
    "    existing_annos = set()\n",
    "    to_add = []\n",
    "    for i, anno in enumerate(doc.annotations):\n",
    "        if anno.id not in existing_annos:\n",
    "            to_add.append(anno)\n",
    "            existing_annos.add(anno.id)\n",
    "    doc.annotations = to_add\n",
    "\n",
    "from collections import defaultdict\n",
    "relations_already_seen = []\n",
    "\n",
    "for feature_set_name in y_pred_dict.keys():\n",
    "# for feature_set_name in ('entities+entities_between+surface',):\n",
    "    print(feature_set_name)\n",
    "    for i in range(len(y_pred_dict[feature_set_name])):\n",
    "        p = y_pred_dict[feature_set_name][i]\n",
    "    #     print(p); break\n",
    "        r = test_relations[i]\n",
    "        r.type = p\n",
    "        doc = test_docs[r.file_name]\n",
    "        if r.type != 'none':\n",
    "            doc.relations.append(r)\n",
    "\n",
    "\n",
    "    for doc in test_docs.values():\n",
    "        existing_relats = set()\n",
    "        to_add = []\n",
    "        for i, relat in enumerate(doc.relations):\n",
    "            if relat.id not in existing_relats:\n",
    "                to_add.append(relat)\n",
    "                existing_relats.add(relat.id)\n",
    "        doc.relations = to_add\n",
    "\n",
    "    OUTDIR = 'tmp_data/output_{}'.format(feature_set_name)\n",
    "    if not os.path.exists(OUTDIR):\n",
    "        os.mkdir(OUTDIR)\n",
    "    for d in test_docs.values():\n",
    "        d.to_bioc_xml(OUTDIR)\n",
    "        \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/176\n",
      "100/176\n"
     ]
    }
   ],
   "source": [
    "# Read the documents back in, this time with true relations\n",
    "# Read in test data\n",
    "gold_test_reader = made_utils.TextAndBioCParser(VALDIR)\n",
    "gold_test_docs = gold_test_reader.read_texts_and_xmls(num_docs=-1, include_relations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4206"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_test_relations = []\n",
    "for doc in gold_test_docs.values():\n",
    "    gold_test_relations += doc.relations\n",
    "len(gold_test_relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21053"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make sure that our predictions are using the full feature set\n",
    "for i in range(len(y_pred_dict[feature_set_name])):\n",
    "    p = y_pred_dict['entities+entities_between+surface'][i]\n",
    "#     print(p); break\n",
    "    r = test_relations[i]\n",
    "    r.type = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a set of all anno -> anno edges in gold\n",
    "gold_edges = {file_name: {} for file_name in gold_test_docs.keys()}\n",
    "for relat in gold_test_relations:\n",
    "     gold_edges[relat.file_name][(relat.annotation_1.start_index,\n",
    "                               relat.annotation_2.end_index)] = relat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_edges = {file_name: {} for file_name in test_docs.keys()}\n",
    "for relat in test_relations:\n",
    "    # Exclude any 'none' relations\n",
    "    if relat.type == 'none':\n",
    "        continue\n",
    "    pred_edges[relat.file_name][(relat.annotation_1.start_index,\n",
    "                               relat.annotation_2.end_index)] = relat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of errors\n",
    "\n",
    "- **Relation-Type Errors** - We predicted a relation between the two entities but the wrong relation\n",
    "- **False Negative** - There is a relation, but we missed it\n",
    "- **False Positive** - There is a relation between the two, but we missed it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type_errors = []\n",
    "false_negatives = []\n",
    "for f_name, anno_edges in gold_edges.items():\n",
    "    for anno_edge in anno_edges:\n",
    "        # If there's not a relation between two annotations, it's a false negative\n",
    "        if anno_edge not in pred_edges[f_name]:\n",
    "            false_negatives.append(anno_edges[anno_edge])\n",
    "        else:\n",
    "            true_relat = anno_edges[anno_edge]\n",
    "            pred_relat = pred_edges[f_name][anno_edge]\n",
    "            if true_relat.type != pred_relat.type:\n",
    "                type_errors.append((true_relat, pred_relat))\n",
    "#         if anno_edges[anno_edge] \n",
    "    \n",
    "# Now go through and find the false positives\n",
    "false_positives = []\n",
    "for f_name, anno_edges in pred_edges.items():\n",
    "    for anno_edge in anno_edges:\n",
    "        if anno_edge not in gold_edges[f_name]:\n",
    "            false_positives.append(anno_edges[anno_edge])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Type Errors: 12\n",
      "Number of False Negatives: 363\n",
      "Number of False Positives: 332\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Type Errors: {}\".format(len(type_errors)))\n",
    "print(\"Number of False Negatives: {}\".format(len(false_negatives)))\n",
    "print(\"Number of False Positives: {}\".format(len(false_positives)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('chemotherapy':'shingles', Drug:ADE, type=adverse,\n",
       "  'chemotherapy':'shingles', Drug:Indication, type=reason),\n",
       " ('Carafate':'mucositis', Drug:Indication, type=reason,\n",
       "  'Carafate':'mucositis', Drug:ADE, type=adverse),\n",
       " ('chemo':'shingles', Drug:ADE, type=adverse,\n",
       "  'chemo':'shingles', Drug:Indication, type=reason),\n",
       " ('steroids':'decreased bone density', Drug:ADE, type=adverse,\n",
       "  'steroids':'decreased bone density', Drug:Indication, type=reason),\n",
       " ('steroids':'low testosterone level', Drug:ADE, type=adverse,\n",
       "  'steroids':'low testosterone level', Drug:Indication, type=reason),\n",
       " ('Zofran':'vomiting', Drug:Indication, type=reason,\n",
       "  'Zofran':'vomiting', Drug:ADE, type=adverse),\n",
       " ('scopolamine':'nausea', Drug:Indication, type=reason,\n",
       "  'scopolamine':'nausea', Drug:ADE, type=adverse),\n",
       " ('Benadryl':'rash', Drug:Indication, type=reason,\n",
       "  'Benadryl':'rash', Drug:ADE, type=adverse),\n",
       " ('steroid':'dyspnea', Drug:ADE, type=adverse,\n",
       "  'steroid':'dyspnea', Drug:Indication, type=reason),\n",
       " ('steroid':'blood pressure going up', Drug:ADE, type=adverse,\n",
       "  'steroid':'blood pressure going up', Drug:Indication, type=reason),\n",
       " ('steroid':'edema', Drug:ADE, type=adverse,\n",
       "  'steroid':'edema', Drug:Indication, type=reason),\n",
       " ('glucocorticoids':'Vitamin D deficiency', Drug:ADE, type=adverse,\n",
       "  'glucocorticoids':'Vitamin D deficiency', Drug:Indication, type=reason)]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (Truth, Pred)\n",
    "type_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6_917\n",
      "Truth: reason\n",
      "Pred: adverse\n",
      "-    o monitored. --- Name --- did have some <REASON><INDICATION>nausea</INDICATION> that was refractory to these <DRUG>scopolamine</DRUG></REASON>  patch and Zofran and was additionally g\n",
      "\n",
      "10_758\n",
      "Truth: adverse\n",
      "Pred: reason\n",
      "-    c  fever with the previous 5 cycles of <ADVERSE><DRUG>chemo</DRUG>, but did have one episode of <ADE>shingles</ADE></ADVERSE>   and has been treated on valacyclovir.\n",
      "\n",
      "7_410\n",
      "Truth: adverse\n",
      "Pred: reason\n",
      "-    ompression fractures. I also think the <ADVERSE><DRUG>steroid</DRUG> regimen may have increased her volume and that has resulted in <ADE>blood pressure going up</ADE></ADVERSE>  as well as increased edema. I therefor\n",
      "\n",
      "3_405\n",
      "Truth: reason\n",
      "Pred: adverse\n",
      "-     and drink, has occasional nausea  and <REASON><INDICATION>vomiting</INDICATION>, which was adequately controlled by <DRUG>Zofran</DRUG></REASON> . The patient has  been fever-free sin\n",
      "\n",
      "10_197\n",
      "Truth: adverse\n",
      "Pred: reason\n",
      "-    fever with either of the cycles of this <ADVERSE><DRUG>chemotherapy</DRUG>, but he did have 1 episode of <ADE>shingles</ADE></ADVERSE> , after which he has been put on valacyc\n",
      "\n",
      "7_410\n",
      "Truth: adverse\n",
      "Pred: reason\n",
      "-    - Name ---, I suspect that --- Name --- <ADVERSE><ADE>dyspnea</ADE> is likely due to the combined effects of worsening restrictive lung disease as by exam her kyphosis and scoliosis seems to be worsening significantly in the setting of her multiple compression fractures. I also think the <DRUG>steroid</DRUG></ADVERSE>  regimen may have increased her volume a\n",
      "\n",
      "7_410\n",
      "Truth: adverse\n",
      "Pred: reason\n",
      "-    ompression fractures. I also think the <ADVERSE><DRUG>steroid</DRUG> regimen may have increased her volume and that has resulted in blood pressure going up as well as increased <ADE>edema</ADE></ADVERSE> . I therefore had to double her furosem\n",
      "\n",
      "1_437\n",
      "Truth: adverse\n",
      "Pred: reason\n",
      "-    re could be additional risk factors for <ADVERSE><ADE>decreased bone density</ADE>. He has  been on <DRUG>steroids</DRUG></ADVERSE>  and he has now newly found low testoste\n",
      "\n",
      "7_472\n",
      "Truth: adverse\n",
      "Pred: reason\n",
      "-     taking extra calcium supplements.  2. <ADVERSE><ADE>Vitamin D deficiency</ADE>. Since at least --- Date ---. Most likely aggravated by the <DRUG>glucocorticoids</DRUG></ADVERSE> . Her achiness in her leg may be associ\n",
      "\n",
      "6_917\n",
      "Truth: reason\n",
      "Pred: adverse\n",
      "-    is believed that this is a methotrexate <REASON><INDICATION>rash</INDICATION> and he was given <DRUG>Benadryl</DRUG></REASON>  after which his symptoms resolved. ---\n",
      "\n",
      "10_508\n",
      "Truth: reason\n",
      "Pred: adverse\n",
      "-     well. The patient  had a few days of <REASON><INDICATION>mucositis</INDICATION> which resolved quickly with <DRUG>Carafate</DRUG></REASON>  and Miracle  Mouthwash. Currently, th\n",
      "\n",
      "1_437\n",
      "Truth: adverse\n",
      "Pred: reason\n",
      "-    creased bone density. He has  been on <ADVERSE><DRUG>steroids</DRUG> and he has now newly found <ADE>low testosterone level</ADE></ADVERSE> .  At this time, I will add a vitamin D\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for (truth_relat, pred_relat) in random.sample(type_errors, len(type_errors)):\n",
    "    doc = gold_test_docs[truth_relat.file_name]\n",
    "    print(doc.file_name)\n",
    "    print(\"Truth: {}\".format(truth_relat.type))\n",
    "    print(\"Pred: {}\".format(pred_relat.type))\n",
    "    print('-    ' + truth_relat.get_example_string(doc))\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "for truth_relat in random.sample(false_negatives, 100):\n",
    "    doc = gold_test_docs[truth_relat.file_name]\n",
    "    print(doc.file_name)\n",
    "    print(\"Truth: {}\".format(truth_relat))\n",
    "    print('-    ' + truth_relat.get_example_string(doc))\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for pred_relat in random.sample(false_positives, 100):\n",
    "    doc = test_docs[pred_relat.file_name]\n",
    "    print(doc.file_name)\n",
    "    print(\"Pred: {}\".format(pred_relat))\n",
    "    print('-    ' + pred_relat.get_example_string(doc))\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_docs['19_566'].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "false_negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "false_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'concat_text=acyclovir:2 capsule': 1.0,\n",
       " 'concat_text=air hunger:some': 1.0,\n",
       " 'concat_text=antivirals:prophylactic': 1.0,\n",
       " 'concat_text=aspirin:few cycles': 6.0,\n",
       " 'concat_text=aspirin:in the a.m': 1.0,\n",
       " 'concat_text=back pain:80%': 1.0,\n",
       " 'concat_text=back pain:adverse reaction': 1.0,\n",
       " 'concat_text=back pain:constipation': 1.0,\n",
       " 'concat_text=bactrim:10 meq': 1.0}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.inverse_transform(X_dict_test['entities+entities_between+surface']['full'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instead of computing metrics this way,\n",
    "# let's use their script to make sure we get the same number.\n",
    "\n",
    "# For now, let's start with just the full feature_set\n",
    "y_pred_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For some reasons there are duplicates in the training data\n",
    "# which probably messed up the cross-validation.\n",
    "\n",
    "new_docs = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-4e4f39427681>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnew_docs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0manno\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_annotations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manno\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'new_docs' is not defined"
     ]
    }
   ],
   "source": [
    "for fname, doc in new_docs.items():\n",
    "    for anno in doc.get_annotations():\n",
    "        print(anno.id)\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adverse\n",
      "{'text_in_anno1': 'lidocaine', 'text_in_anno2': 'anesthesia', 'concat_text': 'lidocaine:anesthesia', 'first_entity_type:<DRUG>': 1, 'second_entity_type:<INDICATION>': 1, 'entity_types_concat': '<DRUG><=><INDICATION>', 'num_entities_between': 0, 'num_sentences_overlap': 1, 'num_tokens_between': 6, 'grams_between:<OOV>': 1, 'grams_between:<local>': 1, 'grams_between:<then was>': 1, 'grams_between:<injected>': 1, 'grams_between:<then>': 1, 'grams_between:<to>': 1, 'grams_between:<was>': 1, 'grams_before:<%>': 1, 'grams_before:<<NUMBER>>': 1, 'grams_before:<% <NUMBER>>': 1, 'grams_after:<OOV>': 1, 'tags_between:<to vb vbn>': 1, 'tags_between:<vb>': 1, 'tags_between:<vbn>': 1, 'tags_between:<rb vbd vbn>': 1, 'tags_between:<rb>': 1, 'tags_between:<rb vbd>': 1, 'tags_between:<rb vbn>': 1, 'tags_between:<jj to vb>': 1, 'tags_between:<rb to vbn>': 1, 'tags_between:<to vbn>': 1, 'tags_between:<to>': 1, 'tags_between:<to vb>': 1, 'tags_between:<jj>': 1, 'tags_between:<vbd>': 1, 'tags_between:<jj vb>': 1, 'tags_before:<cd>': 1, 'tags_before:<nn>': 1, 'tags_before:<cd nn>': 1, 'tags_after:<OOV>': 1, 'same_sentence': 1}\n",
      "'lidocaine':'anesthesia', Drug:Indication, type=adverse\n",
      "10_1010\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    print(y_pred_dict['entities+entities_between+surface'][i])\n",
    "    print(feat_dicts_test['entities+entities_between+surface'][i])\n",
    "    print(test_relations[i])\n",
    "    print(test_relations[i].file_name)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doxorubicin':'infusion', Drug:Route, type=manner/route,\n",
       " 'valacyclovir':'bilateral shingles', Drug:Indication, type=reason,\n",
       " 'bloating':'mild', SSLIF:Severity, type=severity_type,\n",
       " 'neutropenia medication':'neutropenia', Drug:Indication, type=reason,\n",
       " 'Valtrex':'500 mg', Drug:Dose, type=do,\n",
       " 'Valtrex':'3 times daily', Drug:Frequency, type=fr,\n",
       " 'posaconazole':'200 mg per 5 mL', Drug:Dose, type=do,\n",
       " 'posaconazole':'3 times daily', Drug:Frequency, type=fr,\n",
       " 'ciprofloxacin':'500 mg', Drug:Dose, type=do,\n",
       " 'ciprofloxacin':'daily', Drug:Frequency, type=fr,\n",
       " 'antiemetics':'p.r.n.', Drug:Frequency, type=fr,\n",
       " 'uric acid':'daily', Drug:Frequency, type=fr,\n",
       " 'uric acid':'prophylaxis', Drug:Indication, type=reason,\n",
       " 'uric acid':'tumor \\nlysis syndrome', Drug:Indication, type=reason,\n",
       " 'doxorubicin':'infusion', Drug:Route, type=manner/route,\n",
       " 'valacyclovir':'bilateral shingles', Drug:Indication, type=reason,\n",
       " 'bloating':'mild', SSLIF:Severity, type=severity_type,\n",
       " 'neutropenia medication':'neutropenia', Drug:Indication, type=reason,\n",
       " 'Valtrex':'500 mg', Drug:Dose, type=do,\n",
       " 'Valtrex':'3 times daily', Drug:Frequency, type=fr,\n",
       " 'posaconazole':'200 mg per 5 mL', Drug:Dose, type=do,\n",
       " 'posaconazole':'3 times daily', Drug:Frequency, type=fr,\n",
       " 'ciprofloxacin':'500 mg', Drug:Dose, type=do,\n",
       " 'ciprofloxacin':'daily', Drug:Frequency, type=fr,\n",
       " 'antiemetics':'p.r.n.', Drug:Frequency, type=fr,\n",
       " 'uric acid':'daily', Drug:Frequency, type=fr,\n",
       " 'uric acid':'prophylaxis', Drug:Indication, type=reason,\n",
       " 'uric acid':'tumor \\nlysis syndrome', Drug:Indication, type=reason,\n",
       " 'Valtrex':'neutropenia', Drug:Indication, type=reason,\n",
       " 'hyper-CVAD':'daily', Drug:Frequency, type=fr]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[r for r in new_docs['10_1'].relations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[a.id for a in new_docs['10_1'].annotations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "def compute_metrics(y, pred, average='micro'):\n",
    "    metrics = {}\n",
    "    labels = set(y)\n",
    "    labels.remove('none')\n",
    "    labels = list(sorted(labels))\n",
    "    metrics['precision'] = precision_score(y, pred, average=average, labels=labels)\n",
    "    metrics['recall'] = recall_score(y, pred, average=average, labels=labels)\n",
    "    metrics['f1'] = f1_score(y, pred, average=average, labels=labels)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now compute the metrics\n",
    "metrics = {feature_set_name: None for feature_set_name in y_pred_dict.keys()}\n",
    "\n",
    "for feature_set_name, y_pred in y_pred_dict.items():\n",
    "    metrics[feature_set_name] = compute_metrics(y_dict['full'], y_pred, 'micro')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('entities+entities_between+surface',\n",
       "  {'f1': 0.9318378304591165,\n",
       "   'precision': 0.98508826783395065,\n",
       "   'recall': 0.88404921217353771}),\n",
       " ('entities+entities_between',\n",
       "  {'f1': 0.9058706875918896,\n",
       "   'precision': 0.95967255077153413,\n",
       "   'recall': 0.85778113533347722}),\n",
       " ('surface',\n",
       "  {'f1': 0.6509934013629971,\n",
       "   'precision': 0.73451951503973523,\n",
       "   'recall': 0.58452406647960287}),\n",
       " ('entities',\n",
       "  {'f1': 0.52739580229043381,\n",
       "   'precision': 0.76985540870862157,\n",
       "   'recall': 0.40107921433196631}),\n",
       " ('entities_between',\n",
       "  {'f1': 0.1727288185373545,\n",
       "   'precision': 0.30428625891835959,\n",
       "   'recall': 0.12059140945391755})]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(metrics.items(), key=lambda x:x[1]['f1'], reverse=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
